{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "un5MOY7zyVzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09af859-ffbb-48df-d5e7-a6820d661931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (709 kB)\n",
            "\u001b[K     |████████████████████████████████| 709 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=a49ee63d8d10de225cbd0fbfb67d323578162fc0f4618bf02cf4033957ee48f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb\n",
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "hLSzJa5IydYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2aa2975-8854-40cd-c044-7cd8eda86327"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7047 sha256=6b3c7c3ea9c810af32c0e1f6fb208a9810ab92544e51e255f6475324f0ad07db\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.8.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.49->umap-learn) (4.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=dec1115b992ac43795ad4dd9831996a8ef26779959b3e634ddb7cd17e42dab6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.8-py3-none-any.whl size=55513 sha256=c5437bbf28e04b30b9de4144ed4a4f270782dc2f37a0e409d2c5c82612e71ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/bc/eb/974072a56a7082a302f8b4be1ad6d21bf5019235c2eff65928\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.8 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Optional\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import ModuleList, Linear, BatchNorm1d\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, JumpingKnowledge\n",
        "from torch_geometric.data import NeighborSampler\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_sparse import SparseTensor"
      ],
      "metadata": {
        "id": "cYtRS9lyywmr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "    def __init__(self, runs, info=None):\n",
        "        self.info = info\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 3\n",
        "        assert run >= 0 and run < len(self.results)\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None, print_all=True):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            argmax = result[:, 1].argmax().item()\n",
        "            print(f'Run {run + 1:02d}:', flush=True)\n",
        "            print(f'Highest Train: {result[:, 0].max():.2f}', flush=True)\n",
        "            print(f'Highest Valid: {result[:, 1].max():.2f}', flush=True)\n",
        "            print(f'  Final Train: {result[argmax, 0]:.2f}', flush=True)\n",
        "            print(f'   Final Test: {result[argmax, 2]:.2f}', flush=True)\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                train1 = r[:, 0].max().item()\n",
        "                valid = r[:, 1].max().item()\n",
        "                train2 = r[r[:, 1].argmax(), 0].item()\n",
        "                test = r[r[:, 1].argmax(), 2].item()\n",
        "                best_results.append((train1, valid, train2, test))\n",
        "\n",
        "            best_result = torch.tensor(best_results)\n",
        "\n",
        "            print(f'All runs:', flush=True)\n",
        "            if print_all:\n",
        "                r = best_result[:, 0]\n",
        "                print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}', flush=True)\n",
        "                r = best_result[:, 1]\n",
        "                print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}', flush=True)\n",
        "                r = best_result[:, 2]\n",
        "                print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}', flush=True)\n",
        "            r = best_result[:, 3]\n",
        "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}', flush=True)"
      ],
      "metadata": {
        "id": "U8TODVofy9kS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelPropagation(MessagePassing):\n",
        "    def __init__(self, num_layers: int, alpha: float):\n",
        "        super(LabelPropagation, self).__init__(aggr='add')\n",
        "        self.num_layers = num_layers\n",
        "        self.alpha = alpha\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "            self, y: Tensor, edge_index: Adj, mask: Optional[Tensor] = None,\n",
        "            edge_weight: OptTensor = None,\n",
        "            post_step: Callable = lambda y: y.clamp_(0., 1.)\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        if y.dtype == torch.long:\n",
        "            y = F.one_hot(y.view(-1)).to(torch.float)\n",
        "\n",
        "        out = y\n",
        "        if mask is not None:\n",
        "            out = torch.zeros_like(y)\n",
        "            out[mask] = y[mask]\n",
        "\n",
        "        if isinstance(edge_index, SparseTensor) and not edge_index.has_value():\n",
        "            edge_index = gcn_norm(edge_index, add_self_loops=False)\n",
        "        elif isinstance(edge_index, Tensor) and edge_weight is None:\n",
        "            edge_index, edge_weight = gcn_norm(edge_index, num_nodes=y.size(0),\n",
        "                                               add_self_loops=False)\n",
        "\n",
        "        res = (1 - self.alpha) * out\n",
        "        for _ in range(self.num_layers):\n",
        "            # propagate_type: (y: Tensor, edge_weight: OptTensor)\n",
        "            out = self.propagate(edge_index, x=out, edge_weight=edge_weight,\n",
        "                                 size=None)\n",
        "            out.mul_(self.alpha).add_(res)\n",
        "            out = post_step(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n",
        "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(num_layers={}, alpha={})'.format(self.__class__.__name__,\n",
        "                                                    self.num_layers,\n",
        "                                                    self.alpha)"
      ],
      "metadata": {
        "id": "iOISjdKWy-RK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CorrectAndSmooth(torch.nn.Module):\n",
        "    def __init__(self, num_correction_layers: int, correction_alpha: float,\n",
        "                 num_smoothing_layers: int, smoothing_alpha: float,\n",
        "                 autoscale: bool = True, scale: float = 1.0):\n",
        "        super(CorrectAndSmooth, self).__init__()\n",
        "        self.autoscale = autoscale\n",
        "        self.scale = scale\n",
        "\n",
        "        self.prop1 = LabelPropagation(num_correction_layers, correction_alpha)\n",
        "        self.prop2 = LabelPropagation(num_smoothing_layers, smoothing_alpha)\n",
        "\n",
        "    def correct(self, y_soft: Tensor, y_true: Tensor, mask: Tensor,\n",
        "                edge_index: Adj, edge_weight: OptTensor = None) -> Tensor:\n",
        "        assert abs((float(y_soft.sum()) / y_soft.size(0)) - 1.0) < 1e-2\n",
        "\n",
        "        numel = int(mask.sum()) if mask.dtype == torch.bool else mask.size(0)\n",
        "        assert y_true.size(0) == numel\n",
        "\n",
        "        if y_true.dtype == torch.long:\n",
        "            y_true = F.one_hot(y_true.view(-1), y_soft.size(-1))\n",
        "            y_true = y_true.to(y_soft.dtype)\n",
        "\n",
        "        error = torch.zeros_like(y_soft)\n",
        "        error[mask] = y_true - y_soft[mask]\n",
        "\n",
        "        if self.autoscale:\n",
        "            smoothed_error = self.prop1(error, edge_index,\n",
        "                                        edge_weight=edge_weight,\n",
        "                                        post_step=lambda x: x.clamp_(-1., 1.))\n",
        "\n",
        "            sigma = error[mask].abs().sum() / numel\n",
        "            scale = sigma / smoothed_error.abs().sum(dim=1, keepdim=True)\n",
        "            scale[scale.isinf() | (scale > 1000)] = 1.0\n",
        "            return y_soft + scale * smoothed_error\n",
        "        else:\n",
        "\n",
        "            def fix_input(x):\n",
        "                x[mask] = error[mask]\n",
        "                return x\n",
        "\n",
        "            smoothed_error = self.prop1(error, edge_index,\n",
        "                                        edge_weight=edge_weight,\n",
        "                                        post_step=fix_input)\n",
        "            return y_soft + self.scale * smoothed_error\n",
        "\n",
        "    def smooth(self, y_soft: Tensor, y_true: Tensor, mask: Tensor,\n",
        "               edge_index: Adj, edge_weight: OptTensor = None) -> Tensor:\n",
        "\n",
        "        numel = int(mask.sum()) if mask.dtype == torch.bool else mask.size(0)\n",
        "        assert y_true.size(0) == numel\n",
        "\n",
        "        if y_true.dtype == torch.long:\n",
        "            y_true = F.one_hot(y_true.view(-1), y_soft.size(-1))\n",
        "            y_true = y_true.to(y_soft.dtype)\n",
        "\n",
        "        y_soft[mask] = y_true\n",
        "\n",
        "        return self.prop2(y_soft, edge_index, edge_weight=edge_weight)\n",
        "\n",
        "    def __repr__(self):\n",
        "        L1, alpha1 = self.prop1.num_layers, self.prop1.alpha\n",
        "        L2, alpha2 = self.prop2.num_layers, self.prop2.alpha\n",
        "        return (f'{self.__class__.__name__}(\\n'\n",
        "                f'    correct: num_layers={L1}, alpha={alpha1}\\n'\n",
        "                f'    smooth:  num_layers={L2}, alpha={alpha2}\\n'\n",
        "                f'    autoscale={self.autoscale}, scale={self.scale}\\n'\n",
        "                ')')"
      ],
      "metadata": {
        "id": "Smm7nZvBzSy9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "from torch_geometric.nn import Node2Vec\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "def save_embedding(model):\n",
        "    torch.save(model.embedding.weight.data.cpu(), 'embedding.pt')"
      ],
      "metadata": {
        "id": "wn_wIiEyMAdm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "# num_layers = 2\n",
        "# dropout = 0.5\n",
        "# epochs = 1\n",
        "# batch_size = 128\n",
        "# walk_length = 20\n",
        "# lr = 0.01\n",
        "# log_steps = 100\n",
        "# walks_per_node = 1\n",
        "# context_size = 10\n",
        "# embedding_dim = 64 "
      ],
      "metadata": {
        "id": "At3xdVhayit3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "num_layers = 3\n",
        "dropout = 0.5\n",
        "epochs = 1\n",
        "batch_size = 256\n",
        "walk_length = 40\n",
        "lr = 0.01\n",
        "log_steps = 100\n",
        "walks_per_node = 10\n",
        "context_size = 20\n",
        "embedding_dim = 128\n",
        "\n",
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='./products/')\n",
        "data = dataset[0]"
      ],
      "metadata": {
        "id": "Q4D2U2cvF-HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8977ced-a7f6-4f72-e5b1-7e725ed0d41f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [01:24<00:00, 16.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./products/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1115.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Node2Vec(data.edge_index, embedding_dim, walk_length,\n",
        "                     context_size, walks_per_node,\n",
        "                     sparse=True).to(device)\n",
        "\n",
        "loader = model.loader(batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=4)\n",
        "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=lr)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
        "      optimizer.zero_grad()\n",
        "      loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "      loss.backward()    \n",
        "      optimizer.step()\n",
        "\n",
        "      if (i + 1) % log_steps == 0:\n",
        "        print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, 'f'Loss: {loss:.4f}')\n",
        "\n",
        "      if (i + 1) % 100 == 0:  \n",
        "        save_embedding(model)\n",
        "    save_embedding(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCPB28b29Yw",
        "outputId": "731a9888-8258-4b75-beb9-9f85a638e563"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Step: 100/9567, Loss: 9.3861\n",
            "Epoch: 01, Step: 200/9567, Loss: 8.5542\n",
            "Epoch: 01, Step: 300/9567, Loss: 7.7124\n",
            "Epoch: 01, Step: 400/9567, Loss: 6.8442\n",
            "Epoch: 01, Step: 500/9567, Loss: 6.1089\n",
            "Epoch: 01, Step: 600/9567, Loss: 5.4126\n",
            "Epoch: 01, Step: 700/9567, Loss: 4.7310\n",
            "Epoch: 01, Step: 800/9567, Loss: 4.2415\n",
            "Epoch: 01, Step: 900/9567, Loss: 3.7483\n",
            "Epoch: 01, Step: 1000/9567, Loss: 3.3870\n",
            "Epoch: 01, Step: 1100/9567, Loss: 3.0267\n",
            "Epoch: 01, Step: 1200/9567, Loss: 2.7368\n",
            "Epoch: 01, Step: 1300/9567, Loss: 2.4965\n",
            "Epoch: 01, Step: 1400/9567, Loss: 2.3073\n",
            "Epoch: 01, Step: 1500/9567, Loss: 2.1435\n",
            "Epoch: 01, Step: 1600/9567, Loss: 1.9940\n",
            "Epoch: 01, Step: 1700/9567, Loss: 1.9041\n",
            "Epoch: 01, Step: 1800/9567, Loss: 1.7844\n",
            "Epoch: 01, Step: 1900/9567, Loss: 1.6951\n",
            "Epoch: 01, Step: 2000/9567, Loss: 1.6243\n",
            "Epoch: 01, Step: 2100/9567, Loss: 1.5495\n",
            "Epoch: 01, Step: 2200/9567, Loss: 1.4972\n",
            "Epoch: 01, Step: 2300/9567, Loss: 1.4368\n",
            "Epoch: 01, Step: 2400/9567, Loss: 1.3817\n",
            "Epoch: 01, Step: 2500/9567, Loss: 1.3493\n",
            "Epoch: 01, Step: 2600/9567, Loss: 1.2957\n",
            "Epoch: 01, Step: 2700/9567, Loss: 1.2637\n",
            "Epoch: 01, Step: 2800/9567, Loss: 1.2292\n",
            "Epoch: 01, Step: 2900/9567, Loss: 1.2019\n",
            "Epoch: 01, Step: 3000/9567, Loss: 1.1712\n",
            "Epoch: 01, Step: 3100/9567, Loss: 1.1546\n",
            "Epoch: 01, Step: 3200/9567, Loss: 1.1366\n",
            "Epoch: 01, Step: 3300/9567, Loss: 1.1150\n",
            "Epoch: 01, Step: 3400/9567, Loss: 1.1019\n",
            "Epoch: 01, Step: 3500/9567, Loss: 1.0825\n",
            "Epoch: 01, Step: 3600/9567, Loss: 1.0610\n",
            "Epoch: 01, Step: 3700/9567, Loss: 1.0562\n",
            "Epoch: 01, Step: 3800/9567, Loss: 1.0306\n",
            "Epoch: 01, Step: 3900/9567, Loss: 1.0323\n",
            "Epoch: 01, Step: 4000/9567, Loss: 1.0081\n",
            "Epoch: 01, Step: 4100/9567, Loss: 1.0011\n",
            "Epoch: 01, Step: 4200/9567, Loss: 1.0033\n",
            "Epoch: 01, Step: 4300/9567, Loss: 0.9861\n",
            "Epoch: 01, Step: 4400/9567, Loss: 0.9802\n",
            "Epoch: 01, Step: 4500/9567, Loss: 0.9665\n",
            "Epoch: 01, Step: 4600/9567, Loss: 0.9724\n",
            "Epoch: 01, Step: 4700/9567, Loss: 0.9673\n",
            "Epoch: 01, Step: 4800/9567, Loss: 0.9540\n",
            "Epoch: 01, Step: 4900/9567, Loss: 0.9511\n",
            "Epoch: 01, Step: 5000/9567, Loss: 0.9509\n",
            "Epoch: 01, Step: 5100/9567, Loss: 0.9449\n",
            "Epoch: 01, Step: 5200/9567, Loss: 0.9383\n",
            "Epoch: 01, Step: 5300/9567, Loss: 0.9302\n",
            "Epoch: 01, Step: 5400/9567, Loss: 0.9287\n",
            "Epoch: 01, Step: 5500/9567, Loss: 0.9297\n",
            "Epoch: 01, Step: 5600/9567, Loss: 0.9227\n",
            "Epoch: 01, Step: 5700/9567, Loss: 0.9295\n",
            "Epoch: 01, Step: 5800/9567, Loss: 0.9225\n",
            "Epoch: 01, Step: 5900/9567, Loss: 0.9148\n",
            "Epoch: 01, Step: 6000/9567, Loss: 0.9135\n",
            "Epoch: 01, Step: 6100/9567, Loss: 0.9089\n",
            "Epoch: 01, Step: 6200/9567, Loss: 0.9090\n",
            "Epoch: 01, Step: 6300/9567, Loss: 0.9072\n",
            "Epoch: 01, Step: 6400/9567, Loss: 0.9037\n",
            "Epoch: 01, Step: 6500/9567, Loss: 0.8998\n",
            "Epoch: 01, Step: 6600/9567, Loss: 0.9022\n",
            "Epoch: 01, Step: 6700/9567, Loss: 0.8977\n",
            "Epoch: 01, Step: 6800/9567, Loss: 0.8975\n",
            "Epoch: 01, Step: 6900/9567, Loss: 0.8962\n",
            "Epoch: 01, Step: 7000/9567, Loss: 0.8927\n",
            "Epoch: 01, Step: 7100/9567, Loss: 0.8940\n",
            "Epoch: 01, Step: 7200/9567, Loss: 0.8849\n",
            "Epoch: 01, Step: 7300/9567, Loss: 0.8892\n",
            "Epoch: 01, Step: 7400/9567, Loss: 0.8853\n",
            "Epoch: 01, Step: 7500/9567, Loss: 0.8855\n",
            "Epoch: 01, Step: 7600/9567, Loss: 0.8911\n",
            "Epoch: 01, Step: 7700/9567, Loss: 0.8866\n",
            "Epoch: 01, Step: 7800/9567, Loss: 0.8924\n",
            "Epoch: 01, Step: 7900/9567, Loss: 0.8839\n",
            "Epoch: 01, Step: 8000/9567, Loss: 0.8798\n",
            "Epoch: 01, Step: 8100/9567, Loss: 0.8790\n",
            "Epoch: 01, Step: 8200/9567, Loss: 0.8853\n",
            "Epoch: 01, Step: 8300/9567, Loss: 0.8813\n",
            "Epoch: 01, Step: 8400/9567, Loss: 0.8802\n",
            "Epoch: 01, Step: 8500/9567, Loss: 0.8707\n",
            "Epoch: 01, Step: 8600/9567, Loss: 0.8789\n",
            "Epoch: 01, Step: 8700/9567, Loss: 0.8777\n",
            "Epoch: 01, Step: 8800/9567, Loss: 0.8755\n",
            "Epoch: 01, Step: 8900/9567, Loss: 0.8723\n",
            "Epoch: 01, Step: 9000/9567, Loss: 0.8679\n",
            "Epoch: 01, Step: 9100/9567, Loss: 0.8784\n",
            "Epoch: 01, Step: 9200/9567, Loss: 0.8777\n",
            "Epoch: 01, Step: 9300/9567, Loss: 0.8718\n",
            "Epoch: 01, Step: 9400/9567, Loss: 0.8740\n",
            "Epoch: 01, Step: 9500/9567, Loss: 0.8713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "y002j69azj-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PygNodePropPredDataset(name='ogbn-products', root='./products/')\n",
        "print(dataset, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUW_gFPOzXWa",
        "outputId": "58f93d82-9d55-4790-86c7-8905f5db9030"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PygNodePropPredDataset()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "print(data, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhMapPauzpBC",
        "outputId": "a6f360d0-ca25-41b1-92aa-c6e18d2c8407"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(num_nodes=2449029, edge_index=[2, 123718280], x=[2449029, 100], y=[2449029, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split_idx contains a dictionary of train, validation and test node indices\n",
        "split_idx = dataset.get_idx_split()\n",
        "# predefined ogb evaluator method used for validation of predictions\n",
        "evaluator = Evaluator(name='ogbn-products')"
      ],
      "metadata": {
        "id": "WVm3raIFzrHa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the node ids distribution of train, test and val\n",
        "print('Number of training nodes:', split_idx['train'].size(0))\n",
        "print('Number of validation nodes:', split_idx['valid'].size(0))\n",
        "print('Number of test nodes:', split_idx['test'].size(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWRvIk3G0s62",
        "outputId": "2f177274-12fb-4caa-a504-bdf3c82b2a00"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training nodes: 196615\n",
            "Number of validation nodes: 39323\n",
            "Number of test nodes: 2213091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check some graph statistics of ogb-product graph\n",
        "print(\"Number of nodes in the graph:\", data.num_nodes)\n",
        "print(\"Number of edges in the graph:\", data.num_edges)\n",
        "print(\"---------------------------------------------\")\n",
        "print(\"Node feature matrix with shape:\", data.x.shape) # [num_nodes, num_node_features]\n",
        "print(\"Graph connectivity in COO format with shape:\", data.edge_index.shape) # [2, num_edges]\n",
        "print(\"Target to train against :\", data.y.shape) \n",
        "print(\"Node feature length\", dataset.num_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzTT8Lwl0vzO",
        "outputId": "fc2bb295-4985-4417-d438-59364319b6be"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in the graph: 2449029\n",
            "Number of edges in the graph: 123718280\n",
            "---------------------------------------------\n",
            "Node feature matrix with shape: torch.Size([2449029, 100])\n",
            "Graph connectivity in COO format with shape: torch.Size([2, 123718280])\n",
            "Target to train against : torch.Size([2449029, 1])\n",
            "Node feature length 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = split_idx['train']\n",
        "test_idx = split_idx['test']"
      ],
      "metadata": {
        "id": "Fqs_0PKn0yeD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
        "                               sizes=[15, 10, 5], batch_size=1024,\n",
        "                               shuffle=True, num_workers=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8O_oJqO04qF",
        "outputId": "272b5417-2a0a-4f4c-eed1-486ba789d1b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subgraph_loader = NeighborSampler(data.edge_index, node_idx=None, sizes=[-1],\n",
        "                                  batch_size=1024, shuffle=False,\n",
        "                                  num_workers=12)"
      ],
      "metadata": {
        "id": "FWhQOzN607WW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, dataset, in_channels, hidden_channels, num_layers=3):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        for i in range(self.num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.convs.append(SAGEConv(hidden_channels, dataset.num_classes))\n",
        "\n",
        "    # def reset_masks(self):\n",
        "    #     self.masks = [None] * (self.num_layers - 1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "        # for skip in self.skips:\n",
        "        #     skip.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        # `train_loader` computes the k-hop neighborhood of a batch of nodes,\n",
        "        # and returns, for each layer, a bipartite graph object, holding the\n",
        "        # bipartite edges `edge_index`, the index `e_id` of the original edges,\n",
        "        # and the size/shape `size` of the bipartite graph.\n",
        "        # Target nodes are also included in the source nodes so that one can\n",
        "        # easily apply skip-connections or add self-loops.\n",
        "        for i, (adj_t, _, size) in enumerate(adjs):\n",
        "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
        "            x = self.convs[i]((x, x_target), adj_t)\n",
        "            # x = x + self.skips[i](x_target)\n",
        "            if i != self.num_layers - 1:\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x.log_softmax(dim=-1)\n",
        "\n",
        "    def inference(self, x_all):\n",
        "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\n",
        "        pbar.set_description('Evaluating')\n",
        "\n",
        "        # Compute representations of nodes layer by layer, using *all*\n",
        "        # available edges. This leads to faster computation in contrast to\n",
        "        # immediately computing the final representations of each batch.\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        total_edges = 0\n",
        "        for i in range(self.num_layers):\n",
        "            xs = []\n",
        "            for batch_size, n_id, adj in subgraph_loader:\n",
        "                adj_t, _, size = adj.to(device)\n",
        "                total_edges += adj_t.size(1)\n",
        "                x = x_all[n_id].to(device)\n",
        "                x_target = x[:size[1]]\n",
        "                x = self.convs[i]((x, x_target), adj_t)\n",
        "                # x = x + self.skips[i](x_target)\n",
        "\n",
        "                if i != self.num_layers - 1:\n",
        "                    x = self.bns[i](x)\n",
        "                    x = F.relu(x)\n",
        "                xs.append(x.cpu())\n",
        "\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "            x_all = torch.cat(xs, dim=0)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        return x_all"
      ],
      "metadata": {
        "id": "3O7pOd4L1Qx7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.x\n",
        "embedding = torch.load('embedding.pt', map_location='cpu')\n",
        "x = torch.cat([x, embedding], dim=-1)"
      ],
      "metadata": {
        "id": "xCLgDIfOvZJL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SAGE(dataset=dataset, in_channels=x.size(-1), hidden_channels=128, num_layers=3)\n",
        "print(model, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNRvADVM1XKL",
        "outputId": "f6a2d35b-4ccc-465f-b8dc-5f1f0819a7f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAGE(\n",
            "  (convs): ModuleList(\n",
            "    (0): SAGEConv(228, 128, aggr=mean)\n",
            "    (1): SAGEConv(128, 128, aggr=mean)\n",
            "    (2): SAGEConv(128, 47, aggr=mean)\n",
            "  )\n",
            "  (bns): ModuleList(\n",
            "    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device, flush=True)\n",
        "model.to(device)\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgF2Ffof1ZVq",
        "outputId": "6f25dbb9-cca4-46f8-9c3c-2b6176114555"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.to(device)\n",
        "y = data.y.squeeze().to(device)\n",
        "train_idx = split_idx['train'].to(device)\n",
        "val_idx = split_idx['valid'].to(device)\n",
        "test_idx = split_idx['test'].to(device)\n",
        "x_train, y_train = x[train_idx], y[train_idx]"
      ],
      "metadata": {
        "id": "os0Fk5pQ1hKn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_adj(data):\n",
        "    N = data.num_nodes\n",
        "    data.edge_index = to_undirected(data.edge_index, data.num_nodes)\n",
        "\n",
        "    row, col = data.edge_index\n",
        "\n",
        "    adj = SparseTensor(row=row, col=col, sparse_sizes=(N, N))\n",
        "    adj = adj.set_diag()\n",
        "    deg = adj.sum(dim=1).to(torch.float)\n",
        "    deg_inv_sqrt = deg.pow(-0.5)\n",
        "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "    adj = deg_inv_sqrt.view(-1, 1) * adj * deg_inv_sqrt.view(1, -1)\n",
        "    return adj"
      ],
      "metadata": {
        "id": "eRvHWSyZ1jgS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DAD = process_adj(data).to(device)"
      ],
      "metadata": {
        "id": "lMWGFxsa1mP3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.NLLLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ],
      "metadata": {
        "id": "cXWQkTKq1n5w"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=train_idx.size(0))\n",
        "    pbar.set_description(f'Epoch {epoch:02d}')\n",
        "\n",
        "    total_loss = total_correct = 0\n",
        "    for batch_size, n_id, adjs in train_loader:\n",
        "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "        adjs = [adj.to(device) for adj in adjs]\n",
        "\n",
        "        out = model(x[n_id], adjs)\n",
        "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\n",
        "        pbar.update(batch_size)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    loss = total_loss / len(train_loader)\n",
        "    approx_acc = total_correct / int(train_idx.size(0))\n",
        "\n",
        "    return loss, approx_acc"
      ],
      "metadata": {
        "id": "brgeqdDi1pzX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(out=None):\n",
        "    model.eval()\n",
        "\n",
        "    out = model.inference(x) if out is None else out\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc, out"
      ],
      "metadata": {
        "id": "LIZraCwz1scs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    runs = 2\n",
        "    logger = Logger(runs)\n",
        "\n",
        "    for run in range(runs):\n",
        "        print(sum(p.numel() for p in model.parameters()), flush=True)\n",
        "\n",
        "        print('', flush=True)\n",
        "        print(f'Run {run + 1:02d}:', flush=True)\n",
        "        print('', flush=True)\n",
        "\n",
        "        model.reset_parameters()\n",
        "\n",
        "        best_val_acc = 0\n",
        "\n",
        "        for epoch in range(15):\n",
        "            loss, acc = train()\n",
        "            # print('Epoch {:03d} train_loss: {:.4f}'.format(epoch, loss))\n",
        "            print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}', flush=True)\n",
        "\n",
        "            if (epoch + 1) > 5:\n",
        "\n",
        "                train_acc, val_acc, test_acc, out = test()\n",
        "                result = (train_acc, val_acc, test_acc)\n",
        "                # print(f'Train: {train_acc:.4f}, Val: {valid_acc:.4f}, 'f'Test: {test_acc:.4f}')\n",
        "                if val_acc > best_val_acc:\n",
        "                    best_val_acc = val_acc\n",
        "                    y_soft = out.softmax(dim=-1).to(device)\n",
        "\n",
        "                print(f'Run: {run + 1:02d}, '\n",
        "                      f'Epoch: {epoch:02d}, '\n",
        "                      f'Loss: {loss:.4f}, '\n",
        "                      f'Train: {100 * train_acc:.2f}%, '\n",
        "                      f'Valid: {100 * val_acc:.2f}% '\n",
        "                      f'Test: {100 * test_acc:.2f}%', flush=True)\n",
        "\n",
        "            # logger.add_result(run, result)\n",
        "\n",
        "        # post = CorrectAndSmooth(num_correction_layers=50, correction_alpha=1.0,\n",
        "        #                         num_smoothing_layers=50, smoothing_alpha=0.8,\n",
        "        #                         autoscale=False, scale=20.)\n",
        "\n",
        "        post = CorrectAndSmooth(num_correction_layers=100, correction_alpha=0.8,\n",
        "                                num_smoothing_layers=100, smoothing_alpha=0.8,\n",
        "                                autoscale=False, scale=10.)\n",
        "\n",
        "        print('Correct and smooth...', flush=True)\n",
        "\n",
        "        y_soft = post.correct(y_soft, y_train, train_idx, DAD)\n",
        "        y_soft = post.smooth(y_soft, y_train, train_idx, DAD)\n",
        "        print('Done!', flush=True)\n",
        "\n",
        "        train_acc, val_acc, test_acc, _ = test(y_soft)\n",
        "        print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}', flush=True)\n",
        "\n",
        "        result = (train_acc, val_acc, test_acc)\n",
        "        logger.add_result(run, result)\n",
        "\n",
        "    logger.print_statistics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVx80qh1tUi",
        "outputId": "46ba70c5-47b1-4e8b-c274-85ad035ec863"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103983\n",
            "\n",
            "Run 01:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 00: 100%|██████████| 196615/196615 [00:49<00:00, 3944.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00, Loss: 0.5291, Approx. Train: 0.8661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 01: 100%|██████████| 196615/196615 [00:48<00:00, 4033.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01, Loss: 0.3474, Approx. Train: 0.9026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 02: 100%|██████████| 196615/196615 [00:49<00:00, 4007.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02, Loss: 0.3207, Approx. Train: 0.9088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 03: 100%|██████████| 196615/196615 [00:48<00:00, 4057.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03, Loss: 0.3101, Approx. Train: 0.9125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 04: 100%|██████████| 196615/196615 [00:48<00:00, 4040.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04, Loss: 0.3004, Approx. Train: 0.9139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 05: 100%|██████████| 196615/196615 [00:48<00:00, 4085.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05, Loss: 0.2929, Approx. Train: 0.9159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:40<00:00, 73421.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 05, Loss: 0.2929, Train: 92.67%, Valid: 91.83% Test: 80.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 06: 100%|██████████| 196615/196615 [00:49<00:00, 3999.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06, Loss: 0.3040, Approx. Train: 0.9168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:35<00:00, 77168.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 06, Loss: 0.3040, Train: 92.58%, Valid: 91.66% Test: 79.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 07: 100%|██████████| 196615/196615 [00:49<00:00, 3973.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07, Loss: 0.3066, Approx. Train: 0.9124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:32<00:00, 79075.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 07, Loss: 0.3066, Train: 92.63%, Valid: 91.77% Test: 79.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 08: 100%|██████████| 196615/196615 [00:49<00:00, 3990.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08, Loss: 0.2842, Approx. Train: 0.9178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:33<00:00, 78945.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 08, Loss: 0.2842, Train: 92.77%, Valid: 91.76% Test: 79.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 09: 100%|██████████| 196615/196615 [00:49<00:00, 3986.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09, Loss: 0.2785, Approx. Train: 0.9197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:34<00:00, 77605.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 09, Loss: 0.2785, Train: 93.06%, Valid: 92.01% Test: 79.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 196615/196615 [00:49<00:00, 4000.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.2807, Approx. Train: 0.9193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:34<00:00, 77905.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 10, Loss: 0.2807, Train: 93.12%, Valid: 91.97% Test: 79.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11: 100%|██████████| 196615/196615 [00:49<00:00, 4007.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 0.2708, Approx. Train: 0.9210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:35<00:00, 76928.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 11, Loss: 0.2708, Train: 93.21%, Valid: 91.97% Test: 80.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12: 100%|██████████| 196615/196615 [00:49<00:00, 3984.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Loss: 0.2709, Approx. Train: 0.9221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:32<00:00, 79489.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 12, Loss: 0.2709, Train: 93.15%, Valid: 91.82% Test: 80.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13: 100%|██████████| 196615/196615 [00:49<00:00, 3997.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Loss: 0.2799, Approx. Train: 0.9210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:35<00:00, 77245.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 13, Loss: 0.2799, Train: 93.36%, Valid: 92.11% Test: 79.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 196615/196615 [00:49<00:00, 3964.77it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Loss: 0.2773, Approx. Train: 0.9198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:33<00:00, 78605.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 01, Epoch: 14, Loss: 0.2773, Train: 93.35%, Valid: 91.96% Test: 79.90%\n",
            "Correct and smooth...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "Train: 0.9701, Val: 0.9225, Test: 0.8152\n",
            "103983\n",
            "\n",
            "Run 02:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 00: 100%|██████████| 196615/196615 [00:49<00:00, 4001.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00, Loss: 0.5126, Approx. Train: 0.8665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 01: 100%|██████████| 196615/196615 [00:49<00:00, 3988.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01, Loss: 0.3532, Approx. Train: 0.9021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 02: 100%|██████████| 196615/196615 [00:49<00:00, 3976.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02, Loss: 0.3306, Approx. Train: 0.9065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 03: 100%|██████████| 196615/196615 [00:48<00:00, 4018.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03, Loss: 0.3213, Approx. Train: 0.9098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 04: 100%|██████████| 196615/196615 [00:49<00:00, 4011.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04, Loss: 0.3115, Approx. Train: 0.9117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 05: 100%|██████████| 196615/196615 [00:49<00:00, 3997.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05, Loss: 0.3030, Approx. Train: 0.9127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:34<00:00, 77688.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 05, Loss: 0.3030, Train: 92.23%, Valid: 91.49% Test: 79.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 06: 100%|██████████| 196615/196615 [00:48<00:00, 4023.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06, Loss: 0.2915, Approx. Train: 0.9162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:32<00:00, 79676.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 06, Loss: 0.2915, Train: 92.43%, Valid: 91.46% Test: 79.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 07: 100%|██████████| 196615/196615 [00:49<00:00, 4001.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07, Loss: 0.2929, Approx. Train: 0.9157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:33<00:00, 78244.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 07, Loss: 0.2929, Train: 92.66%, Valid: 91.62% Test: 79.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 08: 100%|██████████| 196615/196615 [00:50<00:00, 3930.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08, Loss: 0.2882, Approx. Train: 0.9189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:32<00:00, 79216.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 08, Loss: 0.2882, Train: 92.85%, Valid: 91.82% Test: 79.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 09: 100%|██████████| 196615/196615 [00:49<00:00, 3941.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09, Loss: 0.2893, Approx. Train: 0.9167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:34<00:00, 77774.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 09, Loss: 0.2893, Train: 92.99%, Valid: 91.81% Test: 79.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10: 100%|██████████| 196615/196615 [00:50<00:00, 3926.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.2808, Approx. Train: 0.9187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:32<00:00, 79096.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 10, Loss: 0.2808, Train: 92.99%, Valid: 91.93% Test: 79.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 196615/196615 [00:49<00:00, 3945.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 0.2765, Approx. Train: 0.9204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:34<00:00, 78057.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 11, Loss: 0.2765, Train: 93.05%, Valid: 91.85% Test: 79.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12: 100%|██████████| 196615/196615 [00:49<00:00, 3993.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Loss: 0.2806, Approx. Train: 0.9194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:32<00:00, 79001.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 12, Loss: 0.2806, Train: 93.05%, Valid: 91.97% Test: 79.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 196615/196615 [00:49<00:00, 3993.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Loss: 0.2801, Approx. Train: 0.9199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:34<00:00, 77829.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 13, Loss: 0.2801, Train: 93.24%, Valid: 92.13% Test: 79.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 196615/196615 [00:50<00:00, 3917.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Loss: 0.2822, Approx. Train: 0.9179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 7347087/7347087 [01:33<00:00, 78757.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run: 02, Epoch: 14, Loss: 0.2822, Train: 93.20%, Valid: 91.92% Test: 79.66%\n",
            "Correct and smooth...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "Train: 0.9700, Val: 0.9232, Test: 0.8073\n",
            "All runs:\n",
            "Highest Train: 97.00 ± 0.01\n",
            "Highest Valid: 92.29 ± 0.04\n",
            "  Final Train: 97.00 ± 0.01\n",
            "   Final Test: 81.13 ± 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etgZi9murQLu"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}