{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XQxIVfngu61K",
        "outputId": "f5b9e97a-3844-43cc-ffc3-d2f65e942663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 10.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.15%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-cluster) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-cluster) (1.21.6)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_spline_conv-1.2.1%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (873 kB)\n",
            "\u001b[K     |████████████████████████████████| 873 kB 9.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 14.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 74.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=3ea97cf74758a32fcedb528021ac043858700e369dc6550d92250bf5dac902f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8lb6W_kuW7e",
        "outputId": "b0032288-a836-4f33-e9cf-d2df6ca98dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (4.64.1)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.21.6)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from ogb) (1.13.0+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.8/dist-packages (from outdated>=0.2.0->ogb) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->ogb) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.9.24)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7047 sha256=86ccfdfd6e12829f49fe08a4c0df38e7e6343e50614262e31349c797e9580e9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/33/c4/0ef84d7f5568c2823e3d63a6e08988852fb9e4bc822034870a\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VIUMQ4AuHuk"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList, Linear, BatchNorm1d\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "from torch_geometric.nn import models\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn.models import CorrectAndSmooth\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "    def __init__(self, runs, info=None):\n",
        "        self.info = info\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 3\n",
        "        assert run >= 0 and run < len(self.results)\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            argmax = result[:, 1].argmax().item()\n",
        "            print(f'Run {run + 1:02d}:', flush=True)\n",
        "            print(f'Highest Train: {result[:, 0].max():.2f}', flush=True)\n",
        "            print(f'Highest Valid: {result[:, 1].max():.2f}', flush=True)\n",
        "            print(f'  Final Train: {result[argmax, 0]:.2f}', flush=True)\n",
        "            print(f'   Final Test: {result[argmax, 2]:.2f}', flush=True)\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                train1 = r[:, 0].max().item()\n",
        "                valid = r[:, 1].max().item()\n",
        "                train2 = r[r[:, 1].argmax(), 0].item()\n",
        "                test = r[r[:, 1].argmax(), 2].item()\n",
        "                best_results.append((train1, valid, train2, test))\n",
        "\n",
        "            best_result = torch.tensor(best_results)\n",
        "\n",
        "            print(f'All runs:', flush=True)\n",
        "            r = best_result[:, 0]\n",
        "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}', flush=True)\n",
        "            r = best_result[:, 1]\n",
        "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}', flush=True)\n",
        "            r = best_result[:, 2]\n",
        "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}', flush=True)\n",
        "            r = best_result[:, 3]\n",
        "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}', flush=True)"
      ],
      "metadata": {
        "id": "tYk2eybnuK7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lins = ModuleList([Linear(in_channels, hidden_channels)])\n",
        "        self.bns = ModuleList([BatchNorm1d(hidden_channels)])\n",
        "\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(Linear(hidden_channels, hidden_channels))\n",
        "            self.bns.append(BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.lins.append(Linear(hidden_channels, out_channels))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lins in self.lins:\n",
        "            lins.reset_parameters()\n",
        "        for bns in self.bns:\n",
        "            bns.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for lin, bn in zip(self.lins[:-1], self.bns):\n",
        "            x = bn(lin(x).relu_())\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lins[-1](x)"
      ],
      "metadata": {
        "id": "ylpyboyvuQbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "set -e\n",
        "\n",
        "#---------------------------------------------------#\n",
        "JULIA_VERSION=\"1.8.2\" # any version ≥ 0.7.0\n",
        "JULIA_PACKAGES=\"IJulia BenchmarkTools\"\n",
        "JULIA_PACKAGES_IF_GPU=\"CUDA\" # or CuArrays for older Julia versions\n",
        "JULIA_NUM_THREADS=2\n",
        "#---------------------------------------------------#\n",
        "\n",
        "if [ -z `which julia` ]; then\n",
        "  # Install Julia\n",
        "  JULIA_VER=`cut -d '.' -f -2 <<< \"$JULIA_VERSION\"`\n",
        "  echo \"Installing Julia $JULIA_VERSION on the current Colab Runtime...\"\n",
        "  BASE_URL=\"https://julialang-s3.julialang.org/bin/linux/x64\"\n",
        "  URL=\"$BASE_URL/$JULIA_VER/julia-$JULIA_VERSION-linux-x86_64.tar.gz\"\n",
        "  wget -nv $URL -O /tmp/julia.tar.gz # -nv means \"not verbose\"\n",
        "  tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "  rm /tmp/julia.tar.gz\n",
        "\n",
        "  # Install Packages\n",
        "  nvidia-smi -L &> /dev/null && export GPU=1 || export GPU=0\n",
        "  if [ $GPU -eq 1 ]; then\n",
        "    JULIA_PACKAGES=\"$JULIA_PACKAGES $JULIA_PACKAGES_IF_GPU\"\n",
        "  fi\n",
        "  for PKG in `echo $JULIA_PACKAGES`; do\n",
        "    echo \"Installing Julia package $PKG...\"\n",
        "    julia -e 'using Pkg; pkg\"add '$PKG'; precompile;\"' &> /dev/null\n",
        "  done\n",
        "\n",
        "  # Install kernel and rename it to \"julia\"\n",
        "  echo \"Installing IJulia kernel...\"\n",
        "  julia -e 'using IJulia; IJulia.installkernel(\"julia\", env=Dict(\n",
        "      \"JULIA_NUM_THREADS\"=>\"'\"$JULIA_NUM_THREADS\"'\"))'\n",
        "  KERNEL_DIR=`julia -e \"using IJulia; print(IJulia.kerneldir())\"`\n",
        "  KERNEL_NAME=`ls -d \"$KERNEL_DIR\"/julia*`\n",
        "  mv -f $KERNEL_NAME \"$KERNEL_DIR\"/julia  \n",
        "\n",
        "  echo ''\n",
        "  echo \"Successfully installed `julia -v`!\"\n",
        "  echo \"Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\"\n",
        "  echo \"jump to the 'Checking the Installation' section.\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAUyS0_Uw1Af",
        "outputId": "2494407b-fb5e-440b-db6a-2e7a30cd0e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Julia 1.8.2 on the current Colab Runtime...\n",
            "2022-12-08 05:20:21 URL:https://storage.googleapis.com/julialang2/bin/linux/x64/1.8/julia-1.8.2-linux-x86_64.tar.gz [135859273/135859273] -> \"/tmp/julia.tar.gz\" [1]\n",
            "Installing Julia package IJulia...\n",
            "Installing Julia package BenchmarkTools...\n",
            "Installing Julia package CUDA...\n",
            "Installing IJulia kernel...\n",
            "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mInstalling julia kernelspec in /root/.local/share/jupyter/kernels/julia-1.8\n",
            "\n",
            "Successfully installed julia version 1.8.2!\n",
            "Please reload this page (press Ctrl+R, ⌘+R, or the F5 key) then\n",
            "jump to the 'Checking the Installation' section.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spectral(data, post_fix):\n",
        "    from julia.api import Julia\n",
        "    jl = Julia(compiled_modules=False)\n",
        "    from julia import Main\n",
        "    Main.include(\"./norm_spec.jl\")\n",
        "\n",
        "    print('Setting up spectral embedding', flush=True)\n",
        "    adj = data.adj_t\n",
        "    adj = adj.to_scipy(layout='csr')\n",
        "    result = torch.tensor(Main.main(adj, 128)).float()\n",
        "    print('Done!', flush=True)\n",
        "\n",
        "    torch.save(result, f'./embeddings/spectral{post_fix}.pt')\n",
        "    return result"
      ],
      "metadata": {
        "id": "iFRsb_4bvTg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_adj(data, device):\n",
        "    adj_t = data.adj_t.to(device)\n",
        "    deg = adj_t.sum(dim=1).to(torch.float)\n",
        "    deg_inv_sqrt = deg.pow_(-0.5)\n",
        "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "    DAD = deg_inv_sqrt.view(-1, 1) * adj_t * deg_inv_sqrt.view(1, -1)\n",
        "    DA = deg_inv_sqrt.view(-1, 1) * deg_inv_sqrt.view(-1, 1) * adj_t\n",
        "\n",
        "    return DAD, DA"
      ],
      "metadata": {
        "id": "1tp4Q7QbvXK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, x_train, criterion, y_train):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x_train)\n",
        "    loss = criterion(out, y_train.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)"
      ],
      "metadata": {
        "id": "tYFdtGVzvdGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, x, evaluator, y, train_idx, val_idx, test_idx, out=None):\n",
        "    model.eval()\n",
        "    out = model(x) if out is None else out\n",
        "    pred = out.argmax(dim=-1, keepdim=True)\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': y[train_idx],\n",
        "        'y_pred': pred[train_idx]\n",
        "    })['acc']\n",
        "    val_acc = evaluator.eval({\n",
        "        'y_true': y[val_idx],\n",
        "        'y_pred': pred[val_idx]\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': y[test_idx],\n",
        "        'y_pred': pred[test_idx]\n",
        "    })['acc']\n",
        "    return train_acc, val_acc, test_acc, out"
      ],
      "metadata": {
        "id": "1vrTrqRnvfNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='OGBN-Products (MLP-CS)')\n",
        "parser.add_argument('--device', type=int, default=0)\n",
        "parser.add_argument('--num_layers', type=int, default=3)\n",
        "parser.add_argument('--hidden_channels', type=int, default=512)\n",
        "parser.add_argument('--dropout', type=float, default=0.5)\n",
        "parser.add_argument('--lr', type=float, default=0.01) \n",
        "parser.add_argument('--epochs', type=int, default=200)\n",
        "parser.add_argument('--runs', type=int, default=10)\n",
        "parser.add_argument(\"--use_embed\", action=\"store_true\")\n",
        "parser.add_argument(\"--use_cached\", action=\"store_true\")\n",
        "args = parser.parse_args(args = [])\n",
        "print(args, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWH4w-WiygGs",
        "outputId": "02a4ea84-d076-4c48-e703-6dcad320cda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(device=0, dropout=0.5, epochs=200, hidden_channels=512, lr=0.01, num_layers=3, runs=10, use_cached=False, use_embed=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PygNodePropPredDataset('ogbn-products',\n",
        "                                     root='./dataset/',\n",
        "                                     transform=T.ToSparseTensor())\n",
        "print(dataset, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhDONkUj1aMh",
        "outputId": "bac59389-bf3e-4a12-bcc3-bb8d9860a054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This will download 1.38GB. Will you proceed? (y/N)\n",
            "y\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 1.38 GB: 100%|██████████| 1414/1414 [01:57<00:00, 12.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/products.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1097.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PygNodePropPredDataset()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name='ogbn-products')"
      ],
      "metadata": {
        "id": "ytYSU8F11cCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "print(data, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClHgOyYy1jMP",
        "outputId": "ad3bde6e-5897-45b9-d676-3b96deb9c06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(num_nodes=2449029, x=[2449029, 100], y=[2449029, 1], adj_t=[2449029, 2449029, nnz=123718280])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the node ids distribution of train, test and val\n",
        "print('Number of training nodes:', split_idx['train'].size(0))\n",
        "print('Number of validation nodes:', split_idx['valid'].size(0))\n",
        "print('Number of test nodes:', split_idx['test'].size(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yokVSxQ12dn8",
        "outputId": "75d67645-fd93-4f6f-e923-57fbadc5bf2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training nodes: 196615\n",
            "Number of validation nodes: 39323\n",
            "Number of test nodes: 2213091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check some graph statistics of ogb-product graph\n",
        "print(\"Number of nodes in the graph:\", data.num_nodes)\n",
        "print(\"Number of edges in the graph:\", data.num_edges)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkEE1eik2rWW",
        "outputId": "3267b21e-c52d-4491-a310-eb6d69d904c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in the graph: 2449029\n",
            "Number of edges in the graph: 123718280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = torch.device(\"cuda:%d\" % args.device if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # generate and add embeddings\n",
        "    if args.use_embed:\n",
        "        if args.use_cached:\n",
        "            embeddings = torch.load('./embeddings/spectralproducts.pt', map_location='cpu')\n",
        "        else:\n",
        "            embeddings = spectral(data, 'products')\n",
        "        data.x = torch.cat([data.x, embeddings], dim=-1)\n",
        "\n",
        "    x, y = data.x.to(device), data.y.to(device)\n",
        "\n",
        "    # MLP-Wide\n",
        "    model = MLP(x.size(-1),\n",
        "                dataset.num_classes,\n",
        "                hidden_channels=args.hidden_channels,\n",
        "                num_layers=args.num_layers,\n",
        "                dropout=args.dropout).to(device)  \n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    train_idx = split_idx['train'].to(device)\n",
        "    val_idx = split_idx['valid'].to(device)\n",
        "    test_idx = split_idx['test'].to(device)\n",
        "    x_train, y_train = x[train_idx], y[train_idx]\n",
        "\n",
        "    logger = Logger(args.runs, args)\n",
        "\n",
        "    for run in range(args.runs):\n",
        "        model.reset_parameters()\n",
        "        print(sum(p.numel() for p in model.parameters()), flush=True)\n",
        "\n",
        "        print('', flush=True)\n",
        "        print(f'Run {run + 1:02d}:', flush=True)\n",
        "        print('', flush=True)\n",
        "\n",
        "        best_val_acc = 0\n",
        "        for epoch in range(1, args.epochs+ 1):  ##\n",
        "            loss = train(model, optimizer, x_train, criterion, y_train)\n",
        "            train_acc, val_acc, test_acc, out = test(model, x, evaluator, y, train_idx, val_idx, test_idx)\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                y_soft = out.softmax(dim=-1)\n",
        "                \n",
        "            print(\n",
        "                f'Run: {run + 1:02d}, '\n",
        "                f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "                f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}',\n",
        "                flush=True)\n",
        "\n",
        "        DAD, DA = process_adj(data, device)\n",
        "\n",
        "        post = CorrectAndSmooth(num_correction_layers=50,\n",
        "                                correction_alpha=1.0,\n",
        "                                num_smoothing_layers=50,\n",
        "                                smoothing_alpha=0.8,\n",
        "                                autoscale=False,\n",
        "                                scale=15.)\n",
        "\n",
        "        print('Correct and smooth...', flush=True)\n",
        "        y_soft = post.correct(y_soft, y_train, train_idx, DAD)\n",
        "        y_soft = post.smooth(y_soft, y_train, train_idx, DA)  \n",
        "        print('Done!', flush=True)\n",
        "\n",
        "        train_acc, val_acc, test_acc, _ = test(model, x, evaluator, y, \n",
        "                                               train_idx, val_idx, test_idx, \n",
        "                                               out=y_soft)\n",
        "        \n",
        "        print(\n",
        "            f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}',\n",
        "            flush=True)\n",
        "\n",
        "        result = (train_acc, val_acc, test_acc)\n",
        "        logger.add_result(run, result)\n",
        "\n",
        "    logger.print_statistics()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXPDU0IVvhy8",
        "outputId": "ee0dbc3e-f461-4da8-d537-44cf3a93e47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "340527\n",
            "\n",
            "Run 01:\n",
            "\n",
            "Run: 01, Epoch: 001, Loss: 4.0965, Train: 0.3553, Val: 0.3548, Test: 0.2958\n",
            "Run: 01, Epoch: 002, Loss: 3.0626, Train: 0.5128, Val: 0.5096, Test: 0.4229\n",
            "Run: 01, Epoch: 003, Loss: 2.2037, Train: 0.5161, Val: 0.5116, Test: 0.4232\n",
            "Run: 01, Epoch: 004, Loss: 1.9092, Train: 0.5791, Val: 0.5691, Test: 0.4633\n",
            "Run: 01, Epoch: 005, Loss: 1.7086, Train: 0.5960, Val: 0.5847, Test: 0.4742\n",
            "Run: 01, Epoch: 006, Loss: 1.6230, Train: 0.6143, Val: 0.6013, Test: 0.4861\n",
            "Run: 01, Epoch: 007, Loss: 1.5126, Train: 0.6240, Val: 0.6121, Test: 0.4920\n",
            "Run: 01, Epoch: 008, Loss: 1.4566, Train: 0.6331, Val: 0.6193, Test: 0.5001\n",
            "Run: 01, Epoch: 009, Loss: 1.3905, Train: 0.6359, Val: 0.6243, Test: 0.5053\n",
            "Run: 01, Epoch: 010, Loss: 1.3411, Train: 0.6325, Val: 0.6213, Test: 0.5047\n",
            "Run: 01, Epoch: 011, Loss: 1.3169, Train: 0.6305, Val: 0.6199, Test: 0.5024\n",
            "Run: 01, Epoch: 012, Loss: 1.2898, Train: 0.6317, Val: 0.6194, Test: 0.5007\n",
            "Run: 01, Epoch: 013, Loss: 1.2539, Train: 0.6329, Val: 0.6185, Test: 0.4989\n",
            "Run: 01, Epoch: 014, Loss: 1.2386, Train: 0.6330, Val: 0.6178, Test: 0.4976\n",
            "Run: 01, Epoch: 015, Loss: 1.2187, Train: 0.6361, Val: 0.6220, Test: 0.4995\n",
            "Run: 01, Epoch: 016, Loss: 1.1995, Train: 0.6435, Val: 0.6273, Test: 0.5053\n",
            "Run: 01, Epoch: 017, Loss: 1.1838, Train: 0.6501, Val: 0.6332, Test: 0.5117\n",
            "Run: 01, Epoch: 018, Loss: 1.1709, Train: 0.6578, Val: 0.6423, Test: 0.5183\n",
            "Run: 01, Epoch: 019, Loss: 1.1544, Train: 0.6652, Val: 0.6488, Test: 0.5247\n",
            "Run: 01, Epoch: 020, Loss: 1.1416, Train: 0.6737, Val: 0.6562, Test: 0.5303\n",
            "Run: 01, Epoch: 021, Loss: 1.1305, Train: 0.6810, Val: 0.6639, Test: 0.5356\n",
            "Run: 01, Epoch: 022, Loss: 1.1198, Train: 0.6883, Val: 0.6703, Test: 0.5411\n",
            "Run: 01, Epoch: 023, Loss: 1.1067, Train: 0.6945, Val: 0.6771, Test: 0.5464\n",
            "Run: 01, Epoch: 024, Loss: 1.0966, Train: 0.6995, Val: 0.6811, Test: 0.5504\n",
            "Run: 01, Epoch: 025, Loss: 1.0890, Train: 0.7033, Val: 0.6842, Test: 0.5530\n",
            "Run: 01, Epoch: 026, Loss: 1.0793, Train: 0.7061, Val: 0.6875, Test: 0.5545\n",
            "Run: 01, Epoch: 027, Loss: 1.0707, Train: 0.7090, Val: 0.6897, Test: 0.5564\n",
            "Run: 01, Epoch: 028, Loss: 1.0615, Train: 0.7121, Val: 0.6938, Test: 0.5585\n",
            "Run: 01, Epoch: 029, Loss: 1.0543, Train: 0.7149, Val: 0.6962, Test: 0.5607\n",
            "Run: 01, Epoch: 030, Loss: 1.0457, Train: 0.7178, Val: 0.6992, Test: 0.5627\n",
            "Run: 01, Epoch: 031, Loss: 1.0403, Train: 0.7209, Val: 0.7014, Test: 0.5646\n",
            "Run: 01, Epoch: 032, Loss: 1.0338, Train: 0.7234, Val: 0.7031, Test: 0.5664\n",
            "Run: 01, Epoch: 033, Loss: 1.0218, Train: 0.7255, Val: 0.7044, Test: 0.5681\n",
            "Run: 01, Epoch: 034, Loss: 1.0196, Train: 0.7280, Val: 0.7070, Test: 0.5699\n",
            "Run: 01, Epoch: 035, Loss: 1.0123, Train: 0.7298, Val: 0.7095, Test: 0.5715\n",
            "Run: 01, Epoch: 036, Loss: 1.0087, Train: 0.7310, Val: 0.7100, Test: 0.5723\n",
            "Run: 01, Epoch: 037, Loss: 1.0022, Train: 0.7316, Val: 0.7108, Test: 0.5723\n",
            "Run: 01, Epoch: 038, Loss: 0.9968, Train: 0.7328, Val: 0.7115, Test: 0.5718\n",
            "Run: 01, Epoch: 039, Loss: 0.9904, Train: 0.7351, Val: 0.7123, Test: 0.5716\n",
            "Run: 01, Epoch: 040, Loss: 0.9875, Train: 0.7373, Val: 0.7134, Test: 0.5726\n",
            "Run: 01, Epoch: 041, Loss: 0.9817, Train: 0.7396, Val: 0.7161, Test: 0.5752\n",
            "Run: 01, Epoch: 042, Loss: 0.9749, Train: 0.7465, Val: 0.7237, Test: 0.5817\n",
            "Run: 01, Epoch: 043, Loss: 0.9710, Train: 0.7479, Val: 0.7261, Test: 0.5837\n",
            "Run: 01, Epoch: 044, Loss: 0.9673, Train: 0.7486, Val: 0.7265, Test: 0.5838\n",
            "Run: 01, Epoch: 045, Loss: 0.9646, Train: 0.7494, Val: 0.7270, Test: 0.5832\n",
            "Run: 01, Epoch: 046, Loss: 0.9601, Train: 0.7465, Val: 0.7224, Test: 0.5810\n",
            "Run: 01, Epoch: 047, Loss: 0.9572, Train: 0.7534, Val: 0.7301, Test: 0.5864\n",
            "Run: 01, Epoch: 048, Loss: 0.9523, Train: 0.7563, Val: 0.7326, Test: 0.5888\n",
            "Run: 01, Epoch: 049, Loss: 0.9492, Train: 0.7580, Val: 0.7337, Test: 0.5897\n",
            "Run: 01, Epoch: 050, Loss: 0.9448, Train: 0.7589, Val: 0.7333, Test: 0.5893\n",
            "Run: 01, Epoch: 051, Loss: 0.9413, Train: 0.7595, Val: 0.7329, Test: 0.5888\n",
            "Run: 01, Epoch: 052, Loss: 0.9402, Train: 0.7607, Val: 0.7344, Test: 0.5896\n",
            "Run: 01, Epoch: 053, Loss: 0.9358, Train: 0.7622, Val: 0.7360, Test: 0.5912\n",
            "Run: 01, Epoch: 054, Loss: 0.9314, Train: 0.7637, Val: 0.7369, Test: 0.5925\n",
            "Run: 01, Epoch: 055, Loss: 0.9279, Train: 0.7647, Val: 0.7376, Test: 0.5933\n",
            "Run: 01, Epoch: 056, Loss: 0.9237, Train: 0.7657, Val: 0.7386, Test: 0.5935\n",
            "Run: 01, Epoch: 057, Loss: 0.9248, Train: 0.7664, Val: 0.7391, Test: 0.5938\n",
            "Run: 01, Epoch: 058, Loss: 0.9174, Train: 0.7676, Val: 0.7394, Test: 0.5942\n",
            "Run: 01, Epoch: 059, Loss: 0.9123, Train: 0.7686, Val: 0.7408, Test: 0.5948\n",
            "Run: 01, Epoch: 060, Loss: 0.9121, Train: 0.7693, Val: 0.7404, Test: 0.5952\n",
            "Run: 01, Epoch: 061, Loss: 0.9105, Train: 0.7700, Val: 0.7412, Test: 0.5955\n",
            "Run: 01, Epoch: 062, Loss: 0.9074, Train: 0.7711, Val: 0.7412, Test: 0.5955\n",
            "Run: 01, Epoch: 063, Loss: 0.9061, Train: 0.7721, Val: 0.7432, Test: 0.5958\n",
            "Run: 01, Epoch: 064, Loss: 0.9051, Train: 0.7730, Val: 0.7430, Test: 0.5967\n",
            "Run: 01, Epoch: 065, Loss: 0.9006, Train: 0.7739, Val: 0.7431, Test: 0.5974\n",
            "Run: 01, Epoch: 066, Loss: 0.8980, Train: 0.7749, Val: 0.7437, Test: 0.5983\n",
            "Run: 01, Epoch: 067, Loss: 0.8961, Train: 0.7767, Val: 0.7450, Test: 0.5995\n",
            "Run: 01, Epoch: 068, Loss: 0.8933, Train: 0.7776, Val: 0.7461, Test: 0.6002\n",
            "Run: 01, Epoch: 069, Loss: 0.8914, Train: 0.7778, Val: 0.7461, Test: 0.5998\n",
            "Run: 01, Epoch: 070, Loss: 0.8896, Train: 0.7782, Val: 0.7458, Test: 0.5997\n",
            "Run: 01, Epoch: 071, Loss: 0.8852, Train: 0.7792, Val: 0.7463, Test: 0.6005\n",
            "Run: 01, Epoch: 072, Loss: 0.8857, Train: 0.7806, Val: 0.7478, Test: 0.6017\n",
            "Run: 01, Epoch: 073, Loss: 0.8826, Train: 0.7813, Val: 0.7492, Test: 0.6022\n",
            "Run: 01, Epoch: 074, Loss: 0.8809, Train: 0.7824, Val: 0.7498, Test: 0.6021\n",
            "Run: 01, Epoch: 075, Loss: 0.8782, Train: 0.7832, Val: 0.7503, Test: 0.6020\n",
            "Run: 01, Epoch: 076, Loss: 0.8747, Train: 0.7837, Val: 0.7504, Test: 0.6021\n",
            "Run: 01, Epoch: 077, Loss: 0.8727, Train: 0.7838, Val: 0.7495, Test: 0.6022\n",
            "Run: 01, Epoch: 078, Loss: 0.8720, Train: 0.7836, Val: 0.7493, Test: 0.6024\n",
            "Run: 01, Epoch: 079, Loss: 0.8699, Train: 0.7851, Val: 0.7498, Test: 0.6034\n",
            "Run: 01, Epoch: 080, Loss: 0.8656, Train: 0.7864, Val: 0.7514, Test: 0.6039\n",
            "Run: 01, Epoch: 081, Loss: 0.8637, Train: 0.7876, Val: 0.7519, Test: 0.6041\n",
            "Run: 01, Epoch: 082, Loss: 0.8620, Train: 0.7881, Val: 0.7511, Test: 0.6043\n",
            "Run: 01, Epoch: 083, Loss: 0.8635, Train: 0.7889, Val: 0.7524, Test: 0.6050\n",
            "Run: 01, Epoch: 084, Loss: 0.8613, Train: 0.7889, Val: 0.7528, Test: 0.6052\n",
            "Run: 01, Epoch: 085, Loss: 0.8585, Train: 0.7896, Val: 0.7525, Test: 0.6054\n",
            "Run: 01, Epoch: 086, Loss: 0.8576, Train: 0.7905, Val: 0.7534, Test: 0.6062\n",
            "Run: 01, Epoch: 087, Loss: 0.8557, Train: 0.7909, Val: 0.7537, Test: 0.6060\n",
            "Run: 01, Epoch: 088, Loss: 0.8531, Train: 0.7913, Val: 0.7529, Test: 0.6051\n",
            "Run: 01, Epoch: 089, Loss: 0.8535, Train: 0.7919, Val: 0.7532, Test: 0.6056\n",
            "Run: 01, Epoch: 090, Loss: 0.8507, Train: 0.7934, Val: 0.7544, Test: 0.6071\n",
            "Run: 01, Epoch: 091, Loss: 0.8493, Train: 0.7936, Val: 0.7553, Test: 0.6072\n",
            "Run: 01, Epoch: 092, Loss: 0.8471, Train: 0.7943, Val: 0.7540, Test: 0.6064\n",
            "Run: 01, Epoch: 093, Loss: 0.8439, Train: 0.7947, Val: 0.7535, Test: 0.6058\n",
            "Run: 01, Epoch: 094, Loss: 0.8440, Train: 0.7949, Val: 0.7540, Test: 0.6061\n",
            "Run: 01, Epoch: 095, Loss: 0.8408, Train: 0.7952, Val: 0.7553, Test: 0.6076\n",
            "Run: 01, Epoch: 096, Loss: 0.8387, Train: 0.7965, Val: 0.7564, Test: 0.6091\n",
            "Run: 01, Epoch: 097, Loss: 0.8394, Train: 0.7977, Val: 0.7562, Test: 0.6096\n",
            "Run: 01, Epoch: 098, Loss: 0.8333, Train: 0.7982, Val: 0.7560, Test: 0.6093\n",
            "Run: 01, Epoch: 099, Loss: 0.8353, Train: 0.7983, Val: 0.7575, Test: 0.6084\n",
            "Run: 01, Epoch: 100, Loss: 0.8346, Train: 0.7983, Val: 0.7572, Test: 0.6082\n",
            "Run: 01, Epoch: 101, Loss: 0.8335, Train: 0.7986, Val: 0.7567, Test: 0.6082\n",
            "Run: 01, Epoch: 102, Loss: 0.8317, Train: 0.7993, Val: 0.7563, Test: 0.6088\n",
            "Run: 01, Epoch: 103, Loss: 0.8291, Train: 0.8009, Val: 0.7586, Test: 0.6102\n",
            "Run: 01, Epoch: 104, Loss: 0.8259, Train: 0.8016, Val: 0.7585, Test: 0.6106\n",
            "Run: 01, Epoch: 105, Loss: 0.8278, Train: 0.8017, Val: 0.7578, Test: 0.6105\n",
            "Run: 01, Epoch: 106, Loss: 0.8263, Train: 0.8016, Val: 0.7579, Test: 0.6110\n",
            "Run: 01, Epoch: 107, Loss: 0.8242, Train: 0.8018, Val: 0.7583, Test: 0.6107\n",
            "Run: 01, Epoch: 108, Loss: 0.8243, Train: 0.8025, Val: 0.7580, Test: 0.6099\n",
            "Run: 01, Epoch: 109, Loss: 0.8221, Train: 0.8037, Val: 0.7599, Test: 0.6100\n",
            "Run: 01, Epoch: 110, Loss: 0.8213, Train: 0.8040, Val: 0.7603, Test: 0.6101\n",
            "Run: 01, Epoch: 111, Loss: 0.8176, Train: 0.8040, Val: 0.7591, Test: 0.6110\n",
            "Run: 01, Epoch: 112, Loss: 0.8173, Train: 0.8043, Val: 0.7595, Test: 0.6119\n",
            "Run: 01, Epoch: 113, Loss: 0.8163, Train: 0.8051, Val: 0.7598, Test: 0.6117\n",
            "Run: 01, Epoch: 114, Loss: 0.8132, Train: 0.8058, Val: 0.7609, Test: 0.6115\n",
            "Run: 01, Epoch: 115, Loss: 0.8157, Train: 0.8065, Val: 0.7616, Test: 0.6121\n",
            "Run: 01, Epoch: 116, Loss: 0.8121, Train: 0.8066, Val: 0.7616, Test: 0.6122\n",
            "Run: 01, Epoch: 117, Loss: 0.8113, Train: 0.8068, Val: 0.7614, Test: 0.6119\n",
            "Run: 01, Epoch: 118, Loss: 0.8121, Train: 0.8075, Val: 0.7622, Test: 0.6123\n",
            "Run: 01, Epoch: 119, Loss: 0.8106, Train: 0.8079, Val: 0.7625, Test: 0.6124\n",
            "Run: 01, Epoch: 120, Loss: 0.8083, Train: 0.8076, Val: 0.7619, Test: 0.6115\n",
            "Run: 01, Epoch: 121, Loss: 0.8060, Train: 0.8079, Val: 0.7613, Test: 0.6114\n",
            "Run: 01, Epoch: 122, Loss: 0.8076, Train: 0.8080, Val: 0.7607, Test: 0.6115\n",
            "Run: 01, Epoch: 123, Loss: 0.8044, Train: 0.8085, Val: 0.7624, Test: 0.6123\n",
            "Run: 01, Epoch: 124, Loss: 0.8034, Train: 0.8091, Val: 0.7615, Test: 0.6124\n",
            "Run: 01, Epoch: 125, Loss: 0.8027, Train: 0.8094, Val: 0.7614, Test: 0.6125\n",
            "Run: 01, Epoch: 126, Loss: 0.8015, Train: 0.8096, Val: 0.7614, Test: 0.6131\n",
            "Run: 01, Epoch: 127, Loss: 0.8020, Train: 0.8102, Val: 0.7621, Test: 0.6131\n",
            "Run: 01, Epoch: 128, Loss: 0.8004, Train: 0.8110, Val: 0.7625, Test: 0.6127\n",
            "Run: 01, Epoch: 129, Loss: 0.7975, Train: 0.8116, Val: 0.7624, Test: 0.6129\n",
            "Run: 01, Epoch: 130, Loss: 0.7994, Train: 0.8119, Val: 0.7630, Test: 0.6132\n",
            "Run: 01, Epoch: 131, Loss: 0.7968, Train: 0.8121, Val: 0.7620, Test: 0.6133\n",
            "Run: 01, Epoch: 132, Loss: 0.7941, Train: 0.8124, Val: 0.7634, Test: 0.6140\n",
            "Run: 01, Epoch: 133, Loss: 0.7943, Train: 0.8126, Val: 0.7628, Test: 0.6139\n",
            "Run: 01, Epoch: 134, Loss: 0.7933, Train: 0.8136, Val: 0.7640, Test: 0.6146\n",
            "Run: 01, Epoch: 135, Loss: 0.7941, Train: 0.8146, Val: 0.7657, Test: 0.6147\n",
            "Run: 01, Epoch: 136, Loss: 0.7924, Train: 0.8145, Val: 0.7644, Test: 0.6143\n",
            "Run: 01, Epoch: 137, Loss: 0.7897, Train: 0.8145, Val: 0.7647, Test: 0.6150\n",
            "Run: 01, Epoch: 138, Loss: 0.7890, Train: 0.8146, Val: 0.7636, Test: 0.6151\n",
            "Run: 01, Epoch: 139, Loss: 0.7897, Train: 0.8156, Val: 0.7640, Test: 0.6153\n",
            "Run: 01, Epoch: 140, Loss: 0.7856, Train: 0.8165, Val: 0.7654, Test: 0.6160\n",
            "Run: 01, Epoch: 141, Loss: 0.7869, Train: 0.8165, Val: 0.7657, Test: 0.6158\n",
            "Run: 01, Epoch: 142, Loss: 0.7867, Train: 0.8161, Val: 0.7647, Test: 0.6150\n",
            "Run: 01, Epoch: 143, Loss: 0.7850, Train: 0.8163, Val: 0.7648, Test: 0.6153\n",
            "Run: 01, Epoch: 144, Loss: 0.7855, Train: 0.8170, Val: 0.7647, Test: 0.6152\n",
            "Run: 01, Epoch: 145, Loss: 0.7830, Train: 0.8178, Val: 0.7652, Test: 0.6156\n",
            "Run: 01, Epoch: 146, Loss: 0.7811, Train: 0.8177, Val: 0.7668, Test: 0.6161\n",
            "Run: 01, Epoch: 147, Loss: 0.7804, Train: 0.8171, Val: 0.7653, Test: 0.6157\n",
            "Run: 01, Epoch: 148, Loss: 0.7809, Train: 0.8182, Val: 0.7659, Test: 0.6166\n",
            "Run: 01, Epoch: 149, Loss: 0.7778, Train: 0.8186, Val: 0.7666, Test: 0.6159\n",
            "Run: 01, Epoch: 150, Loss: 0.7778, Train: 0.8186, Val: 0.7662, Test: 0.6154\n",
            "Run: 01, Epoch: 151, Loss: 0.7779, Train: 0.8192, Val: 0.7674, Test: 0.6161\n",
            "Run: 01, Epoch: 152, Loss: 0.7780, Train: 0.8190, Val: 0.7679, Test: 0.6162\n",
            "Run: 01, Epoch: 153, Loss: 0.7769, Train: 0.8198, Val: 0.7678, Test: 0.6168\n",
            "Run: 01, Epoch: 154, Loss: 0.7774, Train: 0.8201, Val: 0.7679, Test: 0.6161\n",
            "Run: 01, Epoch: 155, Loss: 0.7737, Train: 0.8202, Val: 0.7678, Test: 0.6163\n",
            "Run: 01, Epoch: 156, Loss: 0.7744, Train: 0.8205, Val: 0.7675, Test: 0.6168\n",
            "Run: 01, Epoch: 157, Loss: 0.7718, Train: 0.8205, Val: 0.7683, Test: 0.6172\n",
            "Run: 01, Epoch: 158, Loss: 0.7749, Train: 0.8213, Val: 0.7687, Test: 0.6169\n",
            "Run: 01, Epoch: 159, Loss: 0.7708, Train: 0.8217, Val: 0.7692, Test: 0.6167\n",
            "Run: 01, Epoch: 160, Loss: 0.7716, Train: 0.8220, Val: 0.7692, Test: 0.6168\n",
            "Run: 01, Epoch: 161, Loss: 0.7700, Train: 0.8213, Val: 0.7681, Test: 0.6164\n",
            "Run: 01, Epoch: 162, Loss: 0.7673, Train: 0.8213, Val: 0.7683, Test: 0.6166\n",
            "Run: 01, Epoch: 163, Loss: 0.7676, Train: 0.8222, Val: 0.7687, Test: 0.6169\n",
            "Run: 01, Epoch: 164, Loss: 0.7674, Train: 0.8230, Val: 0.7686, Test: 0.6167\n",
            "Run: 01, Epoch: 165, Loss: 0.7654, Train: 0.8241, Val: 0.7690, Test: 0.6178\n",
            "Run: 01, Epoch: 166, Loss: 0.7664, Train: 0.8240, Val: 0.7687, Test: 0.6181\n",
            "Run: 01, Epoch: 167, Loss: 0.7657, Train: 0.8242, Val: 0.7693, Test: 0.6183\n",
            "Run: 01, Epoch: 168, Loss: 0.7662, Train: 0.8243, Val: 0.7685, Test: 0.6174\n",
            "Run: 01, Epoch: 169, Loss: 0.7650, Train: 0.8240, Val: 0.7687, Test: 0.6159\n",
            "Run: 01, Epoch: 170, Loss: 0.7619, Train: 0.8246, Val: 0.7691, Test: 0.6166\n",
            "Run: 01, Epoch: 171, Loss: 0.7639, Train: 0.8246, Val: 0.7696, Test: 0.6170\n",
            "Run: 01, Epoch: 172, Loss: 0.7615, Train: 0.8256, Val: 0.7701, Test: 0.6181\n",
            "Run: 01, Epoch: 173, Loss: 0.7589, Train: 0.8261, Val: 0.7699, Test: 0.6179\n",
            "Run: 01, Epoch: 174, Loss: 0.7622, Train: 0.8264, Val: 0.7705, Test: 0.6182\n",
            "Run: 01, Epoch: 175, Loss: 0.7589, Train: 0.8263, Val: 0.7703, Test: 0.6183\n",
            "Run: 01, Epoch: 176, Loss: 0.7589, Train: 0.8259, Val: 0.7704, Test: 0.6171\n",
            "Run: 01, Epoch: 177, Loss: 0.7601, Train: 0.8265, Val: 0.7698, Test: 0.6172\n",
            "Run: 01, Epoch: 178, Loss: 0.7598, Train: 0.8265, Val: 0.7696, Test: 0.6173\n",
            "Run: 01, Epoch: 179, Loss: 0.7566, Train: 0.8272, Val: 0.7700, Test: 0.6176\n",
            "Run: 01, Epoch: 180, Loss: 0.7563, Train: 0.8278, Val: 0.7704, Test: 0.6182\n",
            "Run: 01, Epoch: 181, Loss: 0.7535, Train: 0.8278, Val: 0.7712, Test: 0.6185\n",
            "Run: 01, Epoch: 182, Loss: 0.7525, Train: 0.8281, Val: 0.7718, Test: 0.6180\n",
            "Run: 01, Epoch: 183, Loss: 0.7537, Train: 0.8279, Val: 0.7716, Test: 0.6173\n",
            "Run: 01, Epoch: 184, Loss: 0.7576, Train: 0.8284, Val: 0.7722, Test: 0.6180\n",
            "Run: 01, Epoch: 185, Loss: 0.7522, Train: 0.8283, Val: 0.7716, Test: 0.6180\n",
            "Run: 01, Epoch: 186, Loss: 0.7509, Train: 0.8289, Val: 0.7714, Test: 0.6182\n",
            "Run: 01, Epoch: 187, Loss: 0.7537, Train: 0.8297, Val: 0.7725, Test: 0.6190\n",
            "Run: 01, Epoch: 188, Loss: 0.7494, Train: 0.8298, Val: 0.7726, Test: 0.6193\n",
            "Run: 01, Epoch: 189, Loss: 0.7494, Train: 0.8298, Val: 0.7725, Test: 0.6195\n",
            "Run: 01, Epoch: 190, Loss: 0.7496, Train: 0.8293, Val: 0.7720, Test: 0.6190\n",
            "Run: 01, Epoch: 191, Loss: 0.7488, Train: 0.8303, Val: 0.7723, Test: 0.6192\n",
            "Run: 01, Epoch: 192, Loss: 0.7486, Train: 0.8300, Val: 0.7718, Test: 0.6180\n",
            "Run: 01, Epoch: 193, Loss: 0.7485, Train: 0.8306, Val: 0.7720, Test: 0.6189\n",
            "Run: 01, Epoch: 194, Loss: 0.7500, Train: 0.8307, Val: 0.7723, Test: 0.6193\n",
            "Run: 01, Epoch: 195, Loss: 0.7480, Train: 0.8310, Val: 0.7721, Test: 0.6192\n",
            "Run: 01, Epoch: 196, Loss: 0.7464, Train: 0.8311, Val: 0.7717, Test: 0.6190\n",
            "Run: 01, Epoch: 197, Loss: 0.7430, Train: 0.8317, Val: 0.7725, Test: 0.6190\n",
            "Run: 01, Epoch: 198, Loss: 0.7449, Train: 0.8317, Val: 0.7722, Test: 0.6188\n",
            "Run: 01, Epoch: 199, Loss: 0.7437, Train: 0.8315, Val: 0.7717, Test: 0.6189\n",
            "Run: 01, Epoch: 200, Loss: 0.7424, Train: 0.8324, Val: 0.7725, Test: 0.6190\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9460, Val: 0.9126, Test: 0.8413\n",
            "340527\n",
            "\n",
            "Run 02:\n",
            "\n",
            "Run: 02, Epoch: 001, Loss: 4.0579, Train: 0.3722, Val: 0.3723, Test: 0.3148\n",
            "Run: 02, Epoch: 002, Loss: 3.0235, Train: 0.5211, Val: 0.5172, Test: 0.4295\n",
            "Run: 02, Epoch: 003, Loss: 2.1376, Train: 0.4992, Val: 0.4937, Test: 0.4268\n",
            "Run: 02, Epoch: 004, Loss: 1.9012, Train: 0.5557, Val: 0.5485, Test: 0.4445\n",
            "Run: 02, Epoch: 005, Loss: 1.7301, Train: 0.5731, Val: 0.5656, Test: 0.4633\n",
            "Run: 02, Epoch: 006, Loss: 1.6639, Train: 0.6041, Val: 0.5915, Test: 0.4793\n",
            "Run: 02, Epoch: 007, Loss: 1.5204, Train: 0.6235, Val: 0.6107, Test: 0.4973\n",
            "Run: 02, Epoch: 008, Loss: 1.4753, Train: 0.6406, Val: 0.6296, Test: 0.5125\n",
            "Run: 02, Epoch: 009, Loss: 1.3917, Train: 0.6408, Val: 0.6297, Test: 0.5141\n",
            "Run: 02, Epoch: 010, Loss: 1.3515, Train: 0.6374, Val: 0.6290, Test: 0.5103\n",
            "Run: 02, Epoch: 011, Loss: 1.3251, Train: 0.6448, Val: 0.6354, Test: 0.5136\n",
            "Run: 02, Epoch: 012, Loss: 1.2894, Train: 0.6495, Val: 0.6370, Test: 0.5148\n",
            "Run: 02, Epoch: 013, Loss: 1.2625, Train: 0.6524, Val: 0.6378, Test: 0.5175\n",
            "Run: 02, Epoch: 014, Loss: 1.2451, Train: 0.6568, Val: 0.6414, Test: 0.5212\n",
            "Run: 02, Epoch: 015, Loss: 1.2271, Train: 0.6670, Val: 0.6539, Test: 0.5311\n",
            "Run: 02, Epoch: 016, Loss: 1.2031, Train: 0.6745, Val: 0.6598, Test: 0.5365\n",
            "Run: 02, Epoch: 017, Loss: 1.1901, Train: 0.6765, Val: 0.6635, Test: 0.5380\n",
            "Run: 02, Epoch: 018, Loss: 1.1709, Train: 0.6802, Val: 0.6669, Test: 0.5392\n",
            "Run: 02, Epoch: 019, Loss: 1.1604, Train: 0.6873, Val: 0.6720, Test: 0.5440\n",
            "Run: 02, Epoch: 020, Loss: 1.1468, Train: 0.6966, Val: 0.6805, Test: 0.5507\n",
            "Run: 02, Epoch: 021, Loss: 1.1299, Train: 0.7021, Val: 0.6860, Test: 0.5559\n",
            "Run: 02, Epoch: 022, Loss: 1.1224, Train: 0.7059, Val: 0.6910, Test: 0.5602\n",
            "Run: 02, Epoch: 023, Loss: 1.1119, Train: 0.7080, Val: 0.6927, Test: 0.5624\n",
            "Run: 02, Epoch: 024, Loss: 1.0999, Train: 0.7095, Val: 0.6946, Test: 0.5630\n",
            "Run: 02, Epoch: 025, Loss: 1.0897, Train: 0.7110, Val: 0.6952, Test: 0.5623\n",
            "Run: 02, Epoch: 026, Loss: 1.0817, Train: 0.7130, Val: 0.6961, Test: 0.5620\n",
            "Run: 02, Epoch: 027, Loss: 1.0751, Train: 0.7170, Val: 0.6996, Test: 0.5640\n",
            "Run: 02, Epoch: 028, Loss: 1.0662, Train: 0.7201, Val: 0.7025, Test: 0.5662\n",
            "Run: 02, Epoch: 029, Loss: 1.0575, Train: 0.7226, Val: 0.7032, Test: 0.5686\n",
            "Run: 02, Epoch: 030, Loss: 1.0497, Train: 0.7244, Val: 0.7054, Test: 0.5702\n",
            "Run: 02, Epoch: 031, Loss: 1.0433, Train: 0.7256, Val: 0.7069, Test: 0.5705\n",
            "Run: 02, Epoch: 032, Loss: 1.0385, Train: 0.7270, Val: 0.7071, Test: 0.5716\n",
            "Run: 02, Epoch: 033, Loss: 1.0322, Train: 0.7281, Val: 0.7095, Test: 0.5725\n",
            "Run: 02, Epoch: 034, Loss: 1.0245, Train: 0.7285, Val: 0.7094, Test: 0.5727\n",
            "Run: 02, Epoch: 035, Loss: 1.0225, Train: 0.7296, Val: 0.7107, Test: 0.5729\n",
            "Run: 02, Epoch: 036, Loss: 1.0167, Train: 0.7310, Val: 0.7119, Test: 0.5734\n",
            "Run: 02, Epoch: 037, Loss: 1.0105, Train: 0.7323, Val: 0.7129, Test: 0.5746\n",
            "Run: 02, Epoch: 038, Loss: 1.0056, Train: 0.7334, Val: 0.7139, Test: 0.5758\n",
            "Run: 02, Epoch: 039, Loss: 1.0024, Train: 0.7341, Val: 0.7143, Test: 0.5765\n",
            "Run: 02, Epoch: 040, Loss: 0.9961, Train: 0.7350, Val: 0.7159, Test: 0.5771\n",
            "Run: 02, Epoch: 041, Loss: 0.9931, Train: 0.7367, Val: 0.7159, Test: 0.5772\n",
            "Run: 02, Epoch: 042, Loss: 0.9887, Train: 0.7374, Val: 0.7165, Test: 0.5768\n",
            "Run: 02, Epoch: 043, Loss: 0.9842, Train: 0.7377, Val: 0.7171, Test: 0.5771\n",
            "Run: 02, Epoch: 044, Loss: 0.9809, Train: 0.7390, Val: 0.7184, Test: 0.5785\n",
            "Run: 02, Epoch: 045, Loss: 0.9789, Train: 0.7407, Val: 0.7204, Test: 0.5799\n",
            "Run: 02, Epoch: 046, Loss: 0.9759, Train: 0.7469, Val: 0.7271, Test: 0.5845\n",
            "Run: 02, Epoch: 047, Loss: 0.9731, Train: 0.7483, Val: 0.7279, Test: 0.5855\n",
            "Run: 02, Epoch: 048, Loss: 0.9688, Train: 0.7490, Val: 0.7281, Test: 0.5851\n",
            "Run: 02, Epoch: 049, Loss: 0.9625, Train: 0.7490, Val: 0.7286, Test: 0.5841\n",
            "Run: 02, Epoch: 050, Loss: 0.9623, Train: 0.7495, Val: 0.7291, Test: 0.5843\n",
            "Run: 02, Epoch: 051, Loss: 0.9601, Train: 0.7500, Val: 0.7294, Test: 0.5846\n",
            "Run: 02, Epoch: 052, Loss: 0.9554, Train: 0.7513, Val: 0.7301, Test: 0.5859\n",
            "Run: 02, Epoch: 053, Loss: 0.9536, Train: 0.7527, Val: 0.7304, Test: 0.5876\n",
            "Run: 02, Epoch: 054, Loss: 0.9508, Train: 0.7544, Val: 0.7316, Test: 0.5894\n",
            "Run: 02, Epoch: 055, Loss: 0.9487, Train: 0.7553, Val: 0.7325, Test: 0.5907\n",
            "Run: 02, Epoch: 056, Loss: 0.9473, Train: 0.7559, Val: 0.7337, Test: 0.5914\n",
            "Run: 02, Epoch: 057, Loss: 0.9449, Train: 0.7570, Val: 0.7344, Test: 0.5913\n",
            "Run: 02, Epoch: 058, Loss: 0.9391, Train: 0.7577, Val: 0.7351, Test: 0.5908\n",
            "Run: 02, Epoch: 059, Loss: 0.9389, Train: 0.7583, Val: 0.7354, Test: 0.5906\n",
            "Run: 02, Epoch: 060, Loss: 0.9352, Train: 0.7587, Val: 0.7351, Test: 0.5906\n",
            "Run: 02, Epoch: 061, Loss: 0.9331, Train: 0.7596, Val: 0.7358, Test: 0.5914\n",
            "Run: 02, Epoch: 062, Loss: 0.9324, Train: 0.7604, Val: 0.7364, Test: 0.5929\n",
            "Run: 02, Epoch: 063, Loss: 0.9282, Train: 0.7614, Val: 0.7373, Test: 0.5942\n",
            "Run: 02, Epoch: 064, Loss: 0.9282, Train: 0.7627, Val: 0.7386, Test: 0.5951\n",
            "Run: 02, Epoch: 065, Loss: 0.9264, Train: 0.7638, Val: 0.7391, Test: 0.5952\n",
            "Run: 02, Epoch: 066, Loss: 0.9234, Train: 0.7641, Val: 0.7392, Test: 0.5949\n",
            "Run: 02, Epoch: 067, Loss: 0.9183, Train: 0.7646, Val: 0.7395, Test: 0.5948\n",
            "Run: 02, Epoch: 068, Loss: 0.9177, Train: 0.7654, Val: 0.7405, Test: 0.5953\n",
            "Run: 02, Epoch: 069, Loss: 0.9166, Train: 0.7659, Val: 0.7404, Test: 0.5958\n",
            "Run: 02, Epoch: 070, Loss: 0.9145, Train: 0.7672, Val: 0.7417, Test: 0.5969\n",
            "Run: 02, Epoch: 071, Loss: 0.9147, Train: 0.7680, Val: 0.7421, Test: 0.5974\n",
            "Run: 02, Epoch: 072, Loss: 0.9068, Train: 0.7687, Val: 0.7415, Test: 0.5971\n",
            "Run: 02, Epoch: 073, Loss: 0.9095, Train: 0.7692, Val: 0.7413, Test: 0.5969\n",
            "Run: 02, Epoch: 074, Loss: 0.9106, Train: 0.7702, Val: 0.7435, Test: 0.5980\n",
            "Run: 02, Epoch: 075, Loss: 0.9048, Train: 0.7711, Val: 0.7446, Test: 0.5992\n",
            "Run: 02, Epoch: 076, Loss: 0.9039, Train: 0.7718, Val: 0.7450, Test: 0.5997\n",
            "Run: 02, Epoch: 077, Loss: 0.9038, Train: 0.7722, Val: 0.7440, Test: 0.5992\n",
            "Run: 02, Epoch: 078, Loss: 0.8983, Train: 0.7722, Val: 0.7428, Test: 0.5985\n",
            "Run: 02, Epoch: 079, Loss: 0.8976, Train: 0.7728, Val: 0.7436, Test: 0.5986\n",
            "Run: 02, Epoch: 080, Loss: 0.8952, Train: 0.7738, Val: 0.7454, Test: 0.5994\n",
            "Run: 02, Epoch: 081, Loss: 0.8980, Train: 0.7748, Val: 0.7461, Test: 0.5999\n",
            "Run: 02, Epoch: 082, Loss: 0.8944, Train: 0.7750, Val: 0.7458, Test: 0.6000\n",
            "Run: 02, Epoch: 083, Loss: 0.8924, Train: 0.7754, Val: 0.7456, Test: 0.5997\n",
            "Run: 02, Epoch: 084, Loss: 0.8912, Train: 0.7760, Val: 0.7456, Test: 0.6001\n",
            "Run: 02, Epoch: 085, Loss: 0.8903, Train: 0.7776, Val: 0.7471, Test: 0.6012\n",
            "Run: 02, Epoch: 086, Loss: 0.8853, Train: 0.7784, Val: 0.7477, Test: 0.6018\n",
            "Run: 02, Epoch: 087, Loss: 0.8865, Train: 0.7784, Val: 0.7478, Test: 0.6016\n",
            "Run: 02, Epoch: 088, Loss: 0.8857, Train: 0.7786, Val: 0.7472, Test: 0.6012\n",
            "Run: 02, Epoch: 089, Loss: 0.8827, Train: 0.7796, Val: 0.7468, Test: 0.6015\n",
            "Run: 02, Epoch: 090, Loss: 0.8797, Train: 0.7805, Val: 0.7477, Test: 0.6021\n",
            "Run: 02, Epoch: 091, Loss: 0.8798, Train: 0.7808, Val: 0.7484, Test: 0.6028\n",
            "Run: 02, Epoch: 092, Loss: 0.8786, Train: 0.7815, Val: 0.7489, Test: 0.6029\n",
            "Run: 02, Epoch: 093, Loss: 0.8786, Train: 0.7815, Val: 0.7493, Test: 0.6031\n",
            "Run: 02, Epoch: 094, Loss: 0.8767, Train: 0.7828, Val: 0.7500, Test: 0.6036\n",
            "Run: 02, Epoch: 095, Loss: 0.8752, Train: 0.7838, Val: 0.7503, Test: 0.6046\n",
            "Run: 02, Epoch: 096, Loss: 0.8754, Train: 0.7841, Val: 0.7510, Test: 0.6049\n",
            "Run: 02, Epoch: 097, Loss: 0.8729, Train: 0.7838, Val: 0.7509, Test: 0.6044\n",
            "Run: 02, Epoch: 098, Loss: 0.8711, Train: 0.7838, Val: 0.7509, Test: 0.6041\n",
            "Run: 02, Epoch: 099, Loss: 0.8702, Train: 0.7849, Val: 0.7515, Test: 0.6042\n",
            "Run: 02, Epoch: 100, Loss: 0.8685, Train: 0.7854, Val: 0.7520, Test: 0.6040\n",
            "Run: 02, Epoch: 101, Loss: 0.8681, Train: 0.7858, Val: 0.7523, Test: 0.6041\n",
            "Run: 02, Epoch: 102, Loss: 0.8637, Train: 0.7865, Val: 0.7531, Test: 0.6051\n",
            "Run: 02, Epoch: 103, Loss: 0.8648, Train: 0.7870, Val: 0.7537, Test: 0.6058\n",
            "Run: 02, Epoch: 104, Loss: 0.8625, Train: 0.7880, Val: 0.7546, Test: 0.6066\n",
            "Run: 02, Epoch: 105, Loss: 0.8604, Train: 0.7881, Val: 0.7545, Test: 0.6069\n",
            "Run: 02, Epoch: 106, Loss: 0.8615, Train: 0.7882, Val: 0.7540, Test: 0.6067\n",
            "Run: 02, Epoch: 107, Loss: 0.8599, Train: 0.7885, Val: 0.7543, Test: 0.6067\n",
            "Run: 02, Epoch: 108, Loss: 0.8581, Train: 0.7895, Val: 0.7550, Test: 0.6070\n",
            "Run: 02, Epoch: 109, Loss: 0.8567, Train: 0.7904, Val: 0.7553, Test: 0.6070\n",
            "Run: 02, Epoch: 110, Loss: 0.8558, Train: 0.7908, Val: 0.7560, Test: 0.6067\n",
            "Run: 02, Epoch: 111, Loss: 0.8543, Train: 0.7900, Val: 0.7552, Test: 0.6063\n",
            "Run: 02, Epoch: 112, Loss: 0.8538, Train: 0.7907, Val: 0.7561, Test: 0.6073\n",
            "Run: 02, Epoch: 113, Loss: 0.8523, Train: 0.7919, Val: 0.7568, Test: 0.6082\n",
            "Run: 02, Epoch: 114, Loss: 0.8495, Train: 0.7919, Val: 0.7561, Test: 0.6080\n",
            "Run: 02, Epoch: 115, Loss: 0.8505, Train: 0.7920, Val: 0.7557, Test: 0.6073\n",
            "Run: 02, Epoch: 116, Loss: 0.8474, Train: 0.7924, Val: 0.7553, Test: 0.6071\n",
            "Run: 02, Epoch: 117, Loss: 0.8481, Train: 0.7932, Val: 0.7561, Test: 0.6078\n",
            "Run: 02, Epoch: 118, Loss: 0.8456, Train: 0.7930, Val: 0.7560, Test: 0.6079\n",
            "Run: 02, Epoch: 119, Loss: 0.8452, Train: 0.7928, Val: 0.7561, Test: 0.6076\n",
            "Run: 02, Epoch: 120, Loss: 0.8442, Train: 0.7940, Val: 0.7568, Test: 0.6083\n",
            "Run: 02, Epoch: 121, Loss: 0.8430, Train: 0.7954, Val: 0.7578, Test: 0.6088\n",
            "Run: 02, Epoch: 122, Loss: 0.8438, Train: 0.7953, Val: 0.7577, Test: 0.6089\n",
            "Run: 02, Epoch: 123, Loss: 0.8404, Train: 0.7945, Val: 0.7571, Test: 0.6084\n",
            "Run: 02, Epoch: 124, Loss: 0.8411, Train: 0.7946, Val: 0.7569, Test: 0.6081\n",
            "Run: 02, Epoch: 125, Loss: 0.8396, Train: 0.7962, Val: 0.7573, Test: 0.6088\n",
            "Run: 02, Epoch: 126, Loss: 0.8390, Train: 0.7970, Val: 0.7571, Test: 0.6090\n",
            "Run: 02, Epoch: 127, Loss: 0.8381, Train: 0.7971, Val: 0.7579, Test: 0.6091\n",
            "Run: 02, Epoch: 128, Loss: 0.8370, Train: 0.7971, Val: 0.7580, Test: 0.6090\n",
            "Run: 02, Epoch: 129, Loss: 0.8325, Train: 0.7982, Val: 0.7587, Test: 0.6092\n",
            "Run: 02, Epoch: 130, Loss: 0.8340, Train: 0.7989, Val: 0.7591, Test: 0.6095\n",
            "Run: 02, Epoch: 131, Loss: 0.8322, Train: 0.7991, Val: 0.7590, Test: 0.6096\n",
            "Run: 02, Epoch: 132, Loss: 0.8344, Train: 0.7991, Val: 0.7596, Test: 0.6100\n",
            "Run: 02, Epoch: 133, Loss: 0.8308, Train: 0.7993, Val: 0.7597, Test: 0.6103\n",
            "Run: 02, Epoch: 134, Loss: 0.8307, Train: 0.8002, Val: 0.7606, Test: 0.6101\n",
            "Run: 02, Epoch: 135, Loss: 0.8273, Train: 0.8005, Val: 0.7601, Test: 0.6095\n",
            "Run: 02, Epoch: 136, Loss: 0.8279, Train: 0.8008, Val: 0.7601, Test: 0.6097\n",
            "Run: 02, Epoch: 137, Loss: 0.8254, Train: 0.8013, Val: 0.7611, Test: 0.6106\n",
            "Run: 02, Epoch: 138, Loss: 0.8284, Train: 0.8013, Val: 0.7610, Test: 0.6114\n",
            "Run: 02, Epoch: 139, Loss: 0.8277, Train: 0.8010, Val: 0.7607, Test: 0.6114\n",
            "Run: 02, Epoch: 140, Loss: 0.8256, Train: 0.8015, Val: 0.7604, Test: 0.6109\n",
            "Run: 02, Epoch: 141, Loss: 0.8212, Train: 0.8018, Val: 0.7608, Test: 0.6104\n",
            "Run: 02, Epoch: 142, Loss: 0.8248, Train: 0.8022, Val: 0.7606, Test: 0.6101\n",
            "Run: 02, Epoch: 143, Loss: 0.8223, Train: 0.8028, Val: 0.7609, Test: 0.6104\n",
            "Run: 02, Epoch: 144, Loss: 0.8217, Train: 0.8030, Val: 0.7617, Test: 0.6110\n",
            "Run: 02, Epoch: 145, Loss: 0.8205, Train: 0.8031, Val: 0.7618, Test: 0.6110\n",
            "Run: 02, Epoch: 146, Loss: 0.8196, Train: 0.8027, Val: 0.7612, Test: 0.6108\n",
            "Run: 02, Epoch: 147, Loss: 0.8167, Train: 0.8036, Val: 0.7620, Test: 0.6116\n",
            "Run: 02, Epoch: 148, Loss: 0.8173, Train: 0.8039, Val: 0.7624, Test: 0.6122\n",
            "Run: 02, Epoch: 149, Loss: 0.8172, Train: 0.8042, Val: 0.7623, Test: 0.6123\n",
            "Run: 02, Epoch: 150, Loss: 0.8145, Train: 0.8043, Val: 0.7629, Test: 0.6126\n",
            "Run: 02, Epoch: 151, Loss: 0.8138, Train: 0.8052, Val: 0.7640, Test: 0.6128\n",
            "Run: 02, Epoch: 152, Loss: 0.8137, Train: 0.8056, Val: 0.7640, Test: 0.6125\n",
            "Run: 02, Epoch: 153, Loss: 0.8146, Train: 0.8048, Val: 0.7635, Test: 0.6117\n",
            "Run: 02, Epoch: 154, Loss: 0.8144, Train: 0.8053, Val: 0.7639, Test: 0.6121\n",
            "Run: 02, Epoch: 155, Loss: 0.8151, Train: 0.8058, Val: 0.7639, Test: 0.6124\n",
            "Run: 02, Epoch: 156, Loss: 0.8124, Train: 0.8059, Val: 0.7638, Test: 0.6122\n",
            "Run: 02, Epoch: 157, Loss: 0.8113, Train: 0.8063, Val: 0.7637, Test: 0.6119\n",
            "Run: 02, Epoch: 158, Loss: 0.8078, Train: 0.8075, Val: 0.7646, Test: 0.6126\n",
            "Run: 02, Epoch: 159, Loss: 0.8108, Train: 0.8082, Val: 0.7654, Test: 0.6130\n",
            "Run: 02, Epoch: 160, Loss: 0.8078, Train: 0.8077, Val: 0.7644, Test: 0.6127\n",
            "Run: 02, Epoch: 161, Loss: 0.8092, Train: 0.8081, Val: 0.7647, Test: 0.6129\n",
            "Run: 02, Epoch: 162, Loss: 0.8066, Train: 0.8090, Val: 0.7654, Test: 0.6132\n",
            "Run: 02, Epoch: 163, Loss: 0.8052, Train: 0.8084, Val: 0.7645, Test: 0.6128\n",
            "Run: 02, Epoch: 164, Loss: 0.8059, Train: 0.8076, Val: 0.7640, Test: 0.6123\n",
            "Run: 02, Epoch: 165, Loss: 0.8050, Train: 0.8083, Val: 0.7648, Test: 0.6131\n",
            "Run: 02, Epoch: 166, Loss: 0.8015, Train: 0.8094, Val: 0.7651, Test: 0.6139\n",
            "Run: 02, Epoch: 167, Loss: 0.8020, Train: 0.8093, Val: 0.7644, Test: 0.6134\n",
            "Run: 02, Epoch: 168, Loss: 0.8028, Train: 0.8095, Val: 0.7643, Test: 0.6135\n",
            "Run: 02, Epoch: 169, Loss: 0.7996, Train: 0.8100, Val: 0.7651, Test: 0.6142\n",
            "Run: 02, Epoch: 170, Loss: 0.8024, Train: 0.8103, Val: 0.7657, Test: 0.6142\n",
            "Run: 02, Epoch: 171, Loss: 0.8020, Train: 0.8103, Val: 0.7650, Test: 0.6134\n",
            "Run: 02, Epoch: 172, Loss: 0.7994, Train: 0.8111, Val: 0.7651, Test: 0.6137\n",
            "Run: 02, Epoch: 173, Loss: 0.7960, Train: 0.8116, Val: 0.7664, Test: 0.6141\n",
            "Run: 02, Epoch: 174, Loss: 0.7990, Train: 0.8115, Val: 0.7654, Test: 0.6138\n",
            "Run: 02, Epoch: 175, Loss: 0.7985, Train: 0.8114, Val: 0.7647, Test: 0.6137\n",
            "Run: 02, Epoch: 176, Loss: 0.7985, Train: 0.8116, Val: 0.7655, Test: 0.6137\n",
            "Run: 02, Epoch: 177, Loss: 0.7953, Train: 0.8120, Val: 0.7663, Test: 0.6136\n",
            "Run: 02, Epoch: 178, Loss: 0.7950, Train: 0.8125, Val: 0.7666, Test: 0.6137\n",
            "Run: 02, Epoch: 179, Loss: 0.7941, Train: 0.8124, Val: 0.7661, Test: 0.6136\n",
            "Run: 02, Epoch: 180, Loss: 0.7958, Train: 0.8119, Val: 0.7656, Test: 0.6136\n",
            "Run: 02, Epoch: 181, Loss: 0.7909, Train: 0.8125, Val: 0.7664, Test: 0.6144\n",
            "Run: 02, Epoch: 182, Loss: 0.7921, Train: 0.8129, Val: 0.7665, Test: 0.6150\n",
            "Run: 02, Epoch: 183, Loss: 0.7902, Train: 0.8133, Val: 0.7658, Test: 0.6148\n",
            "Run: 02, Epoch: 184, Loss: 0.7924, Train: 0.8136, Val: 0.7667, Test: 0.6149\n",
            "Run: 02, Epoch: 185, Loss: 0.7898, Train: 0.8138, Val: 0.7661, Test: 0.6151\n",
            "Run: 02, Epoch: 186, Loss: 0.7901, Train: 0.8141, Val: 0.7663, Test: 0.6150\n",
            "Run: 02, Epoch: 187, Loss: 0.7885, Train: 0.8140, Val: 0.7658, Test: 0.6144\n",
            "Run: 02, Epoch: 188, Loss: 0.7892, Train: 0.8146, Val: 0.7662, Test: 0.6143\n",
            "Run: 02, Epoch: 189, Loss: 0.7876, Train: 0.8150, Val: 0.7670, Test: 0.6147\n",
            "Run: 02, Epoch: 190, Loss: 0.7878, Train: 0.8147, Val: 0.7676, Test: 0.6146\n",
            "Run: 02, Epoch: 191, Loss: 0.7866, Train: 0.8151, Val: 0.7673, Test: 0.6146\n",
            "Run: 02, Epoch: 192, Loss: 0.7874, Train: 0.8158, Val: 0.7674, Test: 0.6152\n",
            "Run: 02, Epoch: 193, Loss: 0.7859, Train: 0.8163, Val: 0.7674, Test: 0.6154\n",
            "Run: 02, Epoch: 194, Loss: 0.7813, Train: 0.8163, Val: 0.7673, Test: 0.6151\n",
            "Run: 02, Epoch: 195, Loss: 0.7842, Train: 0.8169, Val: 0.7677, Test: 0.6155\n",
            "Run: 02, Epoch: 196, Loss: 0.7823, Train: 0.8164, Val: 0.7680, Test: 0.6159\n",
            "Run: 02, Epoch: 197, Loss: 0.7853, Train: 0.8164, Val: 0.7680, Test: 0.6157\n",
            "Run: 02, Epoch: 198, Loss: 0.7806, Train: 0.8166, Val: 0.7677, Test: 0.6152\n",
            "Run: 02, Epoch: 199, Loss: 0.7820, Train: 0.8170, Val: 0.7677, Test: 0.6151\n",
            "Run: 02, Epoch: 200, Loss: 0.7820, Train: 0.8176, Val: 0.7684, Test: 0.6158\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9451, Val: 0.9119, Test: 0.8417\n",
            "340527\n",
            "\n",
            "Run 03:\n",
            "\n",
            "Run: 03, Epoch: 001, Loss: 4.0749, Train: 0.3751, Val: 0.3733, Test: 0.3082\n",
            "Run: 03, Epoch: 002, Loss: 2.9747, Train: 0.4871, Val: 0.4833, Test: 0.4039\n",
            "Run: 03, Epoch: 003, Loss: 2.1179, Train: 0.5133, Val: 0.5085, Test: 0.4134\n",
            "Run: 03, Epoch: 004, Loss: 1.8989, Train: 0.5568, Val: 0.5469, Test: 0.4433\n",
            "Run: 03, Epoch: 005, Loss: 1.7508, Train: 0.5847, Val: 0.5770, Test: 0.4728\n",
            "Run: 03, Epoch: 006, Loss: 1.6139, Train: 0.6061, Val: 0.5958, Test: 0.4902\n",
            "Run: 03, Epoch: 007, Loss: 1.5524, Train: 0.6187, Val: 0.6051, Test: 0.4893\n",
            "Run: 03, Epoch: 008, Loss: 1.4634, Train: 0.6377, Val: 0.6255, Test: 0.5058\n",
            "Run: 03, Epoch: 009, Loss: 1.4050, Train: 0.6442, Val: 0.6333, Test: 0.5158\n",
            "Run: 03, Epoch: 010, Loss: 1.3686, Train: 0.6484, Val: 0.6360, Test: 0.5211\n",
            "Run: 03, Epoch: 011, Loss: 1.3260, Train: 0.6494, Val: 0.6359, Test: 0.5208\n",
            "Run: 03, Epoch: 012, Loss: 1.2939, Train: 0.6509, Val: 0.6393, Test: 0.5195\n",
            "Run: 03, Epoch: 013, Loss: 1.2768, Train: 0.6624, Val: 0.6487, Test: 0.5257\n",
            "Run: 03, Epoch: 014, Loss: 1.2459, Train: 0.6693, Val: 0.6543, Test: 0.5316\n",
            "Run: 03, Epoch: 015, Loss: 1.2314, Train: 0.6743, Val: 0.6590, Test: 0.5367\n",
            "Run: 03, Epoch: 016, Loss: 1.2154, Train: 0.6765, Val: 0.6622, Test: 0.5389\n",
            "Run: 03, Epoch: 017, Loss: 1.1988, Train: 0.6800, Val: 0.6665, Test: 0.5403\n",
            "Run: 03, Epoch: 018, Loss: 1.1793, Train: 0.6876, Val: 0.6724, Test: 0.5445\n",
            "Run: 03, Epoch: 019, Loss: 1.1665, Train: 0.6943, Val: 0.6785, Test: 0.5492\n",
            "Run: 03, Epoch: 020, Loss: 1.1537, Train: 0.6995, Val: 0.6851, Test: 0.5554\n",
            "Run: 03, Epoch: 021, Loss: 1.1362, Train: 0.7032, Val: 0.6878, Test: 0.5598\n",
            "Run: 03, Epoch: 022, Loss: 1.1252, Train: 0.7053, Val: 0.6889, Test: 0.5616\n",
            "Run: 03, Epoch: 023, Loss: 1.1180, Train: 0.7080, Val: 0.6929, Test: 0.5616\n",
            "Run: 03, Epoch: 024, Loss: 1.1052, Train: 0.7098, Val: 0.6945, Test: 0.5611\n",
            "Run: 03, Epoch: 025, Loss: 1.0931, Train: 0.7114, Val: 0.6975, Test: 0.5617\n",
            "Run: 03, Epoch: 026, Loss: 1.0842, Train: 0.7139, Val: 0.6997, Test: 0.5638\n",
            "Run: 03, Epoch: 027, Loss: 1.0800, Train: 0.7152, Val: 0.7013, Test: 0.5649\n",
            "Run: 03, Epoch: 028, Loss: 1.0719, Train: 0.7177, Val: 0.7017, Test: 0.5655\n",
            "Run: 03, Epoch: 029, Loss: 1.0605, Train: 0.7198, Val: 0.7023, Test: 0.5659\n",
            "Run: 03, Epoch: 030, Loss: 1.0558, Train: 0.7215, Val: 0.7027, Test: 0.5666\n",
            "Run: 03, Epoch: 031, Loss: 1.0521, Train: 0.7233, Val: 0.7039, Test: 0.5684\n",
            "Run: 03, Epoch: 032, Loss: 1.0448, Train: 0.7244, Val: 0.7058, Test: 0.5699\n",
            "Run: 03, Epoch: 033, Loss: 1.0387, Train: 0.7251, Val: 0.7066, Test: 0.5708\n",
            "Run: 03, Epoch: 034, Loss: 1.0318, Train: 0.7254, Val: 0.7068, Test: 0.5702\n",
            "Run: 03, Epoch: 035, Loss: 1.0273, Train: 0.7253, Val: 0.7062, Test: 0.5688\n",
            "Run: 03, Epoch: 036, Loss: 1.0230, Train: 0.7263, Val: 0.7073, Test: 0.5693\n",
            "Run: 03, Epoch: 037, Loss: 1.0201, Train: 0.7279, Val: 0.7088, Test: 0.5712\n",
            "Run: 03, Epoch: 038, Loss: 1.0156, Train: 0.7287, Val: 0.7096, Test: 0.5728\n",
            "Run: 03, Epoch: 039, Loss: 1.0105, Train: 0.7310, Val: 0.7124, Test: 0.5746\n",
            "Run: 03, Epoch: 040, Loss: 1.0046, Train: 0.7328, Val: 0.7141, Test: 0.5758\n",
            "Run: 03, Epoch: 041, Loss: 1.0008, Train: 0.7323, Val: 0.7117, Test: 0.5741\n",
            "Run: 03, Epoch: 042, Loss: 0.9997, Train: 0.7311, Val: 0.7095, Test: 0.5725\n",
            "Run: 03, Epoch: 043, Loss: 0.9929, Train: 0.7332, Val: 0.7107, Test: 0.5743\n",
            "Run: 03, Epoch: 044, Loss: 0.9897, Train: 0.7347, Val: 0.7131, Test: 0.5767\n",
            "Run: 03, Epoch: 045, Loss: 0.9872, Train: 0.7373, Val: 0.7177, Test: 0.5788\n",
            "Run: 03, Epoch: 046, Loss: 0.9841, Train: 0.7439, Val: 0.7247, Test: 0.5828\n",
            "Run: 03, Epoch: 047, Loss: 0.9783, Train: 0.7435, Val: 0.7224, Test: 0.5817\n",
            "Run: 03, Epoch: 048, Loss: 0.9796, Train: 0.7391, Val: 0.7163, Test: 0.5772\n",
            "Run: 03, Epoch: 049, Loss: 0.9733, Train: 0.7400, Val: 0.7161, Test: 0.5776\n",
            "Run: 03, Epoch: 050, Loss: 0.9711, Train: 0.7417, Val: 0.7186, Test: 0.5796\n",
            "Run: 03, Epoch: 051, Loss: 0.9676, Train: 0.7441, Val: 0.7212, Test: 0.5818\n",
            "Run: 03, Epoch: 052, Loss: 0.9655, Train: 0.7506, Val: 0.7304, Test: 0.5869\n",
            "Run: 03, Epoch: 053, Loss: 0.9631, Train: 0.7513, Val: 0.7298, Test: 0.5865\n",
            "Run: 03, Epoch: 054, Loss: 0.9577, Train: 0.7456, Val: 0.7220, Test: 0.5820\n",
            "Run: 03, Epoch: 055, Loss: 0.9585, Train: 0.7514, Val: 0.7280, Test: 0.5862\n",
            "Run: 03, Epoch: 056, Loss: 0.9538, Train: 0.7538, Val: 0.7316, Test: 0.5890\n",
            "Run: 03, Epoch: 057, Loss: 0.9484, Train: 0.7553, Val: 0.7326, Test: 0.5906\n",
            "Run: 03, Epoch: 058, Loss: 0.9487, Train: 0.7559, Val: 0.7326, Test: 0.5902\n",
            "Run: 03, Epoch: 059, Loss: 0.9452, Train: 0.7513, Val: 0.7268, Test: 0.5859\n",
            "Run: 03, Epoch: 060, Loss: 0.9459, Train: 0.7569, Val: 0.7338, Test: 0.5897\n",
            "Run: 03, Epoch: 061, Loss: 0.9397, Train: 0.7590, Val: 0.7343, Test: 0.5917\n",
            "Run: 03, Epoch: 062, Loss: 0.9387, Train: 0.7599, Val: 0.7354, Test: 0.5922\n",
            "Run: 03, Epoch: 063, Loss: 0.9368, Train: 0.7605, Val: 0.7358, Test: 0.5918\n",
            "Run: 03, Epoch: 064, Loss: 0.9351, Train: 0.7609, Val: 0.7364, Test: 0.5917\n",
            "Run: 03, Epoch: 065, Loss: 0.9322, Train: 0.7615, Val: 0.7367, Test: 0.5919\n",
            "Run: 03, Epoch: 066, Loss: 0.9293, Train: 0.7623, Val: 0.7377, Test: 0.5928\n",
            "Run: 03, Epoch: 067, Loss: 0.9272, Train: 0.7633, Val: 0.7381, Test: 0.5942\n",
            "Run: 03, Epoch: 068, Loss: 0.9222, Train: 0.7640, Val: 0.7391, Test: 0.5957\n",
            "Run: 03, Epoch: 069, Loss: 0.9235, Train: 0.7648, Val: 0.7402, Test: 0.5966\n",
            "Run: 03, Epoch: 070, Loss: 0.9206, Train: 0.7653, Val: 0.7399, Test: 0.5964\n",
            "Run: 03, Epoch: 071, Loss: 0.9191, Train: 0.7658, Val: 0.7398, Test: 0.5961\n",
            "Run: 03, Epoch: 072, Loss: 0.9187, Train: 0.7672, Val: 0.7409, Test: 0.5968\n",
            "Run: 03, Epoch: 073, Loss: 0.9147, Train: 0.7673, Val: 0.7406, Test: 0.5965\n",
            "Run: 03, Epoch: 074, Loss: 0.9120, Train: 0.7672, Val: 0.7410, Test: 0.5961\n",
            "Run: 03, Epoch: 075, Loss: 0.9127, Train: 0.7677, Val: 0.7420, Test: 0.5967\n",
            "Run: 03, Epoch: 076, Loss: 0.9099, Train: 0.7693, Val: 0.7433, Test: 0.5979\n",
            "Run: 03, Epoch: 077, Loss: 0.9109, Train: 0.7704, Val: 0.7437, Test: 0.5982\n",
            "Run: 03, Epoch: 078, Loss: 0.9073, Train: 0.7711, Val: 0.7438, Test: 0.5979\n",
            "Run: 03, Epoch: 079, Loss: 0.9048, Train: 0.7712, Val: 0.7445, Test: 0.5977\n",
            "Run: 03, Epoch: 080, Loss: 0.9062, Train: 0.7712, Val: 0.7440, Test: 0.5978\n",
            "Run: 03, Epoch: 081, Loss: 0.9026, Train: 0.7724, Val: 0.7454, Test: 0.5992\n",
            "Run: 03, Epoch: 082, Loss: 0.9016, Train: 0.7736, Val: 0.7462, Test: 0.6007\n",
            "Run: 03, Epoch: 083, Loss: 0.8999, Train: 0.7742, Val: 0.7454, Test: 0.6010\n",
            "Run: 03, Epoch: 084, Loss: 0.8970, Train: 0.7746, Val: 0.7454, Test: 0.6007\n",
            "Run: 03, Epoch: 085, Loss: 0.8974, Train: 0.7748, Val: 0.7465, Test: 0.6009\n",
            "Run: 03, Epoch: 086, Loss: 0.8934, Train: 0.7757, Val: 0.7471, Test: 0.6015\n",
            "Run: 03, Epoch: 087, Loss: 0.8924, Train: 0.7764, Val: 0.7475, Test: 0.6019\n",
            "Run: 03, Epoch: 088, Loss: 0.8919, Train: 0.7765, Val: 0.7473, Test: 0.6014\n",
            "Run: 03, Epoch: 089, Loss: 0.8903, Train: 0.7765, Val: 0.7461, Test: 0.6006\n",
            "Run: 03, Epoch: 090, Loss: 0.8889, Train: 0.7783, Val: 0.7480, Test: 0.6021\n",
            "Run: 03, Epoch: 091, Loss: 0.8865, Train: 0.7794, Val: 0.7488, Test: 0.6034\n",
            "Run: 03, Epoch: 092, Loss: 0.8861, Train: 0.7799, Val: 0.7490, Test: 0.6032\n",
            "Run: 03, Epoch: 093, Loss: 0.8844, Train: 0.7796, Val: 0.7486, Test: 0.6021\n",
            "Run: 03, Epoch: 094, Loss: 0.8841, Train: 0.7798, Val: 0.7486, Test: 0.6015\n",
            "Run: 03, Epoch: 095, Loss: 0.8817, Train: 0.7815, Val: 0.7492, Test: 0.6026\n",
            "Run: 03, Epoch: 096, Loss: 0.8802, Train: 0.7828, Val: 0.7504, Test: 0.6046\n",
            "Run: 03, Epoch: 097, Loss: 0.8786, Train: 0.7830, Val: 0.7504, Test: 0.6053\n",
            "Run: 03, Epoch: 098, Loss: 0.8806, Train: 0.7823, Val: 0.7499, Test: 0.6047\n",
            "Run: 03, Epoch: 099, Loss: 0.8760, Train: 0.7812, Val: 0.7482, Test: 0.6035\n",
            "Run: 03, Epoch: 100, Loss: 0.8745, Train: 0.7825, Val: 0.7484, Test: 0.6043\n",
            "Run: 03, Epoch: 101, Loss: 0.8729, Train: 0.7841, Val: 0.7505, Test: 0.6052\n",
            "Run: 03, Epoch: 102, Loss: 0.8732, Train: 0.7847, Val: 0.7509, Test: 0.6050\n",
            "Run: 03, Epoch: 103, Loss: 0.8693, Train: 0.7844, Val: 0.7515, Test: 0.6045\n",
            "Run: 03, Epoch: 104, Loss: 0.8683, Train: 0.7847, Val: 0.7514, Test: 0.6044\n",
            "Run: 03, Epoch: 105, Loss: 0.8681, Train: 0.7853, Val: 0.7522, Test: 0.6050\n",
            "Run: 03, Epoch: 106, Loss: 0.8657, Train: 0.7860, Val: 0.7529, Test: 0.6050\n",
            "Run: 03, Epoch: 107, Loss: 0.8677, Train: 0.7859, Val: 0.7526, Test: 0.6043\n",
            "Run: 03, Epoch: 108, Loss: 0.8673, Train: 0.7858, Val: 0.7519, Test: 0.6041\n",
            "Run: 03, Epoch: 109, Loss: 0.8672, Train: 0.7868, Val: 0.7524, Test: 0.6055\n",
            "Run: 03, Epoch: 110, Loss: 0.8636, Train: 0.7878, Val: 0.7533, Test: 0.6073\n",
            "Run: 03, Epoch: 111, Loss: 0.8634, Train: 0.7882, Val: 0.7539, Test: 0.6076\n",
            "Run: 03, Epoch: 112, Loss: 0.8622, Train: 0.7881, Val: 0.7535, Test: 0.6071\n",
            "Run: 03, Epoch: 113, Loss: 0.8587, Train: 0.7890, Val: 0.7539, Test: 0.6074\n",
            "Run: 03, Epoch: 114, Loss: 0.8594, Train: 0.7899, Val: 0.7544, Test: 0.6083\n",
            "Run: 03, Epoch: 115, Loss: 0.8570, Train: 0.7904, Val: 0.7547, Test: 0.6085\n",
            "Run: 03, Epoch: 116, Loss: 0.8552, Train: 0.7897, Val: 0.7550, Test: 0.6078\n",
            "Run: 03, Epoch: 117, Loss: 0.8566, Train: 0.7898, Val: 0.7545, Test: 0.6071\n",
            "Run: 03, Epoch: 118, Loss: 0.8541, Train: 0.7905, Val: 0.7550, Test: 0.6073\n",
            "Run: 03, Epoch: 119, Loss: 0.8538, Train: 0.7913, Val: 0.7553, Test: 0.6076\n",
            "Run: 03, Epoch: 120, Loss: 0.8541, Train: 0.7917, Val: 0.7549, Test: 0.6079\n",
            "Run: 03, Epoch: 121, Loss: 0.8506, Train: 0.7917, Val: 0.7549, Test: 0.6080\n",
            "Run: 03, Epoch: 122, Loss: 0.8499, Train: 0.7922, Val: 0.7552, Test: 0.6084\n",
            "Run: 03, Epoch: 123, Loss: 0.8465, Train: 0.7926, Val: 0.7557, Test: 0.6085\n",
            "Run: 03, Epoch: 124, Loss: 0.8488, Train: 0.7933, Val: 0.7566, Test: 0.6088\n",
            "Run: 03, Epoch: 125, Loss: 0.8503, Train: 0.7936, Val: 0.7561, Test: 0.6087\n",
            "Run: 03, Epoch: 126, Loss: 0.8478, Train: 0.7936, Val: 0.7558, Test: 0.6086\n",
            "Run: 03, Epoch: 127, Loss: 0.8440, Train: 0.7934, Val: 0.7549, Test: 0.6082\n",
            "Run: 03, Epoch: 128, Loss: 0.8465, Train: 0.7945, Val: 0.7563, Test: 0.6091\n",
            "Run: 03, Epoch: 129, Loss: 0.8443, Train: 0.7950, Val: 0.7571, Test: 0.6092\n",
            "Run: 03, Epoch: 130, Loss: 0.8433, Train: 0.7954, Val: 0.7576, Test: 0.6093\n",
            "Run: 03, Epoch: 131, Loss: 0.8400, Train: 0.7959, Val: 0.7585, Test: 0.6096\n",
            "Run: 03, Epoch: 132, Loss: 0.8414, Train: 0.7964, Val: 0.7589, Test: 0.6100\n",
            "Run: 03, Epoch: 133, Loss: 0.8404, Train: 0.7967, Val: 0.7591, Test: 0.6106\n",
            "Run: 03, Epoch: 134, Loss: 0.8372, Train: 0.7972, Val: 0.7594, Test: 0.6108\n",
            "Run: 03, Epoch: 135, Loss: 0.8379, Train: 0.7966, Val: 0.7585, Test: 0.6100\n",
            "Run: 03, Epoch: 136, Loss: 0.8373, Train: 0.7967, Val: 0.7581, Test: 0.6097\n",
            "Run: 03, Epoch: 137, Loss: 0.8351, Train: 0.7974, Val: 0.7590, Test: 0.6098\n",
            "Run: 03, Epoch: 138, Loss: 0.8331, Train: 0.7975, Val: 0.7593, Test: 0.6098\n",
            "Run: 03, Epoch: 139, Loss: 0.8350, Train: 0.7976, Val: 0.7587, Test: 0.6099\n",
            "Run: 03, Epoch: 140, Loss: 0.8340, Train: 0.7985, Val: 0.7595, Test: 0.6107\n",
            "Run: 03, Epoch: 141, Loss: 0.8346, Train: 0.7991, Val: 0.7602, Test: 0.6115\n",
            "Run: 03, Epoch: 142, Loss: 0.8310, Train: 0.7991, Val: 0.7602, Test: 0.6117\n",
            "Run: 03, Epoch: 143, Loss: 0.8334, Train: 0.7993, Val: 0.7597, Test: 0.6115\n",
            "Run: 03, Epoch: 144, Loss: 0.8299, Train: 0.7994, Val: 0.7597, Test: 0.6114\n",
            "Run: 03, Epoch: 145, Loss: 0.8279, Train: 0.7994, Val: 0.7601, Test: 0.6112\n",
            "Run: 03, Epoch: 146, Loss: 0.8273, Train: 0.7998, Val: 0.7606, Test: 0.6110\n",
            "Run: 03, Epoch: 147, Loss: 0.8271, Train: 0.8000, Val: 0.7608, Test: 0.6113\n",
            "Run: 03, Epoch: 148, Loss: 0.8282, Train: 0.8006, Val: 0.7609, Test: 0.6121\n",
            "Run: 03, Epoch: 149, Loss: 0.8263, Train: 0.8012, Val: 0.7615, Test: 0.6124\n",
            "Run: 03, Epoch: 150, Loss: 0.8264, Train: 0.8013, Val: 0.7619, Test: 0.6124\n",
            "Run: 03, Epoch: 151, Loss: 0.8237, Train: 0.8015, Val: 0.7617, Test: 0.6121\n",
            "Run: 03, Epoch: 152, Loss: 0.8247, Train: 0.8018, Val: 0.7613, Test: 0.6116\n",
            "Run: 03, Epoch: 153, Loss: 0.8243, Train: 0.8022, Val: 0.7616, Test: 0.6120\n",
            "Run: 03, Epoch: 154, Loss: 0.8253, Train: 0.8026, Val: 0.7619, Test: 0.6126\n",
            "Run: 03, Epoch: 155, Loss: 0.8235, Train: 0.8025, Val: 0.7602, Test: 0.6124\n",
            "Run: 03, Epoch: 156, Loss: 0.8194, Train: 0.8027, Val: 0.7606, Test: 0.6124\n",
            "Run: 03, Epoch: 157, Loss: 0.8209, Train: 0.8033, Val: 0.7623, Test: 0.6124\n",
            "Run: 03, Epoch: 158, Loss: 0.8168, Train: 0.8034, Val: 0.7628, Test: 0.6125\n",
            "Run: 03, Epoch: 159, Loss: 0.8165, Train: 0.8039, Val: 0.7630, Test: 0.6133\n",
            "Run: 03, Epoch: 160, Loss: 0.8160, Train: 0.8044, Val: 0.7635, Test: 0.6136\n",
            "Run: 03, Epoch: 161, Loss: 0.8156, Train: 0.8045, Val: 0.7632, Test: 0.6135\n",
            "Run: 03, Epoch: 162, Loss: 0.8197, Train: 0.8048, Val: 0.7630, Test: 0.6138\n",
            "Run: 03, Epoch: 163, Loss: 0.8149, Train: 0.8051, Val: 0.7630, Test: 0.6139\n",
            "Run: 03, Epoch: 164, Loss: 0.8163, Train: 0.8049, Val: 0.7631, Test: 0.6133\n",
            "Run: 03, Epoch: 165, Loss: 0.8177, Train: 0.8048, Val: 0.7632, Test: 0.6132\n",
            "Run: 03, Epoch: 166, Loss: 0.8153, Train: 0.8055, Val: 0.7634, Test: 0.6136\n",
            "Run: 03, Epoch: 167, Loss: 0.8099, Train: 0.8056, Val: 0.7631, Test: 0.6131\n",
            "Run: 03, Epoch: 168, Loss: 0.8147, Train: 0.8062, Val: 0.7636, Test: 0.6130\n",
            "Run: 03, Epoch: 169, Loss: 0.8082, Train: 0.8068, Val: 0.7644, Test: 0.6137\n",
            "Run: 03, Epoch: 170, Loss: 0.8113, Train: 0.8067, Val: 0.7649, Test: 0.6136\n",
            "Run: 03, Epoch: 171, Loss: 0.8094, Train: 0.8064, Val: 0.7642, Test: 0.6131\n",
            "Run: 03, Epoch: 172, Loss: 0.8091, Train: 0.8070, Val: 0.7641, Test: 0.6136\n",
            "Run: 03, Epoch: 173, Loss: 0.8109, Train: 0.8077, Val: 0.7645, Test: 0.6140\n",
            "Run: 03, Epoch: 174, Loss: 0.8069, Train: 0.8072, Val: 0.7635, Test: 0.6136\n",
            "Run: 03, Epoch: 175, Loss: 0.8091, Train: 0.8076, Val: 0.7641, Test: 0.6137\n",
            "Run: 03, Epoch: 176, Loss: 0.8027, Train: 0.8083, Val: 0.7653, Test: 0.6141\n",
            "Run: 03, Epoch: 177, Loss: 0.8068, Train: 0.8085, Val: 0.7649, Test: 0.6138\n",
            "Run: 03, Epoch: 178, Loss: 0.8066, Train: 0.8081, Val: 0.7644, Test: 0.6134\n",
            "Run: 03, Epoch: 179, Loss: 0.8032, Train: 0.8085, Val: 0.7644, Test: 0.6138\n",
            "Run: 03, Epoch: 180, Loss: 0.8018, Train: 0.8095, Val: 0.7657, Test: 0.6153\n",
            "Run: 03, Epoch: 181, Loss: 0.8054, Train: 0.8093, Val: 0.7655, Test: 0.6153\n",
            "Run: 03, Epoch: 182, Loss: 0.8023, Train: 0.8092, Val: 0.7651, Test: 0.6154\n",
            "Run: 03, Epoch: 183, Loss: 0.8029, Train: 0.8096, Val: 0.7654, Test: 0.6154\n",
            "Run: 03, Epoch: 184, Loss: 0.8023, Train: 0.8100, Val: 0.7658, Test: 0.6150\n",
            "Run: 03, Epoch: 185, Loss: 0.7996, Train: 0.8104, Val: 0.7656, Test: 0.6145\n",
            "Run: 03, Epoch: 186, Loss: 0.8009, Train: 0.8106, Val: 0.7661, Test: 0.6153\n",
            "Run: 03, Epoch: 187, Loss: 0.8001, Train: 0.8107, Val: 0.7657, Test: 0.6156\n",
            "Run: 03, Epoch: 188, Loss: 0.7979, Train: 0.8107, Val: 0.7654, Test: 0.6150\n",
            "Run: 03, Epoch: 189, Loss: 0.7976, Train: 0.8107, Val: 0.7653, Test: 0.6148\n",
            "Run: 03, Epoch: 190, Loss: 0.7970, Train: 0.8110, Val: 0.7653, Test: 0.6148\n",
            "Run: 03, Epoch: 191, Loss: 0.7972, Train: 0.8112, Val: 0.7660, Test: 0.6147\n",
            "Run: 03, Epoch: 192, Loss: 0.7966, Train: 0.8110, Val: 0.7653, Test: 0.6141\n",
            "Run: 03, Epoch: 193, Loss: 0.7950, Train: 0.8119, Val: 0.7666, Test: 0.6147\n",
            "Run: 03, Epoch: 194, Loss: 0.7948, Train: 0.8119, Val: 0.7661, Test: 0.6150\n",
            "Run: 03, Epoch: 195, Loss: 0.7954, Train: 0.8116, Val: 0.7663, Test: 0.6149\n",
            "Run: 03, Epoch: 196, Loss: 0.7947, Train: 0.8123, Val: 0.7665, Test: 0.6156\n",
            "Run: 03, Epoch: 197, Loss: 0.7937, Train: 0.8126, Val: 0.7669, Test: 0.6157\n",
            "Run: 03, Epoch: 198, Loss: 0.7906, Train: 0.8127, Val: 0.7668, Test: 0.6158\n",
            "Run: 03, Epoch: 199, Loss: 0.7938, Train: 0.8127, Val: 0.7668, Test: 0.6158\n",
            "Run: 03, Epoch: 200, Loss: 0.7916, Train: 0.8132, Val: 0.7668, Test: 0.6165\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9457, Val: 0.9126, Test: 0.8412\n",
            "340527\n",
            "\n",
            "Run 04:\n",
            "\n",
            "Run: 04, Epoch: 001, Loss: 4.1142, Train: 0.3817, Val: 0.3795, Test: 0.3160\n",
            "Run: 04, Epoch: 002, Loss: 3.0506, Train: 0.4801, Val: 0.4753, Test: 0.4018\n",
            "Run: 04, Epoch: 003, Loss: 2.2006, Train: 0.5018, Val: 0.5002, Test: 0.4048\n",
            "Run: 04, Epoch: 004, Loss: 1.9165, Train: 0.5540, Val: 0.5476, Test: 0.4473\n",
            "Run: 04, Epoch: 005, Loss: 1.7987, Train: 0.5893, Val: 0.5812, Test: 0.4722\n",
            "Run: 04, Epoch: 006, Loss: 1.6168, Train: 0.5946, Val: 0.5853, Test: 0.4786\n",
            "Run: 04, Epoch: 007, Loss: 1.5880, Train: 0.6170, Val: 0.6071, Test: 0.4989\n",
            "Run: 04, Epoch: 008, Loss: 1.4837, Train: 0.6308, Val: 0.6187, Test: 0.5035\n",
            "Run: 04, Epoch: 009, Loss: 1.4342, Train: 0.6267, Val: 0.6159, Test: 0.4994\n",
            "Run: 04, Epoch: 010, Loss: 1.3995, Train: 0.6343, Val: 0.6232, Test: 0.5071\n",
            "Run: 04, Epoch: 011, Loss: 1.3487, Train: 0.6343, Val: 0.6218, Test: 0.5097\n",
            "Run: 04, Epoch: 012, Loss: 1.3200, Train: 0.6422, Val: 0.6278, Test: 0.5158\n",
            "Run: 04, Epoch: 013, Loss: 1.2900, Train: 0.6445, Val: 0.6296, Test: 0.5156\n",
            "Run: 04, Epoch: 014, Loss: 1.2842, Train: 0.6590, Val: 0.6434, Test: 0.5247\n",
            "Run: 04, Epoch: 015, Loss: 1.2450, Train: 0.6673, Val: 0.6526, Test: 0.5309\n",
            "Run: 04, Epoch: 016, Loss: 1.2290, Train: 0.6712, Val: 0.6570, Test: 0.5343\n",
            "Run: 04, Epoch: 017, Loss: 1.2167, Train: 0.6774, Val: 0.6625, Test: 0.5375\n",
            "Run: 04, Epoch: 018, Loss: 1.1942, Train: 0.6830, Val: 0.6683, Test: 0.5409\n",
            "Run: 04, Epoch: 019, Loss: 1.1766, Train: 0.6890, Val: 0.6747, Test: 0.5457\n",
            "Run: 04, Epoch: 020, Loss: 1.1672, Train: 0.6980, Val: 0.6825, Test: 0.5536\n",
            "Run: 04, Epoch: 021, Loss: 1.1527, Train: 0.7014, Val: 0.6878, Test: 0.5581\n",
            "Run: 04, Epoch: 022, Loss: 1.1390, Train: 0.7045, Val: 0.6898, Test: 0.5609\n",
            "Run: 04, Epoch: 023, Loss: 1.1296, Train: 0.7085, Val: 0.6930, Test: 0.5620\n",
            "Run: 04, Epoch: 024, Loss: 1.1151, Train: 0.7100, Val: 0.6933, Test: 0.5610\n",
            "Run: 04, Epoch: 025, Loss: 1.1052, Train: 0.7109, Val: 0.6943, Test: 0.5609\n",
            "Run: 04, Epoch: 026, Loss: 1.0998, Train: 0.7138, Val: 0.6970, Test: 0.5642\n",
            "Run: 04, Epoch: 027, Loss: 1.0891, Train: 0.7166, Val: 0.7006, Test: 0.5679\n",
            "Run: 04, Epoch: 028, Loss: 1.0805, Train: 0.7191, Val: 0.7016, Test: 0.5699\n",
            "Run: 04, Epoch: 029, Loss: 1.0753, Train: 0.7206, Val: 0.7022, Test: 0.5697\n",
            "Run: 04, Epoch: 030, Loss: 1.0658, Train: 0.7210, Val: 0.7027, Test: 0.5680\n",
            "Run: 04, Epoch: 031, Loss: 1.0577, Train: 0.7206, Val: 0.7020, Test: 0.5655\n",
            "Run: 04, Epoch: 032, Loss: 1.0527, Train: 0.7212, Val: 0.7033, Test: 0.5646\n",
            "Run: 04, Epoch: 033, Loss: 1.0492, Train: 0.7232, Val: 0.7053, Test: 0.5660\n",
            "Run: 04, Epoch: 034, Loss: 1.0396, Train: 0.7249, Val: 0.7076, Test: 0.5682\n",
            "Run: 04, Epoch: 035, Loss: 1.0341, Train: 0.7265, Val: 0.7088, Test: 0.5705\n",
            "Run: 04, Epoch: 036, Loss: 1.0291, Train: 0.7272, Val: 0.7091, Test: 0.5719\n",
            "Run: 04, Epoch: 037, Loss: 1.0257, Train: 0.7274, Val: 0.7085, Test: 0.5719\n",
            "Run: 04, Epoch: 038, Loss: 1.0205, Train: 0.7277, Val: 0.7083, Test: 0.5710\n",
            "Run: 04, Epoch: 039, Loss: 1.0160, Train: 0.7283, Val: 0.7085, Test: 0.5708\n",
            "Run: 04, Epoch: 040, Loss: 1.0100, Train: 0.7306, Val: 0.7105, Test: 0.5717\n",
            "Run: 04, Epoch: 041, Loss: 1.0078, Train: 0.7321, Val: 0.7111, Test: 0.5724\n",
            "Run: 04, Epoch: 042, Loss: 1.0042, Train: 0.7327, Val: 0.7117, Test: 0.5731\n",
            "Run: 04, Epoch: 043, Loss: 0.9992, Train: 0.7327, Val: 0.7124, Test: 0.5733\n",
            "Run: 04, Epoch: 044, Loss: 0.9944, Train: 0.7330, Val: 0.7122, Test: 0.5733\n",
            "Run: 04, Epoch: 045, Loss: 0.9913, Train: 0.7331, Val: 0.7121, Test: 0.5733\n",
            "Run: 04, Epoch: 046, Loss: 0.9889, Train: 0.7346, Val: 0.7137, Test: 0.5744\n",
            "Run: 04, Epoch: 047, Loss: 0.9850, Train: 0.7375, Val: 0.7153, Test: 0.5762\n",
            "Run: 04, Epoch: 048, Loss: 0.9811, Train: 0.7397, Val: 0.7178, Test: 0.5778\n",
            "Run: 04, Epoch: 049, Loss: 0.9795, Train: 0.7402, Val: 0.7191, Test: 0.5787\n",
            "Run: 04, Epoch: 050, Loss: 0.9735, Train: 0.7402, Val: 0.7192, Test: 0.5790\n",
            "Run: 04, Epoch: 051, Loss: 0.9731, Train: 0.7411, Val: 0.7192, Test: 0.5795\n",
            "Run: 04, Epoch: 052, Loss: 0.9727, Train: 0.7478, Val: 0.7259, Test: 0.5839\n",
            "Run: 04, Epoch: 053, Loss: 0.9657, Train: 0.7495, Val: 0.7268, Test: 0.5848\n",
            "Run: 04, Epoch: 054, Loss: 0.9652, Train: 0.7459, Val: 0.7219, Test: 0.5816\n",
            "Run: 04, Epoch: 055, Loss: 0.9618, Train: 0.7455, Val: 0.7221, Test: 0.5810\n",
            "Run: 04, Epoch: 056, Loss: 0.9599, Train: 0.7455, Val: 0.7222, Test: 0.5809\n",
            "Run: 04, Epoch: 057, Loss: 0.9569, Train: 0.7511, Val: 0.7286, Test: 0.5848\n",
            "Run: 04, Epoch: 058, Loss: 0.9541, Train: 0.7534, Val: 0.7307, Test: 0.5867\n",
            "Run: 04, Epoch: 059, Loss: 0.9521, Train: 0.7552, Val: 0.7317, Test: 0.5888\n",
            "Run: 04, Epoch: 060, Loss: 0.9501, Train: 0.7561, Val: 0.7318, Test: 0.5901\n",
            "Run: 04, Epoch: 061, Loss: 0.9457, Train: 0.7565, Val: 0.7324, Test: 0.5909\n",
            "Run: 04, Epoch: 062, Loss: 0.9445, Train: 0.7566, Val: 0.7323, Test: 0.5908\n",
            "Run: 04, Epoch: 063, Loss: 0.9406, Train: 0.7576, Val: 0.7328, Test: 0.5910\n",
            "Run: 04, Epoch: 064, Loss: 0.9388, Train: 0.7590, Val: 0.7348, Test: 0.5911\n",
            "Run: 04, Epoch: 065, Loss: 0.9392, Train: 0.7597, Val: 0.7351, Test: 0.5913\n",
            "Run: 04, Epoch: 066, Loss: 0.9366, Train: 0.7605, Val: 0.7365, Test: 0.5918\n",
            "Run: 04, Epoch: 067, Loss: 0.9332, Train: 0.7602, Val: 0.7361, Test: 0.5919\n",
            "Run: 04, Epoch: 068, Loss: 0.9324, Train: 0.7604, Val: 0.7357, Test: 0.5921\n",
            "Run: 04, Epoch: 069, Loss: 0.9292, Train: 0.7621, Val: 0.7373, Test: 0.5928\n",
            "Run: 04, Epoch: 070, Loss: 0.9276, Train: 0.7632, Val: 0.7382, Test: 0.5935\n",
            "Run: 04, Epoch: 071, Loss: 0.9269, Train: 0.7642, Val: 0.7384, Test: 0.5937\n",
            "Run: 04, Epoch: 072, Loss: 0.9253, Train: 0.7643, Val: 0.7387, Test: 0.5939\n",
            "Run: 04, Epoch: 073, Loss: 0.9215, Train: 0.7647, Val: 0.7376, Test: 0.5943\n",
            "Run: 04, Epoch: 074, Loss: 0.9197, Train: 0.7654, Val: 0.7390, Test: 0.5950\n",
            "Run: 04, Epoch: 075, Loss: 0.9210, Train: 0.7664, Val: 0.7400, Test: 0.5948\n",
            "Run: 04, Epoch: 076, Loss: 0.9185, Train: 0.7666, Val: 0.7390, Test: 0.5942\n",
            "Run: 04, Epoch: 077, Loss: 0.9172, Train: 0.7669, Val: 0.7391, Test: 0.5943\n",
            "Run: 04, Epoch: 078, Loss: 0.9138, Train: 0.7677, Val: 0.7398, Test: 0.5955\n",
            "Run: 04, Epoch: 079, Loss: 0.9128, Train: 0.7690, Val: 0.7417, Test: 0.5972\n",
            "Run: 04, Epoch: 080, Loss: 0.9100, Train: 0.7702, Val: 0.7426, Test: 0.5982\n",
            "Run: 04, Epoch: 081, Loss: 0.9078, Train: 0.7708, Val: 0.7428, Test: 0.5983\n",
            "Run: 04, Epoch: 082, Loss: 0.9066, Train: 0.7708, Val: 0.7427, Test: 0.5976\n",
            "Run: 04, Epoch: 083, Loss: 0.9045, Train: 0.7706, Val: 0.7426, Test: 0.5969\n",
            "Run: 04, Epoch: 084, Loss: 0.9032, Train: 0.7708, Val: 0.7427, Test: 0.5966\n",
            "Run: 04, Epoch: 085, Loss: 0.9014, Train: 0.7719, Val: 0.7442, Test: 0.5977\n",
            "Run: 04, Epoch: 086, Loss: 0.9016, Train: 0.7733, Val: 0.7450, Test: 0.5986\n",
            "Run: 04, Epoch: 087, Loss: 0.8986, Train: 0.7741, Val: 0.7444, Test: 0.5985\n",
            "Run: 04, Epoch: 088, Loss: 0.8973, Train: 0.7743, Val: 0.7442, Test: 0.5981\n",
            "Run: 04, Epoch: 089, Loss: 0.8950, Train: 0.7744, Val: 0.7443, Test: 0.5984\n",
            "Run: 04, Epoch: 090, Loss: 0.8940, Train: 0.7750, Val: 0.7451, Test: 0.5992\n",
            "Run: 04, Epoch: 091, Loss: 0.8949, Train: 0.7757, Val: 0.7462, Test: 0.6000\n",
            "Run: 04, Epoch: 092, Loss: 0.8908, Train: 0.7764, Val: 0.7462, Test: 0.6003\n",
            "Run: 04, Epoch: 093, Loss: 0.8902, Train: 0.7765, Val: 0.7457, Test: 0.6003\n",
            "Run: 04, Epoch: 094, Loss: 0.8889, Train: 0.7766, Val: 0.7462, Test: 0.6004\n",
            "Run: 04, Epoch: 095, Loss: 0.8884, Train: 0.7774, Val: 0.7468, Test: 0.6014\n",
            "Run: 04, Epoch: 096, Loss: 0.8866, Train: 0.7785, Val: 0.7482, Test: 0.6021\n",
            "Run: 04, Epoch: 097, Loss: 0.8854, Train: 0.7792, Val: 0.7477, Test: 0.6018\n",
            "Run: 04, Epoch: 098, Loss: 0.8841, Train: 0.7798, Val: 0.7483, Test: 0.6012\n",
            "Run: 04, Epoch: 099, Loss: 0.8807, Train: 0.7799, Val: 0.7486, Test: 0.6010\n",
            "Run: 04, Epoch: 100, Loss: 0.8796, Train: 0.7800, Val: 0.7487, Test: 0.6012\n",
            "Run: 04, Epoch: 101, Loss: 0.8798, Train: 0.7802, Val: 0.7487, Test: 0.6020\n",
            "Run: 04, Epoch: 102, Loss: 0.8792, Train: 0.7804, Val: 0.7490, Test: 0.6028\n",
            "Run: 04, Epoch: 103, Loss: 0.8788, Train: 0.7810, Val: 0.7489, Test: 0.6032\n",
            "Run: 04, Epoch: 104, Loss: 0.8780, Train: 0.7818, Val: 0.7494, Test: 0.6039\n",
            "Run: 04, Epoch: 105, Loss: 0.8749, Train: 0.7830, Val: 0.7501, Test: 0.6043\n",
            "Run: 04, Epoch: 106, Loss: 0.8760, Train: 0.7837, Val: 0.7510, Test: 0.6045\n",
            "Run: 04, Epoch: 107, Loss: 0.8717, Train: 0.7840, Val: 0.7516, Test: 0.6045\n",
            "Run: 04, Epoch: 108, Loss: 0.8714, Train: 0.7844, Val: 0.7514, Test: 0.6042\n",
            "Run: 04, Epoch: 109, Loss: 0.8716, Train: 0.7845, Val: 0.7505, Test: 0.6041\n",
            "Run: 04, Epoch: 110, Loss: 0.8692, Train: 0.7853, Val: 0.7513, Test: 0.6049\n",
            "Run: 04, Epoch: 111, Loss: 0.8680, Train: 0.7860, Val: 0.7515, Test: 0.6052\n",
            "Run: 04, Epoch: 112, Loss: 0.8647, Train: 0.7861, Val: 0.7517, Test: 0.6046\n",
            "Run: 04, Epoch: 113, Loss: 0.8647, Train: 0.7861, Val: 0.7511, Test: 0.6042\n",
            "Run: 04, Epoch: 114, Loss: 0.8668, Train: 0.7868, Val: 0.7523, Test: 0.6049\n",
            "Run: 04, Epoch: 115, Loss: 0.8662, Train: 0.7877, Val: 0.7539, Test: 0.6060\n",
            "Run: 04, Epoch: 116, Loss: 0.8632, Train: 0.7880, Val: 0.7529, Test: 0.6064\n",
            "Run: 04, Epoch: 117, Loss: 0.8614, Train: 0.7877, Val: 0.7524, Test: 0.6057\n",
            "Run: 04, Epoch: 118, Loss: 0.8605, Train: 0.7880, Val: 0.7520, Test: 0.6054\n",
            "Run: 04, Epoch: 119, Loss: 0.8633, Train: 0.7881, Val: 0.7524, Test: 0.6049\n",
            "Run: 04, Epoch: 120, Loss: 0.8587, Train: 0.7890, Val: 0.7535, Test: 0.6048\n",
            "Run: 04, Epoch: 121, Loss: 0.8566, Train: 0.7892, Val: 0.7538, Test: 0.6044\n",
            "Run: 04, Epoch: 122, Loss: 0.8556, Train: 0.7896, Val: 0.7536, Test: 0.6046\n",
            "Run: 04, Epoch: 123, Loss: 0.8548, Train: 0.7899, Val: 0.7537, Test: 0.6056\n",
            "Run: 04, Epoch: 124, Loss: 0.8548, Train: 0.7908, Val: 0.7544, Test: 0.6068\n",
            "Run: 04, Epoch: 125, Loss: 0.8543, Train: 0.7915, Val: 0.7555, Test: 0.6074\n",
            "Run: 04, Epoch: 126, Loss: 0.8524, Train: 0.7911, Val: 0.7554, Test: 0.6065\n",
            "Run: 04, Epoch: 127, Loss: 0.8528, Train: 0.7912, Val: 0.7548, Test: 0.6058\n",
            "Run: 04, Epoch: 128, Loss: 0.8483, Train: 0.7917, Val: 0.7555, Test: 0.6062\n",
            "Run: 04, Epoch: 129, Loss: 0.8492, Train: 0.7925, Val: 0.7561, Test: 0.6072\n",
            "Run: 04, Epoch: 130, Loss: 0.8505, Train: 0.7925, Val: 0.7561, Test: 0.6074\n",
            "Run: 04, Epoch: 131, Loss: 0.8480, Train: 0.7928, Val: 0.7562, Test: 0.6076\n",
            "Run: 04, Epoch: 132, Loss: 0.8461, Train: 0.7940, Val: 0.7574, Test: 0.6082\n",
            "Run: 04, Epoch: 133, Loss: 0.8475, Train: 0.7943, Val: 0.7576, Test: 0.6083\n",
            "Run: 04, Epoch: 134, Loss: 0.8443, Train: 0.7942, Val: 0.7572, Test: 0.6078\n",
            "Run: 04, Epoch: 135, Loss: 0.8434, Train: 0.7934, Val: 0.7568, Test: 0.6071\n",
            "Run: 04, Epoch: 136, Loss: 0.8421, Train: 0.7940, Val: 0.7571, Test: 0.6077\n",
            "Run: 04, Epoch: 137, Loss: 0.8432, Train: 0.7951, Val: 0.7579, Test: 0.6083\n",
            "Run: 04, Epoch: 138, Loss: 0.8411, Train: 0.7952, Val: 0.7572, Test: 0.6079\n",
            "Run: 04, Epoch: 139, Loss: 0.8401, Train: 0.7949, Val: 0.7568, Test: 0.6074\n",
            "Run: 04, Epoch: 140, Loss: 0.8376, Train: 0.7955, Val: 0.7570, Test: 0.6083\n",
            "Run: 04, Epoch: 141, Loss: 0.8394, Train: 0.7970, Val: 0.7583, Test: 0.6092\n",
            "Run: 04, Epoch: 142, Loss: 0.8374, Train: 0.7971, Val: 0.7582, Test: 0.6087\n",
            "Run: 04, Epoch: 143, Loss: 0.8364, Train: 0.7970, Val: 0.7583, Test: 0.6082\n",
            "Run: 04, Epoch: 144, Loss: 0.8376, Train: 0.7971, Val: 0.7581, Test: 0.6085\n",
            "Run: 04, Epoch: 145, Loss: 0.8357, Train: 0.7973, Val: 0.7584, Test: 0.6089\n",
            "Run: 04, Epoch: 146, Loss: 0.8349, Train: 0.7977, Val: 0.7584, Test: 0.6094\n",
            "Run: 04, Epoch: 147, Loss: 0.8345, Train: 0.7985, Val: 0.7584, Test: 0.6098\n",
            "Run: 04, Epoch: 148, Loss: 0.8338, Train: 0.7990, Val: 0.7589, Test: 0.6103\n",
            "Run: 04, Epoch: 149, Loss: 0.8334, Train: 0.7991, Val: 0.7586, Test: 0.6101\n",
            "Run: 04, Epoch: 150, Loss: 0.8326, Train: 0.7990, Val: 0.7588, Test: 0.6092\n",
            "Run: 04, Epoch: 151, Loss: 0.8341, Train: 0.7994, Val: 0.7595, Test: 0.6093\n",
            "Run: 04, Epoch: 152, Loss: 0.8301, Train: 0.7997, Val: 0.7595, Test: 0.6099\n",
            "Run: 04, Epoch: 153, Loss: 0.8317, Train: 0.8000, Val: 0.7595, Test: 0.6107\n",
            "Run: 04, Epoch: 154, Loss: 0.8280, Train: 0.8005, Val: 0.7600, Test: 0.6114\n",
            "Run: 04, Epoch: 155, Loss: 0.8283, Train: 0.8011, Val: 0.7604, Test: 0.6120\n",
            "Run: 04, Epoch: 156, Loss: 0.8280, Train: 0.8011, Val: 0.7599, Test: 0.6113\n",
            "Run: 04, Epoch: 157, Loss: 0.8279, Train: 0.8012, Val: 0.7601, Test: 0.6105\n",
            "Run: 04, Epoch: 158, Loss: 0.8270, Train: 0.8012, Val: 0.7606, Test: 0.6104\n",
            "Run: 04, Epoch: 159, Loss: 0.8240, Train: 0.8019, Val: 0.7602, Test: 0.6106\n",
            "Run: 04, Epoch: 160, Loss: 0.8257, Train: 0.8023, Val: 0.7601, Test: 0.6109\n",
            "Run: 04, Epoch: 161, Loss: 0.8215, Train: 0.8026, Val: 0.7605, Test: 0.6111\n",
            "Run: 04, Epoch: 162, Loss: 0.8206, Train: 0.8026, Val: 0.7609, Test: 0.6112\n",
            "Run: 04, Epoch: 163, Loss: 0.8238, Train: 0.8025, Val: 0.7608, Test: 0.6109\n",
            "Run: 04, Epoch: 164, Loss: 0.8203, Train: 0.8031, Val: 0.7605, Test: 0.6108\n",
            "Run: 04, Epoch: 165, Loss: 0.8197, Train: 0.8036, Val: 0.7612, Test: 0.6112\n",
            "Run: 04, Epoch: 166, Loss: 0.8188, Train: 0.8045, Val: 0.7620, Test: 0.6121\n",
            "Run: 04, Epoch: 167, Loss: 0.8192, Train: 0.8046, Val: 0.7620, Test: 0.6128\n",
            "Run: 04, Epoch: 168, Loss: 0.8200, Train: 0.8037, Val: 0.7618, Test: 0.6123\n",
            "Run: 04, Epoch: 169, Loss: 0.8186, Train: 0.8037, Val: 0.7611, Test: 0.6115\n",
            "Run: 04, Epoch: 170, Loss: 0.8166, Train: 0.8042, Val: 0.7615, Test: 0.6110\n",
            "Run: 04, Epoch: 171, Loss: 0.8170, Train: 0.8044, Val: 0.7610, Test: 0.6103\n",
            "Run: 04, Epoch: 172, Loss: 0.8151, Train: 0.8050, Val: 0.7620, Test: 0.6111\n",
            "Run: 04, Epoch: 173, Loss: 0.8146, Train: 0.8058, Val: 0.7627, Test: 0.6127\n",
            "Run: 04, Epoch: 174, Loss: 0.8132, Train: 0.8059, Val: 0.7628, Test: 0.6131\n",
            "Run: 04, Epoch: 175, Loss: 0.8150, Train: 0.8058, Val: 0.7620, Test: 0.6120\n",
            "Run: 04, Epoch: 176, Loss: 0.8120, Train: 0.8067, Val: 0.7622, Test: 0.6125\n",
            "Run: 04, Epoch: 177, Loss: 0.8121, Train: 0.8069, Val: 0.7630, Test: 0.6127\n",
            "Run: 04, Epoch: 178, Loss: 0.8111, Train: 0.8064, Val: 0.7629, Test: 0.6121\n",
            "Run: 04, Epoch: 179, Loss: 0.8104, Train: 0.8068, Val: 0.7630, Test: 0.6127\n",
            "Run: 04, Epoch: 180, Loss: 0.8108, Train: 0.8078, Val: 0.7640, Test: 0.6135\n",
            "Run: 04, Epoch: 181, Loss: 0.8105, Train: 0.8080, Val: 0.7634, Test: 0.6129\n",
            "Run: 04, Epoch: 182, Loss: 0.8102, Train: 0.8076, Val: 0.7631, Test: 0.6126\n",
            "Run: 04, Epoch: 183, Loss: 0.8079, Train: 0.8084, Val: 0.7643, Test: 0.6138\n",
            "Run: 04, Epoch: 184, Loss: 0.8065, Train: 0.8087, Val: 0.7651, Test: 0.6141\n",
            "Run: 04, Epoch: 185, Loss: 0.8070, Train: 0.8085, Val: 0.7639, Test: 0.6134\n",
            "Run: 04, Epoch: 186, Loss: 0.8081, Train: 0.8088, Val: 0.7645, Test: 0.6132\n",
            "Run: 04, Epoch: 187, Loss: 0.8052, Train: 0.8091, Val: 0.7647, Test: 0.6131\n",
            "Run: 04, Epoch: 188, Loss: 0.8069, Train: 0.8089, Val: 0.7643, Test: 0.6125\n",
            "Run: 04, Epoch: 189, Loss: 0.8066, Train: 0.8090, Val: 0.7642, Test: 0.6125\n",
            "Run: 04, Epoch: 190, Loss: 0.8068, Train: 0.8093, Val: 0.7649, Test: 0.6135\n",
            "Run: 04, Epoch: 191, Loss: 0.8068, Train: 0.8097, Val: 0.7650, Test: 0.6135\n",
            "Run: 04, Epoch: 192, Loss: 0.8020, Train: 0.8097, Val: 0.7640, Test: 0.6134\n",
            "Run: 04, Epoch: 193, Loss: 0.8029, Train: 0.8099, Val: 0.7645, Test: 0.6131\n",
            "Run: 04, Epoch: 194, Loss: 0.8004, Train: 0.8098, Val: 0.7643, Test: 0.6130\n",
            "Run: 04, Epoch: 195, Loss: 0.8015, Train: 0.8094, Val: 0.7638, Test: 0.6128\n",
            "Run: 04, Epoch: 196, Loss: 0.7986, Train: 0.8098, Val: 0.7640, Test: 0.6133\n",
            "Run: 04, Epoch: 197, Loss: 0.7996, Train: 0.8110, Val: 0.7650, Test: 0.6147\n",
            "Run: 04, Epoch: 198, Loss: 0.7978, Train: 0.8115, Val: 0.7650, Test: 0.6150\n",
            "Run: 04, Epoch: 199, Loss: 0.7993, Train: 0.8113, Val: 0.7652, Test: 0.6145\n",
            "Run: 04, Epoch: 200, Loss: 0.7985, Train: 0.8111, Val: 0.7653, Test: 0.6146\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9463, Val: 0.9131, Test: 0.8408\n",
            "340527\n",
            "\n",
            "Run 05:\n",
            "\n",
            "Run: 05, Epoch: 001, Loss: 4.1083, Train: 0.3605, Val: 0.3579, Test: 0.2989\n",
            "Run: 05, Epoch: 002, Loss: 3.0213, Train: 0.4609, Val: 0.4598, Test: 0.3877\n",
            "Run: 05, Epoch: 003, Loss: 2.1800, Train: 0.4509, Val: 0.4507, Test: 0.3737\n",
            "Run: 05, Epoch: 004, Loss: 1.9255, Train: 0.5555, Val: 0.5529, Test: 0.4495\n",
            "Run: 05, Epoch: 005, Loss: 1.7981, Train: 0.5702, Val: 0.5605, Test: 0.4647\n",
            "Run: 05, Epoch: 006, Loss: 1.6080, Train: 0.6002, Val: 0.5872, Test: 0.4797\n",
            "Run: 05, Epoch: 007, Loss: 1.5804, Train: 0.6283, Val: 0.6151, Test: 0.4982\n",
            "Run: 05, Epoch: 008, Loss: 1.4650, Train: 0.6300, Val: 0.6171, Test: 0.5030\n",
            "Run: 05, Epoch: 009, Loss: 1.4225, Train: 0.6336, Val: 0.6214, Test: 0.5081\n",
            "Run: 05, Epoch: 010, Loss: 1.3807, Train: 0.6396, Val: 0.6280, Test: 0.5120\n",
            "Run: 05, Epoch: 011, Loss: 1.3351, Train: 0.6434, Val: 0.6331, Test: 0.5160\n",
            "Run: 05, Epoch: 012, Loss: 1.3094, Train: 0.6501, Val: 0.6378, Test: 0.5185\n",
            "Run: 05, Epoch: 013, Loss: 1.2938, Train: 0.6560, Val: 0.6425, Test: 0.5208\n",
            "Run: 05, Epoch: 014, Loss: 1.2674, Train: 0.6632, Val: 0.6508, Test: 0.5275\n",
            "Run: 05, Epoch: 015, Loss: 1.2398, Train: 0.6694, Val: 0.6561, Test: 0.5338\n",
            "Run: 05, Epoch: 016, Loss: 1.2256, Train: 0.6732, Val: 0.6593, Test: 0.5363\n",
            "Run: 05, Epoch: 017, Loss: 1.2147, Train: 0.6805, Val: 0.6662, Test: 0.5394\n",
            "Run: 05, Epoch: 018, Loss: 1.1940, Train: 0.6871, Val: 0.6708, Test: 0.5436\n",
            "Run: 05, Epoch: 019, Loss: 1.1735, Train: 0.6920, Val: 0.6768, Test: 0.5485\n",
            "Run: 05, Epoch: 020, Loss: 1.1592, Train: 0.6948, Val: 0.6803, Test: 0.5522\n",
            "Run: 05, Epoch: 021, Loss: 1.1579, Train: 0.6981, Val: 0.6835, Test: 0.5561\n",
            "Run: 05, Epoch: 022, Loss: 1.1353, Train: 0.7016, Val: 0.6860, Test: 0.5580\n",
            "Run: 05, Epoch: 023, Loss: 1.1261, Train: 0.7051, Val: 0.6889, Test: 0.5588\n",
            "Run: 05, Epoch: 024, Loss: 1.1183, Train: 0.7079, Val: 0.6925, Test: 0.5593\n",
            "Run: 05, Epoch: 025, Loss: 1.1078, Train: 0.7080, Val: 0.6920, Test: 0.5589\n",
            "Run: 05, Epoch: 026, Loss: 1.0983, Train: 0.7100, Val: 0.6961, Test: 0.5610\n",
            "Run: 05, Epoch: 027, Loss: 1.0914, Train: 0.7136, Val: 0.6995, Test: 0.5640\n",
            "Run: 05, Epoch: 028, Loss: 1.0850, Train: 0.7164, Val: 0.6998, Test: 0.5653\n",
            "Run: 05, Epoch: 029, Loss: 1.0784, Train: 0.7185, Val: 0.6997, Test: 0.5654\n",
            "Run: 05, Epoch: 030, Loss: 1.0658, Train: 0.7189, Val: 0.7014, Test: 0.5651\n",
            "Run: 05, Epoch: 031, Loss: 1.0607, Train: 0.7179, Val: 0.6996, Test: 0.5642\n",
            "Run: 05, Epoch: 032, Loss: 1.0574, Train: 0.7195, Val: 0.7007, Test: 0.5655\n",
            "Run: 05, Epoch: 033, Loss: 1.0518, Train: 0.7226, Val: 0.7037, Test: 0.5680\n",
            "Run: 05, Epoch: 034, Loss: 1.0419, Train: 0.7238, Val: 0.7051, Test: 0.5698\n",
            "Run: 05, Epoch: 035, Loss: 1.0388, Train: 0.7243, Val: 0.7055, Test: 0.5706\n",
            "Run: 05, Epoch: 036, Loss: 1.0349, Train: 0.7245, Val: 0.7060, Test: 0.5706\n",
            "Run: 05, Epoch: 037, Loss: 1.0269, Train: 0.7246, Val: 0.7063, Test: 0.5698\n",
            "Run: 05, Epoch: 038, Loss: 1.0225, Train: 0.7249, Val: 0.7065, Test: 0.5688\n",
            "Run: 05, Epoch: 039, Loss: 1.0181, Train: 0.7259, Val: 0.7072, Test: 0.5684\n",
            "Run: 05, Epoch: 040, Loss: 1.0124, Train: 0.7268, Val: 0.7082, Test: 0.5687\n",
            "Run: 05, Epoch: 041, Loss: 1.0117, Train: 0.7273, Val: 0.7086, Test: 0.5693\n",
            "Run: 05, Epoch: 042, Loss: 1.0062, Train: 0.7276, Val: 0.7084, Test: 0.5701\n",
            "Run: 05, Epoch: 043, Loss: 1.0060, Train: 0.7305, Val: 0.7100, Test: 0.5723\n",
            "Run: 05, Epoch: 044, Loss: 0.9970, Train: 0.7322, Val: 0.7108, Test: 0.5731\n",
            "Run: 05, Epoch: 045, Loss: 0.9948, Train: 0.7329, Val: 0.7118, Test: 0.5725\n",
            "Run: 05, Epoch: 046, Loss: 0.9917, Train: 0.7327, Val: 0.7121, Test: 0.5718\n",
            "Run: 05, Epoch: 047, Loss: 0.9885, Train: 0.7334, Val: 0.7126, Test: 0.5718\n",
            "Run: 05, Epoch: 048, Loss: 0.9846, Train: 0.7354, Val: 0.7141, Test: 0.5736\n",
            "Run: 05, Epoch: 049, Loss: 0.9800, Train: 0.7380, Val: 0.7160, Test: 0.5762\n",
            "Run: 05, Epoch: 050, Loss: 0.9774, Train: 0.7396, Val: 0.7181, Test: 0.5784\n",
            "Run: 05, Epoch: 051, Loss: 0.9749, Train: 0.7403, Val: 0.7178, Test: 0.5800\n",
            "Run: 05, Epoch: 052, Loss: 0.9733, Train: 0.7464, Val: 0.7249, Test: 0.5839\n",
            "Run: 05, Epoch: 053, Loss: 0.9699, Train: 0.7426, Val: 0.7208, Test: 0.5805\n",
            "Run: 05, Epoch: 054, Loss: 0.9661, Train: 0.7435, Val: 0.7211, Test: 0.5798\n",
            "Run: 05, Epoch: 055, Loss: 0.9641, Train: 0.7440, Val: 0.7215, Test: 0.5799\n",
            "Run: 05, Epoch: 056, Loss: 0.9622, Train: 0.7502, Val: 0.7286, Test: 0.5848\n",
            "Run: 05, Epoch: 057, Loss: 0.9586, Train: 0.7515, Val: 0.7297, Test: 0.5869\n",
            "Run: 05, Epoch: 058, Loss: 0.9568, Train: 0.7530, Val: 0.7309, Test: 0.5885\n",
            "Run: 05, Epoch: 059, Loss: 0.9571, Train: 0.7533, Val: 0.7306, Test: 0.5886\n",
            "Run: 05, Epoch: 060, Loss: 0.9517, Train: 0.7494, Val: 0.7257, Test: 0.5854\n",
            "Run: 05, Epoch: 061, Loss: 0.9492, Train: 0.7552, Val: 0.7321, Test: 0.5885\n",
            "Run: 05, Epoch: 062, Loss: 0.9484, Train: 0.7567, Val: 0.7344, Test: 0.5892\n",
            "Run: 05, Epoch: 063, Loss: 0.9437, Train: 0.7579, Val: 0.7355, Test: 0.5896\n",
            "Run: 05, Epoch: 064, Loss: 0.9425, Train: 0.7585, Val: 0.7362, Test: 0.5901\n",
            "Run: 05, Epoch: 065, Loss: 0.9412, Train: 0.7586, Val: 0.7362, Test: 0.5909\n",
            "Run: 05, Epoch: 066, Loss: 0.9386, Train: 0.7584, Val: 0.7347, Test: 0.5911\n",
            "Run: 05, Epoch: 067, Loss: 0.9336, Train: 0.7548, Val: 0.7290, Test: 0.5882\n",
            "Run: 05, Epoch: 068, Loss: 0.9350, Train: 0.7609, Val: 0.7364, Test: 0.5923\n",
            "Run: 05, Epoch: 069, Loss: 0.9329, Train: 0.7628, Val: 0.7388, Test: 0.5936\n",
            "Run: 05, Epoch: 070, Loss: 0.9302, Train: 0.7634, Val: 0.7406, Test: 0.5933\n",
            "Run: 05, Epoch: 071, Loss: 0.9291, Train: 0.7631, Val: 0.7410, Test: 0.5927\n",
            "Run: 05, Epoch: 072, Loss: 0.9270, Train: 0.7631, Val: 0.7396, Test: 0.5919\n",
            "Run: 05, Epoch: 073, Loss: 0.9247, Train: 0.7631, Val: 0.7377, Test: 0.5916\n",
            "Run: 05, Epoch: 074, Loss: 0.9219, Train: 0.7646, Val: 0.7388, Test: 0.5933\n",
            "Run: 05, Epoch: 075, Loss: 0.9215, Train: 0.7661, Val: 0.7403, Test: 0.5949\n",
            "Run: 05, Epoch: 076, Loss: 0.9184, Train: 0.7665, Val: 0.7412, Test: 0.5958\n",
            "Run: 05, Epoch: 077, Loss: 0.9192, Train: 0.7661, Val: 0.7415, Test: 0.5951\n",
            "Run: 05, Epoch: 078, Loss: 0.9163, Train: 0.7660, Val: 0.7410, Test: 0.5944\n",
            "Run: 05, Epoch: 079, Loss: 0.9132, Train: 0.7675, Val: 0.7408, Test: 0.5946\n",
            "Run: 05, Epoch: 080, Loss: 0.9119, Train: 0.7693, Val: 0.7428, Test: 0.5961\n",
            "Run: 05, Epoch: 081, Loss: 0.9112, Train: 0.7704, Val: 0.7439, Test: 0.5973\n",
            "Run: 05, Epoch: 082, Loss: 0.9110, Train: 0.7701, Val: 0.7437, Test: 0.5976\n",
            "Run: 05, Epoch: 083, Loss: 0.9059, Train: 0.7691, Val: 0.7424, Test: 0.5968\n",
            "Run: 05, Epoch: 084, Loss: 0.9078, Train: 0.7690, Val: 0.7420, Test: 0.5961\n",
            "Run: 05, Epoch: 085, Loss: 0.9047, Train: 0.7706, Val: 0.7429, Test: 0.5964\n",
            "Run: 05, Epoch: 086, Loss: 0.9035, Train: 0.7723, Val: 0.7446, Test: 0.5971\n",
            "Run: 05, Epoch: 087, Loss: 0.9001, Train: 0.7734, Val: 0.7457, Test: 0.5977\n",
            "Run: 05, Epoch: 088, Loss: 0.9003, Train: 0.7733, Val: 0.7455, Test: 0.5976\n",
            "Run: 05, Epoch: 089, Loss: 0.8994, Train: 0.7731, Val: 0.7453, Test: 0.5981\n",
            "Run: 05, Epoch: 090, Loss: 0.8965, Train: 0.7744, Val: 0.7458, Test: 0.5994\n",
            "Run: 05, Epoch: 091, Loss: 0.8957, Train: 0.7757, Val: 0.7467, Test: 0.6003\n",
            "Run: 05, Epoch: 092, Loss: 0.8933, Train: 0.7761, Val: 0.7469, Test: 0.6001\n",
            "Run: 05, Epoch: 093, Loss: 0.8927, Train: 0.7759, Val: 0.7457, Test: 0.5994\n",
            "Run: 05, Epoch: 094, Loss: 0.8927, Train: 0.7765, Val: 0.7474, Test: 0.6001\n",
            "Run: 05, Epoch: 095, Loss: 0.8902, Train: 0.7780, Val: 0.7491, Test: 0.6018\n",
            "Run: 05, Epoch: 096, Loss: 0.8872, Train: 0.7786, Val: 0.7505, Test: 0.6026\n",
            "Run: 05, Epoch: 097, Loss: 0.8882, Train: 0.7792, Val: 0.7504, Test: 0.6023\n",
            "Run: 05, Epoch: 098, Loss: 0.8840, Train: 0.7795, Val: 0.7492, Test: 0.6015\n",
            "Run: 05, Epoch: 099, Loss: 0.8840, Train: 0.7808, Val: 0.7491, Test: 0.6021\n",
            "Run: 05, Epoch: 100, Loss: 0.8831, Train: 0.7814, Val: 0.7500, Test: 0.6026\n",
            "Run: 05, Epoch: 101, Loss: 0.8803, Train: 0.7813, Val: 0.7505, Test: 0.6027\n",
            "Run: 05, Epoch: 102, Loss: 0.8801, Train: 0.7816, Val: 0.7506, Test: 0.6023\n",
            "Run: 05, Epoch: 103, Loss: 0.8801, Train: 0.7817, Val: 0.7512, Test: 0.6020\n",
            "Run: 05, Epoch: 104, Loss: 0.8790, Train: 0.7831, Val: 0.7518, Test: 0.6028\n",
            "Run: 05, Epoch: 105, Loss: 0.8755, Train: 0.7836, Val: 0.7520, Test: 0.6032\n",
            "Run: 05, Epoch: 106, Loss: 0.8772, Train: 0.7835, Val: 0.7515, Test: 0.6027\n",
            "Run: 05, Epoch: 107, Loss: 0.8728, Train: 0.7831, Val: 0.7511, Test: 0.6022\n",
            "Run: 05, Epoch: 108, Loss: 0.8714, Train: 0.7840, Val: 0.7522, Test: 0.6027\n",
            "Run: 05, Epoch: 109, Loss: 0.8728, Train: 0.7850, Val: 0.7526, Test: 0.6035\n",
            "Run: 05, Epoch: 110, Loss: 0.8723, Train: 0.7861, Val: 0.7532, Test: 0.6042\n",
            "Run: 05, Epoch: 111, Loss: 0.8684, Train: 0.7860, Val: 0.7535, Test: 0.6046\n",
            "Run: 05, Epoch: 112, Loss: 0.8676, Train: 0.7857, Val: 0.7535, Test: 0.6048\n",
            "Run: 05, Epoch: 113, Loss: 0.8676, Train: 0.7869, Val: 0.7535, Test: 0.6052\n",
            "Run: 05, Epoch: 114, Loss: 0.8662, Train: 0.7879, Val: 0.7533, Test: 0.6056\n",
            "Run: 05, Epoch: 115, Loss: 0.8665, Train: 0.7878, Val: 0.7529, Test: 0.6051\n",
            "Run: 05, Epoch: 116, Loss: 0.8648, Train: 0.7874, Val: 0.7523, Test: 0.6043\n",
            "Run: 05, Epoch: 117, Loss: 0.8621, Train: 0.7879, Val: 0.7539, Test: 0.6044\n",
            "Run: 05, Epoch: 118, Loss: 0.8615, Train: 0.7887, Val: 0.7546, Test: 0.6048\n",
            "Run: 05, Epoch: 119, Loss: 0.8604, Train: 0.7891, Val: 0.7550, Test: 0.6050\n",
            "Run: 05, Epoch: 120, Loss: 0.8611, Train: 0.7891, Val: 0.7538, Test: 0.6045\n",
            "Run: 05, Epoch: 121, Loss: 0.8582, Train: 0.7893, Val: 0.7539, Test: 0.6048\n",
            "Run: 05, Epoch: 122, Loss: 0.8578, Train: 0.7905, Val: 0.7555, Test: 0.6060\n",
            "Run: 05, Epoch: 123, Loss: 0.8577, Train: 0.7915, Val: 0.7560, Test: 0.6068\n",
            "Run: 05, Epoch: 124, Loss: 0.8574, Train: 0.7911, Val: 0.7560, Test: 0.6066\n",
            "Run: 05, Epoch: 125, Loss: 0.8508, Train: 0.7908, Val: 0.7562, Test: 0.6062\n",
            "Run: 05, Epoch: 126, Loss: 0.8547, Train: 0.7916, Val: 0.7562, Test: 0.6064\n",
            "Run: 05, Epoch: 127, Loss: 0.8513, Train: 0.7926, Val: 0.7568, Test: 0.6069\n",
            "Run: 05, Epoch: 128, Loss: 0.8500, Train: 0.7929, Val: 0.7571, Test: 0.6071\n",
            "Run: 05, Epoch: 129, Loss: 0.8517, Train: 0.7931, Val: 0.7569, Test: 0.6072\n",
            "Run: 05, Epoch: 130, Loss: 0.8511, Train: 0.7936, Val: 0.7573, Test: 0.6074\n",
            "Run: 05, Epoch: 131, Loss: 0.8488, Train: 0.7940, Val: 0.7577, Test: 0.6071\n",
            "Run: 05, Epoch: 132, Loss: 0.8476, Train: 0.7939, Val: 0.7576, Test: 0.6069\n",
            "Run: 05, Epoch: 133, Loss: 0.8493, Train: 0.7944, Val: 0.7580, Test: 0.6074\n",
            "Run: 05, Epoch: 134, Loss: 0.8464, Train: 0.7950, Val: 0.7583, Test: 0.6078\n",
            "Run: 05, Epoch: 135, Loss: 0.8461, Train: 0.7949, Val: 0.7577, Test: 0.6075\n",
            "Run: 05, Epoch: 136, Loss: 0.8430, Train: 0.7956, Val: 0.7581, Test: 0.6080\n",
            "Run: 05, Epoch: 137, Loss: 0.8417, Train: 0.7961, Val: 0.7579, Test: 0.6082\n",
            "Run: 05, Epoch: 138, Loss: 0.8425, Train: 0.7955, Val: 0.7578, Test: 0.6076\n",
            "Run: 05, Epoch: 139, Loss: 0.8416, Train: 0.7955, Val: 0.7573, Test: 0.6072\n",
            "Run: 05, Epoch: 140, Loss: 0.8408, Train: 0.7970, Val: 0.7587, Test: 0.6083\n",
            "Run: 05, Epoch: 141, Loss: 0.8407, Train: 0.7978, Val: 0.7595, Test: 0.6091\n",
            "Run: 05, Epoch: 142, Loss: 0.8379, Train: 0.7976, Val: 0.7595, Test: 0.6090\n",
            "Run: 05, Epoch: 143, Loss: 0.8357, Train: 0.7976, Val: 0.7590, Test: 0.6090\n",
            "Run: 05, Epoch: 144, Loss: 0.8368, Train: 0.7979, Val: 0.7600, Test: 0.6094\n",
            "Run: 05, Epoch: 145, Loss: 0.8356, Train: 0.7984, Val: 0.7599, Test: 0.6096\n",
            "Run: 05, Epoch: 146, Loss: 0.8330, Train: 0.7985, Val: 0.7597, Test: 0.6090\n",
            "Run: 05, Epoch: 147, Loss: 0.8342, Train: 0.7985, Val: 0.7600, Test: 0.6085\n",
            "Run: 05, Epoch: 148, Loss: 0.8329, Train: 0.7992, Val: 0.7608, Test: 0.6088\n",
            "Run: 05, Epoch: 149, Loss: 0.8339, Train: 0.7995, Val: 0.7601, Test: 0.6090\n",
            "Run: 05, Epoch: 150, Loss: 0.8297, Train: 0.7998, Val: 0.7604, Test: 0.6092\n",
            "Run: 05, Epoch: 151, Loss: 0.8314, Train: 0.8001, Val: 0.7614, Test: 0.6097\n",
            "Run: 05, Epoch: 152, Loss: 0.8301, Train: 0.8005, Val: 0.7609, Test: 0.6102\n",
            "Run: 05, Epoch: 153, Loss: 0.8309, Train: 0.8009, Val: 0.7615, Test: 0.6104\n",
            "Run: 05, Epoch: 154, Loss: 0.8280, Train: 0.8014, Val: 0.7615, Test: 0.6104\n",
            "Run: 05, Epoch: 155, Loss: 0.8292, Train: 0.8010, Val: 0.7603, Test: 0.6102\n",
            "Run: 05, Epoch: 156, Loss: 0.8285, Train: 0.8003, Val: 0.7603, Test: 0.6099\n",
            "Run: 05, Epoch: 157, Loss: 0.8284, Train: 0.8007, Val: 0.7607, Test: 0.6096\n",
            "Run: 05, Epoch: 158, Loss: 0.8253, Train: 0.8020, Val: 0.7606, Test: 0.6094\n",
            "Run: 05, Epoch: 159, Loss: 0.8244, Train: 0.8026, Val: 0.7613, Test: 0.6094\n",
            "Run: 05, Epoch: 160, Loss: 0.8226, Train: 0.8026, Val: 0.7617, Test: 0.6101\n",
            "Run: 05, Epoch: 161, Loss: 0.8221, Train: 0.8026, Val: 0.7623, Test: 0.6108\n",
            "Run: 05, Epoch: 162, Loss: 0.8234, Train: 0.8029, Val: 0.7627, Test: 0.6113\n",
            "Run: 05, Epoch: 163, Loss: 0.8222, Train: 0.8034, Val: 0.7631, Test: 0.6115\n",
            "Run: 05, Epoch: 164, Loss: 0.8182, Train: 0.8038, Val: 0.7624, Test: 0.6110\n",
            "Run: 05, Epoch: 165, Loss: 0.8228, Train: 0.8040, Val: 0.7629, Test: 0.6111\n",
            "Run: 05, Epoch: 166, Loss: 0.8186, Train: 0.8041, Val: 0.7633, Test: 0.6116\n",
            "Run: 05, Epoch: 167, Loss: 0.8187, Train: 0.8041, Val: 0.7633, Test: 0.6115\n",
            "Run: 05, Epoch: 168, Loss: 0.8186, Train: 0.8042, Val: 0.7631, Test: 0.6114\n",
            "Run: 05, Epoch: 169, Loss: 0.8173, Train: 0.8047, Val: 0.7629, Test: 0.6116\n",
            "Run: 05, Epoch: 170, Loss: 0.8170, Train: 0.8047, Val: 0.7630, Test: 0.6119\n",
            "Run: 05, Epoch: 171, Loss: 0.8187, Train: 0.8048, Val: 0.7632, Test: 0.6118\n",
            "Run: 05, Epoch: 172, Loss: 0.8148, Train: 0.8052, Val: 0.7632, Test: 0.6116\n",
            "Run: 05, Epoch: 173, Loss: 0.8136, Train: 0.8054, Val: 0.7636, Test: 0.6112\n",
            "Run: 05, Epoch: 174, Loss: 0.8147, Train: 0.8057, Val: 0.7636, Test: 0.6111\n",
            "Run: 05, Epoch: 175, Loss: 0.8132, Train: 0.8058, Val: 0.7634, Test: 0.6111\n",
            "Run: 05, Epoch: 176, Loss: 0.8128, Train: 0.8064, Val: 0.7644, Test: 0.6119\n",
            "Run: 05, Epoch: 177, Loss: 0.8119, Train: 0.8068, Val: 0.7646, Test: 0.6124\n",
            "Run: 05, Epoch: 178, Loss: 0.8103, Train: 0.8067, Val: 0.7647, Test: 0.6122\n",
            "Run: 05, Epoch: 179, Loss: 0.8105, Train: 0.8068, Val: 0.7645, Test: 0.6121\n",
            "Run: 05, Epoch: 180, Loss: 0.8065, Train: 0.8074, Val: 0.7644, Test: 0.6120\n",
            "Run: 05, Epoch: 181, Loss: 0.8079, Train: 0.8075, Val: 0.7643, Test: 0.6116\n",
            "Run: 05, Epoch: 182, Loss: 0.8071, Train: 0.8075, Val: 0.7645, Test: 0.6109\n",
            "Run: 05, Epoch: 183, Loss: 0.8087, Train: 0.8078, Val: 0.7653, Test: 0.6113\n",
            "Run: 05, Epoch: 184, Loss: 0.8059, Train: 0.8084, Val: 0.7653, Test: 0.6120\n",
            "Run: 05, Epoch: 185, Loss: 0.8050, Train: 0.8089, Val: 0.7651, Test: 0.6127\n",
            "Run: 05, Epoch: 186, Loss: 0.8057, Train: 0.8091, Val: 0.7654, Test: 0.6131\n",
            "Run: 05, Epoch: 187, Loss: 0.8064, Train: 0.8093, Val: 0.7653, Test: 0.6131\n",
            "Run: 05, Epoch: 188, Loss: 0.8045, Train: 0.8096, Val: 0.7653, Test: 0.6130\n",
            "Run: 05, Epoch: 189, Loss: 0.8049, Train: 0.8092, Val: 0.7656, Test: 0.6128\n",
            "Run: 05, Epoch: 190, Loss: 0.8032, Train: 0.8093, Val: 0.7658, Test: 0.6130\n",
            "Run: 05, Epoch: 191, Loss: 0.8047, Train: 0.8095, Val: 0.7664, Test: 0.6131\n",
            "Run: 05, Epoch: 192, Loss: 0.8019, Train: 0.8098, Val: 0.7662, Test: 0.6127\n",
            "Run: 05, Epoch: 193, Loss: 0.8028, Train: 0.8098, Val: 0.7660, Test: 0.6130\n",
            "Run: 05, Epoch: 194, Loss: 0.7996, Train: 0.8104, Val: 0.7660, Test: 0.6137\n",
            "Run: 05, Epoch: 195, Loss: 0.8007, Train: 0.8104, Val: 0.7658, Test: 0.6139\n",
            "Run: 05, Epoch: 196, Loss: 0.7992, Train: 0.8109, Val: 0.7658, Test: 0.6138\n",
            "Run: 05, Epoch: 197, Loss: 0.7978, Train: 0.8115, Val: 0.7663, Test: 0.6140\n",
            "Run: 05, Epoch: 198, Loss: 0.7994, Train: 0.8114, Val: 0.7656, Test: 0.6138\n",
            "Run: 05, Epoch: 199, Loss: 0.7989, Train: 0.8108, Val: 0.7655, Test: 0.6131\n",
            "Run: 05, Epoch: 200, Loss: 0.7989, Train: 0.8117, Val: 0.7659, Test: 0.6134\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9455, Val: 0.9119, Test: 0.8411\n",
            "340527\n",
            "\n",
            "Run 06:\n",
            "\n",
            "Run: 06, Epoch: 001, Loss: 4.0594, Train: 0.3696, Val: 0.3683, Test: 0.3078\n",
            "Run: 06, Epoch: 002, Loss: 3.0417, Train: 0.4831, Val: 0.4822, Test: 0.4112\n",
            "Run: 06, Epoch: 003, Loss: 2.1392, Train: 0.4950, Val: 0.4899, Test: 0.3950\n",
            "Run: 06, Epoch: 004, Loss: 1.8790, Train: 0.5540, Val: 0.5462, Test: 0.4428\n",
            "Run: 06, Epoch: 005, Loss: 1.7541, Train: 0.5631, Val: 0.5563, Test: 0.4549\n",
            "Run: 06, Epoch: 006, Loss: 1.6220, Train: 0.5957, Val: 0.5876, Test: 0.4774\n",
            "Run: 06, Epoch: 007, Loss: 1.5848, Train: 0.6060, Val: 0.5950, Test: 0.4940\n",
            "Run: 06, Epoch: 008, Loss: 1.4728, Train: 0.6313, Val: 0.6201, Test: 0.5032\n",
            "Run: 06, Epoch: 009, Loss: 1.4278, Train: 0.6359, Val: 0.6236, Test: 0.5053\n",
            "Run: 06, Epoch: 010, Loss: 1.3724, Train: 0.6369, Val: 0.6250, Test: 0.5073\n",
            "Run: 06, Epoch: 011, Loss: 1.3294, Train: 0.6374, Val: 0.6251, Test: 0.5094\n",
            "Run: 06, Epoch: 012, Loss: 1.3135, Train: 0.6437, Val: 0.6318, Test: 0.5151\n",
            "Run: 06, Epoch: 013, Loss: 1.2841, Train: 0.6529, Val: 0.6391, Test: 0.5220\n",
            "Run: 06, Epoch: 014, Loss: 1.2581, Train: 0.6640, Val: 0.6487, Test: 0.5299\n",
            "Run: 06, Epoch: 015, Loss: 1.2438, Train: 0.6716, Val: 0.6570, Test: 0.5354\n",
            "Run: 06, Epoch: 016, Loss: 1.2137, Train: 0.6757, Val: 0.6612, Test: 0.5388\n",
            "Run: 06, Epoch: 017, Loss: 1.1921, Train: 0.6784, Val: 0.6645, Test: 0.5421\n",
            "Run: 06, Epoch: 018, Loss: 1.1829, Train: 0.6851, Val: 0.6712, Test: 0.5483\n",
            "Run: 06, Epoch: 019, Loss: 1.1702, Train: 0.6913, Val: 0.6762, Test: 0.5522\n",
            "Run: 06, Epoch: 020, Loss: 1.1550, Train: 0.6964, Val: 0.6799, Test: 0.5541\n",
            "Run: 06, Epoch: 021, Loss: 1.1405, Train: 0.6995, Val: 0.6822, Test: 0.5552\n",
            "Run: 06, Epoch: 022, Loss: 1.1312, Train: 0.7039, Val: 0.6870, Test: 0.5582\n",
            "Run: 06, Epoch: 023, Loss: 1.1158, Train: 0.7089, Val: 0.6915, Test: 0.5610\n",
            "Run: 06, Epoch: 024, Loss: 1.1070, Train: 0.7098, Val: 0.6925, Test: 0.5616\n",
            "Run: 06, Epoch: 025, Loss: 1.0967, Train: 0.7106, Val: 0.6939, Test: 0.5623\n",
            "Run: 06, Epoch: 026, Loss: 1.0911, Train: 0.7127, Val: 0.6968, Test: 0.5639\n",
            "Run: 06, Epoch: 027, Loss: 1.0812, Train: 0.7170, Val: 0.7016, Test: 0.5667\n",
            "Run: 06, Epoch: 028, Loss: 1.0712, Train: 0.7187, Val: 0.7034, Test: 0.5688\n",
            "Run: 06, Epoch: 029, Loss: 1.0659, Train: 0.7207, Val: 0.7028, Test: 0.5696\n",
            "Run: 06, Epoch: 030, Loss: 1.0606, Train: 0.7225, Val: 0.7045, Test: 0.5703\n",
            "Run: 06, Epoch: 031, Loss: 1.0551, Train: 0.7233, Val: 0.7045, Test: 0.5703\n",
            "Run: 06, Epoch: 032, Loss: 1.0458, Train: 0.7231, Val: 0.7042, Test: 0.5698\n",
            "Run: 06, Epoch: 033, Loss: 1.0418, Train: 0.7249, Val: 0.7064, Test: 0.5702\n",
            "Run: 06, Epoch: 034, Loss: 1.0334, Train: 0.7263, Val: 0.7077, Test: 0.5708\n",
            "Run: 06, Epoch: 035, Loss: 1.0298, Train: 0.7275, Val: 0.7088, Test: 0.5712\n",
            "Run: 06, Epoch: 036, Loss: 1.0252, Train: 0.7273, Val: 0.7077, Test: 0.5710\n",
            "Run: 06, Epoch: 037, Loss: 1.0202, Train: 0.7273, Val: 0.7082, Test: 0.5717\n",
            "Run: 06, Epoch: 038, Loss: 1.0154, Train: 0.7285, Val: 0.7083, Test: 0.5729\n",
            "Run: 06, Epoch: 039, Loss: 1.0097, Train: 0.7352, Val: 0.7151, Test: 0.5781\n",
            "Run: 06, Epoch: 040, Loss: 1.0071, Train: 0.7374, Val: 0.7171, Test: 0.5800\n",
            "Run: 06, Epoch: 041, Loss: 1.0015, Train: 0.7374, Val: 0.7167, Test: 0.5803\n",
            "Run: 06, Epoch: 042, Loss: 0.9985, Train: 0.7321, Val: 0.7119, Test: 0.5766\n",
            "Run: 06, Epoch: 043, Loss: 0.9953, Train: 0.7331, Val: 0.7111, Test: 0.5761\n",
            "Run: 06, Epoch: 044, Loss: 0.9924, Train: 0.7361, Val: 0.7143, Test: 0.5772\n",
            "Run: 06, Epoch: 045, Loss: 0.9852, Train: 0.7430, Val: 0.7218, Test: 0.5813\n",
            "Run: 06, Epoch: 046, Loss: 0.9833, Train: 0.7443, Val: 0.7245, Test: 0.5828\n",
            "Run: 06, Epoch: 047, Loss: 0.9826, Train: 0.7453, Val: 0.7263, Test: 0.5844\n",
            "Run: 06, Epoch: 048, Loss: 0.9766, Train: 0.7450, Val: 0.7261, Test: 0.5848\n",
            "Run: 06, Epoch: 049, Loss: 0.9773, Train: 0.7392, Val: 0.7163, Test: 0.5800\n",
            "Run: 06, Epoch: 050, Loss: 0.9743, Train: 0.7413, Val: 0.7177, Test: 0.5809\n",
            "Run: 06, Epoch: 051, Loss: 0.9712, Train: 0.7491, Val: 0.7275, Test: 0.5864\n",
            "Run: 06, Epoch: 052, Loss: 0.9659, Train: 0.7498, Val: 0.7290, Test: 0.5873\n",
            "Run: 06, Epoch: 053, Loss: 0.9622, Train: 0.7496, Val: 0.7291, Test: 0.5869\n",
            "Run: 06, Epoch: 054, Loss: 0.9629, Train: 0.7484, Val: 0.7270, Test: 0.5855\n",
            "Run: 06, Epoch: 055, Loss: 0.9587, Train: 0.7453, Val: 0.7224, Test: 0.5832\n",
            "Run: 06, Epoch: 056, Loss: 0.9541, Train: 0.7525, Val: 0.7309, Test: 0.5889\n",
            "Run: 06, Epoch: 057, Loss: 0.9530, Train: 0.7554, Val: 0.7329, Test: 0.5914\n",
            "Run: 06, Epoch: 058, Loss: 0.9486, Train: 0.7562, Val: 0.7338, Test: 0.5923\n",
            "Run: 06, Epoch: 059, Loss: 0.9481, Train: 0.7562, Val: 0.7333, Test: 0.5913\n",
            "Run: 06, Epoch: 060, Loss: 0.9464, Train: 0.7562, Val: 0.7332, Test: 0.5902\n",
            "Run: 06, Epoch: 061, Loss: 0.9489, Train: 0.7574, Val: 0.7338, Test: 0.5911\n",
            "Run: 06, Epoch: 062, Loss: 0.9406, Train: 0.7581, Val: 0.7352, Test: 0.5920\n",
            "Run: 06, Epoch: 063, Loss: 0.9389, Train: 0.7590, Val: 0.7356, Test: 0.5932\n",
            "Run: 06, Epoch: 064, Loss: 0.9349, Train: 0.7600, Val: 0.7362, Test: 0.5941\n",
            "Run: 06, Epoch: 065, Loss: 0.9351, Train: 0.7611, Val: 0.7364, Test: 0.5942\n",
            "Run: 06, Epoch: 066, Loss: 0.9318, Train: 0.7615, Val: 0.7365, Test: 0.5940\n",
            "Run: 06, Epoch: 067, Loss: 0.9315, Train: 0.7623, Val: 0.7368, Test: 0.5948\n",
            "Run: 06, Epoch: 068, Loss: 0.9290, Train: 0.7630, Val: 0.7382, Test: 0.5960\n",
            "Run: 06, Epoch: 069, Loss: 0.9273, Train: 0.7641, Val: 0.7397, Test: 0.5971\n",
            "Run: 06, Epoch: 070, Loss: 0.9237, Train: 0.7652, Val: 0.7398, Test: 0.5970\n",
            "Run: 06, Epoch: 071, Loss: 0.9226, Train: 0.7648, Val: 0.7387, Test: 0.5957\n",
            "Run: 06, Epoch: 072, Loss: 0.9233, Train: 0.7643, Val: 0.7374, Test: 0.5946\n",
            "Run: 06, Epoch: 073, Loss: 0.9191, Train: 0.7651, Val: 0.7385, Test: 0.5956\n",
            "Run: 06, Epoch: 074, Loss: 0.9188, Train: 0.7665, Val: 0.7406, Test: 0.5975\n",
            "Run: 06, Epoch: 075, Loss: 0.9158, Train: 0.7679, Val: 0.7415, Test: 0.5986\n",
            "Run: 06, Epoch: 076, Loss: 0.9152, Train: 0.7686, Val: 0.7412, Test: 0.5987\n",
            "Run: 06, Epoch: 077, Loss: 0.9154, Train: 0.7691, Val: 0.7407, Test: 0.5979\n",
            "Run: 06, Epoch: 078, Loss: 0.9118, Train: 0.7691, Val: 0.7406, Test: 0.5976\n",
            "Run: 06, Epoch: 079, Loss: 0.9095, Train: 0.7694, Val: 0.7417, Test: 0.5984\n",
            "Run: 06, Epoch: 080, Loss: 0.9095, Train: 0.7705, Val: 0.7428, Test: 0.5998\n",
            "Run: 06, Epoch: 081, Loss: 0.9077, Train: 0.7713, Val: 0.7436, Test: 0.6005\n",
            "Run: 06, Epoch: 082, Loss: 0.9064, Train: 0.7709, Val: 0.7430, Test: 0.5997\n",
            "Run: 06, Epoch: 083, Loss: 0.9027, Train: 0.7714, Val: 0.7430, Test: 0.5994\n",
            "Run: 06, Epoch: 084, Loss: 0.9028, Train: 0.7730, Val: 0.7438, Test: 0.6004\n",
            "Run: 06, Epoch: 085, Loss: 0.9004, Train: 0.7741, Val: 0.7442, Test: 0.6014\n",
            "Run: 06, Epoch: 086, Loss: 0.9006, Train: 0.7738, Val: 0.7449, Test: 0.6013\n",
            "Run: 06, Epoch: 087, Loss: 0.8998, Train: 0.7734, Val: 0.7447, Test: 0.6007\n",
            "Run: 06, Epoch: 088, Loss: 0.8976, Train: 0.7737, Val: 0.7449, Test: 0.6011\n",
            "Run: 06, Epoch: 089, Loss: 0.8948, Train: 0.7758, Val: 0.7465, Test: 0.6027\n",
            "Run: 06, Epoch: 090, Loss: 0.8939, Train: 0.7771, Val: 0.7464, Test: 0.6036\n",
            "Run: 06, Epoch: 091, Loss: 0.8924, Train: 0.7773, Val: 0.7464, Test: 0.6029\n",
            "Run: 06, Epoch: 092, Loss: 0.8902, Train: 0.7773, Val: 0.7463, Test: 0.6019\n",
            "Run: 06, Epoch: 093, Loss: 0.8896, Train: 0.7772, Val: 0.7464, Test: 0.6013\n",
            "Run: 06, Epoch: 094, Loss: 0.8884, Train: 0.7783, Val: 0.7472, Test: 0.6024\n",
            "Run: 06, Epoch: 095, Loss: 0.8879, Train: 0.7788, Val: 0.7479, Test: 0.6036\n",
            "Run: 06, Epoch: 096, Loss: 0.8870, Train: 0.7791, Val: 0.7474, Test: 0.6038\n",
            "Run: 06, Epoch: 097, Loss: 0.8847, Train: 0.7795, Val: 0.7475, Test: 0.6038\n",
            "Run: 06, Epoch: 098, Loss: 0.8810, Train: 0.7806, Val: 0.7485, Test: 0.6043\n",
            "Run: 06, Epoch: 099, Loss: 0.8841, Train: 0.7810, Val: 0.7495, Test: 0.6042\n",
            "Run: 06, Epoch: 100, Loss: 0.8804, Train: 0.7808, Val: 0.7495, Test: 0.6039\n",
            "Run: 06, Epoch: 101, Loss: 0.8808, Train: 0.7811, Val: 0.7494, Test: 0.6037\n",
            "Run: 06, Epoch: 102, Loss: 0.8776, Train: 0.7815, Val: 0.7493, Test: 0.6037\n",
            "Run: 06, Epoch: 103, Loss: 0.8783, Train: 0.7822, Val: 0.7497, Test: 0.6044\n",
            "Run: 06, Epoch: 104, Loss: 0.8770, Train: 0.7826, Val: 0.7504, Test: 0.6049\n",
            "Run: 06, Epoch: 105, Loss: 0.8737, Train: 0.7828, Val: 0.7503, Test: 0.6048\n",
            "Run: 06, Epoch: 106, Loss: 0.8736, Train: 0.7832, Val: 0.7502, Test: 0.6048\n",
            "Run: 06, Epoch: 107, Loss: 0.8707, Train: 0.7838, Val: 0.7502, Test: 0.6049\n",
            "Run: 06, Epoch: 108, Loss: 0.8725, Train: 0.7847, Val: 0.7508, Test: 0.6050\n",
            "Run: 06, Epoch: 109, Loss: 0.8703, Train: 0.7853, Val: 0.7511, Test: 0.6054\n",
            "Run: 06, Epoch: 110, Loss: 0.8680, Train: 0.7857, Val: 0.7513, Test: 0.6056\n",
            "Run: 06, Epoch: 111, Loss: 0.8691, Train: 0.7858, Val: 0.7513, Test: 0.6054\n",
            "Run: 06, Epoch: 112, Loss: 0.8641, Train: 0.7857, Val: 0.7521, Test: 0.6054\n",
            "Run: 06, Epoch: 113, Loss: 0.8676, Train: 0.7870, Val: 0.7526, Test: 0.6064\n",
            "Run: 06, Epoch: 114, Loss: 0.8647, Train: 0.7875, Val: 0.7529, Test: 0.6066\n",
            "Run: 06, Epoch: 115, Loss: 0.8626, Train: 0.7876, Val: 0.7531, Test: 0.6064\n",
            "Run: 06, Epoch: 116, Loss: 0.8654, Train: 0.7880, Val: 0.7534, Test: 0.6064\n",
            "Run: 06, Epoch: 117, Loss: 0.8607, Train: 0.7890, Val: 0.7534, Test: 0.6072\n",
            "Run: 06, Epoch: 118, Loss: 0.8582, Train: 0.7896, Val: 0.7538, Test: 0.6081\n",
            "Run: 06, Epoch: 119, Loss: 0.8612, Train: 0.7894, Val: 0.7541, Test: 0.6080\n",
            "Run: 06, Epoch: 120, Loss: 0.8571, Train: 0.7888, Val: 0.7537, Test: 0.6075\n",
            "Run: 06, Epoch: 121, Loss: 0.8560, Train: 0.7900, Val: 0.7543, Test: 0.6074\n",
            "Run: 06, Epoch: 122, Loss: 0.8555, Train: 0.7911, Val: 0.7545, Test: 0.6078\n",
            "Run: 06, Epoch: 123, Loss: 0.8552, Train: 0.7913, Val: 0.7551, Test: 0.6078\n",
            "Run: 06, Epoch: 124, Loss: 0.8518, Train: 0.7914, Val: 0.7546, Test: 0.6073\n",
            "Run: 06, Epoch: 125, Loss: 0.8535, Train: 0.7906, Val: 0.7538, Test: 0.6069\n",
            "Run: 06, Epoch: 126, Loss: 0.8527, Train: 0.7914, Val: 0.7541, Test: 0.6077\n",
            "Run: 06, Epoch: 127, Loss: 0.8527, Train: 0.7926, Val: 0.7551, Test: 0.6087\n",
            "Run: 06, Epoch: 128, Loss: 0.8516, Train: 0.7926, Val: 0.7555, Test: 0.6088\n",
            "Run: 06, Epoch: 129, Loss: 0.8497, Train: 0.7919, Val: 0.7551, Test: 0.6083\n",
            "Run: 06, Epoch: 130, Loss: 0.8488, Train: 0.7926, Val: 0.7556, Test: 0.6085\n",
            "Run: 06, Epoch: 131, Loss: 0.8482, Train: 0.7935, Val: 0.7566, Test: 0.6091\n",
            "Run: 06, Epoch: 132, Loss: 0.8468, Train: 0.7940, Val: 0.7562, Test: 0.6086\n",
            "Run: 06, Epoch: 133, Loss: 0.8468, Train: 0.7938, Val: 0.7567, Test: 0.6081\n",
            "Run: 06, Epoch: 134, Loss: 0.8432, Train: 0.7945, Val: 0.7576, Test: 0.6086\n",
            "Run: 06, Epoch: 135, Loss: 0.8435, Train: 0.7952, Val: 0.7572, Test: 0.6096\n",
            "Run: 06, Epoch: 136, Loss: 0.8429, Train: 0.7957, Val: 0.7577, Test: 0.6101\n",
            "Run: 06, Epoch: 137, Loss: 0.8426, Train: 0.7962, Val: 0.7581, Test: 0.6104\n",
            "Run: 06, Epoch: 138, Loss: 0.8424, Train: 0.7963, Val: 0.7580, Test: 0.6101\n",
            "Run: 06, Epoch: 139, Loss: 0.8399, Train: 0.7968, Val: 0.7578, Test: 0.6099\n",
            "Run: 06, Epoch: 140, Loss: 0.8402, Train: 0.7975, Val: 0.7587, Test: 0.6099\n",
            "Run: 06, Epoch: 141, Loss: 0.8391, Train: 0.7973, Val: 0.7597, Test: 0.6101\n",
            "Run: 06, Epoch: 142, Loss: 0.8382, Train: 0.7967, Val: 0.7586, Test: 0.6095\n",
            "Run: 06, Epoch: 143, Loss: 0.8360, Train: 0.7973, Val: 0.7582, Test: 0.6098\n",
            "Run: 06, Epoch: 144, Loss: 0.8342, Train: 0.7981, Val: 0.7590, Test: 0.6108\n",
            "Run: 06, Epoch: 145, Loss: 0.8356, Train: 0.7982, Val: 0.7595, Test: 0.6113\n",
            "Run: 06, Epoch: 146, Loss: 0.8348, Train: 0.7985, Val: 0.7601, Test: 0.6114\n",
            "Run: 06, Epoch: 147, Loss: 0.8340, Train: 0.7987, Val: 0.7595, Test: 0.6108\n",
            "Run: 06, Epoch: 148, Loss: 0.8316, Train: 0.7994, Val: 0.7591, Test: 0.6106\n",
            "Run: 06, Epoch: 149, Loss: 0.8328, Train: 0.7998, Val: 0.7594, Test: 0.6104\n",
            "Run: 06, Epoch: 150, Loss: 0.8318, Train: 0.7995, Val: 0.7596, Test: 0.6103\n",
            "Run: 06, Epoch: 151, Loss: 0.8302, Train: 0.7985, Val: 0.7590, Test: 0.6097\n",
            "Run: 06, Epoch: 152, Loss: 0.8311, Train: 0.7999, Val: 0.7600, Test: 0.6106\n",
            "Run: 06, Epoch: 153, Loss: 0.8312, Train: 0.8005, Val: 0.7603, Test: 0.6110\n",
            "Run: 06, Epoch: 154, Loss: 0.8284, Train: 0.8005, Val: 0.7599, Test: 0.6108\n",
            "Run: 06, Epoch: 155, Loss: 0.8301, Train: 0.8002, Val: 0.7602, Test: 0.6110\n",
            "Run: 06, Epoch: 156, Loss: 0.8271, Train: 0.8007, Val: 0.7601, Test: 0.6115\n",
            "Run: 06, Epoch: 157, Loss: 0.8264, Train: 0.8013, Val: 0.7601, Test: 0.6114\n",
            "Run: 06, Epoch: 158, Loss: 0.8264, Train: 0.8013, Val: 0.7591, Test: 0.6105\n",
            "Run: 06, Epoch: 159, Loss: 0.8253, Train: 0.8016, Val: 0.7605, Test: 0.6111\n",
            "Run: 06, Epoch: 160, Loss: 0.8226, Train: 0.8021, Val: 0.7612, Test: 0.6119\n",
            "Run: 06, Epoch: 161, Loss: 0.8241, Train: 0.8024, Val: 0.7615, Test: 0.6123\n",
            "Run: 06, Epoch: 162, Loss: 0.8239, Train: 0.8028, Val: 0.7608, Test: 0.6118\n",
            "Run: 06, Epoch: 163, Loss: 0.8248, Train: 0.8032, Val: 0.7610, Test: 0.6119\n",
            "Run: 06, Epoch: 164, Loss: 0.8214, Train: 0.8036, Val: 0.7624, Test: 0.6124\n",
            "Run: 06, Epoch: 165, Loss: 0.8221, Train: 0.8031, Val: 0.7620, Test: 0.6124\n",
            "Run: 06, Epoch: 166, Loss: 0.8210, Train: 0.8033, Val: 0.7616, Test: 0.6127\n",
            "Run: 06, Epoch: 167, Loss: 0.8196, Train: 0.8042, Val: 0.7626, Test: 0.6132\n",
            "Run: 06, Epoch: 168, Loss: 0.8181, Train: 0.8048, Val: 0.7628, Test: 0.6129\n",
            "Run: 06, Epoch: 169, Loss: 0.8207, Train: 0.8048, Val: 0.7632, Test: 0.6130\n",
            "Run: 06, Epoch: 170, Loss: 0.8177, Train: 0.8051, Val: 0.7631, Test: 0.6132\n",
            "Run: 06, Epoch: 171, Loss: 0.8162, Train: 0.8052, Val: 0.7633, Test: 0.6135\n",
            "Run: 06, Epoch: 172, Loss: 0.8184, Train: 0.8055, Val: 0.7630, Test: 0.6136\n",
            "Run: 06, Epoch: 173, Loss: 0.8145, Train: 0.8055, Val: 0.7621, Test: 0.6135\n",
            "Run: 06, Epoch: 174, Loss: 0.8129, Train: 0.8053, Val: 0.7628, Test: 0.6135\n",
            "Run: 06, Epoch: 175, Loss: 0.8120, Train: 0.8054, Val: 0.7631, Test: 0.6131\n",
            "Run: 06, Epoch: 176, Loss: 0.8133, Train: 0.8056, Val: 0.7633, Test: 0.6130\n",
            "Run: 06, Epoch: 177, Loss: 0.8137, Train: 0.8063, Val: 0.7633, Test: 0.6130\n",
            "Run: 06, Epoch: 178, Loss: 0.8116, Train: 0.8065, Val: 0.7638, Test: 0.6134\n",
            "Run: 06, Epoch: 179, Loss: 0.8111, Train: 0.8067, Val: 0.7635, Test: 0.6134\n",
            "Run: 06, Epoch: 180, Loss: 0.8104, Train: 0.8075, Val: 0.7636, Test: 0.6140\n",
            "Run: 06, Epoch: 181, Loss: 0.8111, Train: 0.8072, Val: 0.7632, Test: 0.6138\n",
            "Run: 06, Epoch: 182, Loss: 0.8108, Train: 0.8069, Val: 0.7636, Test: 0.6138\n",
            "Run: 06, Epoch: 183, Loss: 0.8096, Train: 0.8072, Val: 0.7636, Test: 0.6142\n",
            "Run: 06, Epoch: 184, Loss: 0.8098, Train: 0.8074, Val: 0.7631, Test: 0.6133\n",
            "Run: 06, Epoch: 185, Loss: 0.8081, Train: 0.8077, Val: 0.7636, Test: 0.6126\n",
            "Run: 06, Epoch: 186, Loss: 0.8057, Train: 0.8086, Val: 0.7639, Test: 0.6135\n",
            "Run: 06, Epoch: 187, Loss: 0.8059, Train: 0.8090, Val: 0.7638, Test: 0.6140\n",
            "Run: 06, Epoch: 188, Loss: 0.8060, Train: 0.8085, Val: 0.7636, Test: 0.6140\n",
            "Run: 06, Epoch: 189, Loss: 0.8035, Train: 0.8087, Val: 0.7641, Test: 0.6140\n",
            "Run: 06, Epoch: 190, Loss: 0.8032, Train: 0.8088, Val: 0.7643, Test: 0.6138\n",
            "Run: 06, Epoch: 191, Loss: 0.8040, Train: 0.8088, Val: 0.7639, Test: 0.6138\n",
            "Run: 06, Epoch: 192, Loss: 0.8037, Train: 0.8092, Val: 0.7642, Test: 0.6143\n",
            "Run: 06, Epoch: 193, Loss: 0.8031, Train: 0.8097, Val: 0.7645, Test: 0.6145\n",
            "Run: 06, Epoch: 194, Loss: 0.8032, Train: 0.8102, Val: 0.7645, Test: 0.6146\n",
            "Run: 06, Epoch: 195, Loss: 0.8025, Train: 0.8103, Val: 0.7652, Test: 0.6143\n",
            "Run: 06, Epoch: 196, Loss: 0.8014, Train: 0.8104, Val: 0.7652, Test: 0.6142\n",
            "Run: 06, Epoch: 197, Loss: 0.8000, Train: 0.8106, Val: 0.7650, Test: 0.6142\n",
            "Run: 06, Epoch: 198, Loss: 0.7998, Train: 0.8105, Val: 0.7647, Test: 0.6137\n",
            "Run: 06, Epoch: 199, Loss: 0.7975, Train: 0.8109, Val: 0.7649, Test: 0.6137\n",
            "Run: 06, Epoch: 200, Loss: 0.7995, Train: 0.8112, Val: 0.7653, Test: 0.6142\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9460, Val: 0.9126, Test: 0.8413\n",
            "340527\n",
            "\n",
            "Run 07:\n",
            "\n",
            "Run: 07, Epoch: 001, Loss: 4.0887, Train: 0.3813, Val: 0.3797, Test: 0.3136\n",
            "Run: 07, Epoch: 002, Loss: 3.0122, Train: 0.4764, Val: 0.4713, Test: 0.4021\n",
            "Run: 07, Epoch: 003, Loss: 2.1699, Train: 0.4896, Val: 0.4854, Test: 0.3971\n",
            "Run: 07, Epoch: 004, Loss: 1.8699, Train: 0.5557, Val: 0.5431, Test: 0.4462\n",
            "Run: 07, Epoch: 005, Loss: 1.7670, Train: 0.5749, Val: 0.5677, Test: 0.4669\n",
            "Run: 07, Epoch: 006, Loss: 1.6119, Train: 0.6000, Val: 0.5912, Test: 0.4845\n",
            "Run: 07, Epoch: 007, Loss: 1.5497, Train: 0.6089, Val: 0.5975, Test: 0.4884\n",
            "Run: 07, Epoch: 008, Loss: 1.4650, Train: 0.6311, Val: 0.6199, Test: 0.5070\n",
            "Run: 07, Epoch: 009, Loss: 1.4111, Train: 0.6351, Val: 0.6241, Test: 0.5103\n",
            "Run: 07, Epoch: 010, Loss: 1.3576, Train: 0.6306, Val: 0.6183, Test: 0.5079\n",
            "Run: 07, Epoch: 011, Loss: 1.3278, Train: 0.6392, Val: 0.6275, Test: 0.5143\n",
            "Run: 07, Epoch: 012, Loss: 1.2938, Train: 0.6456, Val: 0.6317, Test: 0.5165\n",
            "Run: 07, Epoch: 013, Loss: 1.2759, Train: 0.6537, Val: 0.6386, Test: 0.5195\n",
            "Run: 07, Epoch: 014, Loss: 1.2500, Train: 0.6584, Val: 0.6428, Test: 0.5211\n",
            "Run: 07, Epoch: 015, Loss: 1.2343, Train: 0.6664, Val: 0.6506, Test: 0.5272\n",
            "Run: 07, Epoch: 016, Loss: 1.2180, Train: 0.6729, Val: 0.6575, Test: 0.5340\n",
            "Run: 07, Epoch: 017, Loss: 1.1998, Train: 0.6763, Val: 0.6616, Test: 0.5381\n",
            "Run: 07, Epoch: 018, Loss: 1.1871, Train: 0.6791, Val: 0.6625, Test: 0.5403\n",
            "Run: 07, Epoch: 019, Loss: 1.1717, Train: 0.6830, Val: 0.6671, Test: 0.5431\n",
            "Run: 07, Epoch: 020, Loss: 1.1603, Train: 0.6932, Val: 0.6764, Test: 0.5496\n",
            "Run: 07, Epoch: 021, Loss: 1.1448, Train: 0.7001, Val: 0.6828, Test: 0.5545\n",
            "Run: 07, Epoch: 022, Loss: 1.1334, Train: 0.7050, Val: 0.6881, Test: 0.5586\n",
            "Run: 07, Epoch: 023, Loss: 1.1225, Train: 0.7080, Val: 0.6915, Test: 0.5606\n",
            "Run: 07, Epoch: 024, Loss: 1.1127, Train: 0.7093, Val: 0.6927, Test: 0.5602\n",
            "Run: 07, Epoch: 025, Loss: 1.1059, Train: 0.7112, Val: 0.6955, Test: 0.5612\n",
            "Run: 07, Epoch: 026, Loss: 1.0959, Train: 0.7124, Val: 0.6976, Test: 0.5622\n",
            "Run: 07, Epoch: 027, Loss: 1.0861, Train: 0.7148, Val: 0.6993, Test: 0.5646\n",
            "Run: 07, Epoch: 028, Loss: 1.0764, Train: 0.7174, Val: 0.7014, Test: 0.5672\n",
            "Run: 07, Epoch: 029, Loss: 1.0687, Train: 0.7182, Val: 0.7020, Test: 0.5678\n",
            "Run: 07, Epoch: 030, Loss: 1.0642, Train: 0.7194, Val: 0.7021, Test: 0.5681\n",
            "Run: 07, Epoch: 031, Loss: 1.0603, Train: 0.7221, Val: 0.7041, Test: 0.5691\n",
            "Run: 07, Epoch: 032, Loss: 1.0522, Train: 0.7235, Val: 0.7059, Test: 0.5689\n",
            "Run: 07, Epoch: 033, Loss: 1.0464, Train: 0.7239, Val: 0.7069, Test: 0.5691\n",
            "Run: 07, Epoch: 034, Loss: 1.0446, Train: 0.7246, Val: 0.7067, Test: 0.5694\n",
            "Run: 07, Epoch: 035, Loss: 1.0335, Train: 0.7252, Val: 0.7068, Test: 0.5696\n",
            "Run: 07, Epoch: 036, Loss: 1.0312, Train: 0.7262, Val: 0.7091, Test: 0.5709\n",
            "Run: 07, Epoch: 037, Loss: 1.0276, Train: 0.7274, Val: 0.7098, Test: 0.5716\n",
            "Run: 07, Epoch: 038, Loss: 1.0198, Train: 0.7271, Val: 0.7093, Test: 0.5713\n",
            "Run: 07, Epoch: 039, Loss: 1.0167, Train: 0.7275, Val: 0.7091, Test: 0.5716\n",
            "Run: 07, Epoch: 040, Loss: 1.0118, Train: 0.7294, Val: 0.7107, Test: 0.5733\n",
            "Run: 07, Epoch: 041, Loss: 1.0066, Train: 0.7312, Val: 0.7128, Test: 0.5748\n",
            "Run: 07, Epoch: 042, Loss: 1.0052, Train: 0.7372, Val: 0.7193, Test: 0.5784\n",
            "Run: 07, Epoch: 043, Loss: 1.0020, Train: 0.7376, Val: 0.7198, Test: 0.5773\n",
            "Run: 07, Epoch: 044, Loss: 0.9993, Train: 0.7313, Val: 0.7131, Test: 0.5719\n",
            "Run: 07, Epoch: 045, Loss: 0.9930, Train: 0.7315, Val: 0.7124, Test: 0.5718\n",
            "Run: 07, Epoch: 046, Loss: 0.9897, Train: 0.7336, Val: 0.7137, Test: 0.5740\n",
            "Run: 07, Epoch: 047, Loss: 0.9858, Train: 0.7367, Val: 0.7164, Test: 0.5773\n",
            "Run: 07, Epoch: 048, Loss: 0.9855, Train: 0.7392, Val: 0.7191, Test: 0.5799\n",
            "Run: 07, Epoch: 049, Loss: 0.9815, Train: 0.7455, Val: 0.7259, Test: 0.5842\n",
            "Run: 07, Epoch: 050, Loss: 0.9770, Train: 0.7457, Val: 0.7264, Test: 0.5839\n",
            "Run: 07, Epoch: 051, Loss: 0.9767, Train: 0.7399, Val: 0.7187, Test: 0.5790\n",
            "Run: 07, Epoch: 052, Loss: 0.9720, Train: 0.7398, Val: 0.7182, Test: 0.5782\n",
            "Run: 07, Epoch: 053, Loss: 0.9683, Train: 0.7423, Val: 0.7196, Test: 0.5796\n",
            "Run: 07, Epoch: 054, Loss: 0.9665, Train: 0.7453, Val: 0.7215, Test: 0.5819\n",
            "Run: 07, Epoch: 055, Loss: 0.9642, Train: 0.7518, Val: 0.7292, Test: 0.5872\n",
            "Run: 07, Epoch: 056, Loss: 0.9616, Train: 0.7521, Val: 0.7298, Test: 0.5880\n",
            "Run: 07, Epoch: 057, Loss: 0.9579, Train: 0.7520, Val: 0.7294, Test: 0.5877\n",
            "Run: 07, Epoch: 058, Loss: 0.9584, Train: 0.7520, Val: 0.7291, Test: 0.5871\n",
            "Run: 07, Epoch: 059, Loss: 0.9543, Train: 0.7538, Val: 0.7311, Test: 0.5881\n",
            "Run: 07, Epoch: 060, Loss: 0.9519, Train: 0.7558, Val: 0.7326, Test: 0.5895\n",
            "Run: 07, Epoch: 061, Loss: 0.9464, Train: 0.7568, Val: 0.7331, Test: 0.5902\n",
            "Run: 07, Epoch: 062, Loss: 0.9482, Train: 0.7573, Val: 0.7337, Test: 0.5902\n",
            "Run: 07, Epoch: 063, Loss: 0.9446, Train: 0.7568, Val: 0.7319, Test: 0.5894\n",
            "Run: 07, Epoch: 064, Loss: 0.9428, Train: 0.7574, Val: 0.7332, Test: 0.5901\n",
            "Run: 07, Epoch: 065, Loss: 0.9412, Train: 0.7596, Val: 0.7355, Test: 0.5924\n",
            "Run: 07, Epoch: 066, Loss: 0.9393, Train: 0.7613, Val: 0.7368, Test: 0.5939\n",
            "Run: 07, Epoch: 067, Loss: 0.9392, Train: 0.7614, Val: 0.7368, Test: 0.5933\n",
            "Run: 07, Epoch: 068, Loss: 0.9340, Train: 0.7607, Val: 0.7350, Test: 0.5914\n",
            "Run: 07, Epoch: 069, Loss: 0.9327, Train: 0.7549, Val: 0.7283, Test: 0.5870\n",
            "Run: 07, Epoch: 070, Loss: 0.9321, Train: 0.7617, Val: 0.7360, Test: 0.5924\n",
            "Run: 07, Epoch: 071, Loss: 0.9281, Train: 0.7637, Val: 0.7382, Test: 0.5947\n",
            "Run: 07, Epoch: 072, Loss: 0.9263, Train: 0.7640, Val: 0.7387, Test: 0.5950\n",
            "Run: 07, Epoch: 073, Loss: 0.9286, Train: 0.7638, Val: 0.7380, Test: 0.5936\n",
            "Run: 07, Epoch: 074, Loss: 0.9264, Train: 0.7632, Val: 0.7360, Test: 0.5917\n",
            "Run: 07, Epoch: 075, Loss: 0.9230, Train: 0.7643, Val: 0.7364, Test: 0.5924\n",
            "Run: 07, Epoch: 076, Loss: 0.9203, Train: 0.7670, Val: 0.7401, Test: 0.5956\n",
            "Run: 07, Epoch: 077, Loss: 0.9175, Train: 0.7680, Val: 0.7413, Test: 0.5971\n",
            "Run: 07, Epoch: 078, Loss: 0.9176, Train: 0.7679, Val: 0.7412, Test: 0.5970\n",
            "Run: 07, Epoch: 079, Loss: 0.9134, Train: 0.7682, Val: 0.7400, Test: 0.5963\n",
            "Run: 07, Epoch: 080, Loss: 0.9152, Train: 0.7680, Val: 0.7398, Test: 0.5954\n",
            "Run: 07, Epoch: 081, Loss: 0.9122, Train: 0.7687, Val: 0.7396, Test: 0.5954\n",
            "Run: 07, Epoch: 082, Loss: 0.9117, Train: 0.7699, Val: 0.7408, Test: 0.5968\n",
            "Run: 07, Epoch: 083, Loss: 0.9109, Train: 0.7715, Val: 0.7428, Test: 0.5983\n",
            "Run: 07, Epoch: 084, Loss: 0.9081, Train: 0.7721, Val: 0.7436, Test: 0.5985\n",
            "Run: 07, Epoch: 085, Loss: 0.9075, Train: 0.7718, Val: 0.7430, Test: 0.5980\n",
            "Run: 07, Epoch: 086, Loss: 0.9067, Train: 0.7723, Val: 0.7429, Test: 0.5987\n",
            "Run: 07, Epoch: 087, Loss: 0.9024, Train: 0.7741, Val: 0.7441, Test: 0.6006\n",
            "Run: 07, Epoch: 088, Loss: 0.9031, Train: 0.7750, Val: 0.7449, Test: 0.6013\n",
            "Run: 07, Epoch: 089, Loss: 0.9040, Train: 0.7742, Val: 0.7441, Test: 0.6001\n",
            "Run: 07, Epoch: 090, Loss: 0.9009, Train: 0.7727, Val: 0.7423, Test: 0.5984\n",
            "Run: 07, Epoch: 091, Loss: 0.8994, Train: 0.7744, Val: 0.7443, Test: 0.5990\n",
            "Run: 07, Epoch: 092, Loss: 0.8976, Train: 0.7764, Val: 0.7462, Test: 0.6004\n",
            "Run: 07, Epoch: 093, Loss: 0.8931, Train: 0.7770, Val: 0.7460, Test: 0.6005\n",
            "Run: 07, Epoch: 094, Loss: 0.8919, Train: 0.7772, Val: 0.7461, Test: 0.6001\n",
            "Run: 07, Epoch: 095, Loss: 0.8897, Train: 0.7768, Val: 0.7457, Test: 0.6004\n",
            "Run: 07, Epoch: 096, Loss: 0.8895, Train: 0.7774, Val: 0.7475, Test: 0.6019\n",
            "Run: 07, Epoch: 097, Loss: 0.8877, Train: 0.7784, Val: 0.7485, Test: 0.6031\n",
            "Run: 07, Epoch: 098, Loss: 0.8880, Train: 0.7795, Val: 0.7485, Test: 0.6030\n",
            "Run: 07, Epoch: 099, Loss: 0.8855, Train: 0.7791, Val: 0.7476, Test: 0.6015\n",
            "Run: 07, Epoch: 100, Loss: 0.8856, Train: 0.7785, Val: 0.7475, Test: 0.6005\n",
            "Run: 07, Epoch: 101, Loss: 0.8853, Train: 0.7787, Val: 0.7475, Test: 0.6010\n",
            "Run: 07, Epoch: 102, Loss: 0.8820, Train: 0.7802, Val: 0.7486, Test: 0.6031\n",
            "Run: 07, Epoch: 103, Loss: 0.8830, Train: 0.7811, Val: 0.7492, Test: 0.6035\n",
            "Run: 07, Epoch: 104, Loss: 0.8814, Train: 0.7805, Val: 0.7485, Test: 0.6025\n",
            "Run: 07, Epoch: 105, Loss: 0.8778, Train: 0.7809, Val: 0.7487, Test: 0.6021\n",
            "Run: 07, Epoch: 106, Loss: 0.8777, Train: 0.7818, Val: 0.7487, Test: 0.6027\n",
            "Run: 07, Epoch: 107, Loss: 0.8787, Train: 0.7823, Val: 0.7496, Test: 0.6036\n",
            "Run: 07, Epoch: 108, Loss: 0.8766, Train: 0.7824, Val: 0.7498, Test: 0.6040\n",
            "Run: 07, Epoch: 109, Loss: 0.8736, Train: 0.7827, Val: 0.7492, Test: 0.6037\n",
            "Run: 07, Epoch: 110, Loss: 0.8751, Train: 0.7833, Val: 0.7493, Test: 0.6036\n",
            "Run: 07, Epoch: 111, Loss: 0.8732, Train: 0.7846, Val: 0.7496, Test: 0.6041\n",
            "Run: 07, Epoch: 112, Loss: 0.8710, Train: 0.7853, Val: 0.7501, Test: 0.6047\n",
            "Run: 07, Epoch: 113, Loss: 0.8718, Train: 0.7858, Val: 0.7512, Test: 0.6051\n",
            "Run: 07, Epoch: 114, Loss: 0.8690, Train: 0.7857, Val: 0.7519, Test: 0.6053\n",
            "Run: 07, Epoch: 115, Loss: 0.8696, Train: 0.7858, Val: 0.7514, Test: 0.6045\n",
            "Run: 07, Epoch: 116, Loss: 0.8673, Train: 0.7858, Val: 0.7509, Test: 0.6041\n",
            "Run: 07, Epoch: 117, Loss: 0.8668, Train: 0.7868, Val: 0.7509, Test: 0.6047\n",
            "Run: 07, Epoch: 118, Loss: 0.8638, Train: 0.7880, Val: 0.7520, Test: 0.6055\n",
            "Run: 07, Epoch: 119, Loss: 0.8649, Train: 0.7875, Val: 0.7524, Test: 0.6054\n",
            "Run: 07, Epoch: 120, Loss: 0.8623, Train: 0.7877, Val: 0.7527, Test: 0.6058\n",
            "Run: 07, Epoch: 121, Loss: 0.8630, Train: 0.7886, Val: 0.7531, Test: 0.6068\n",
            "Run: 07, Epoch: 122, Loss: 0.8603, Train: 0.7895, Val: 0.7537, Test: 0.6073\n",
            "Run: 07, Epoch: 123, Loss: 0.8588, Train: 0.7895, Val: 0.7527, Test: 0.6065\n",
            "Run: 07, Epoch: 124, Loss: 0.8590, Train: 0.7894, Val: 0.7531, Test: 0.6062\n",
            "Run: 07, Epoch: 125, Loss: 0.8559, Train: 0.7900, Val: 0.7541, Test: 0.6064\n",
            "Run: 07, Epoch: 126, Loss: 0.8569, Train: 0.7905, Val: 0.7543, Test: 0.6070\n",
            "Run: 07, Epoch: 127, Loss: 0.8573, Train: 0.7908, Val: 0.7540, Test: 0.6070\n",
            "Run: 07, Epoch: 128, Loss: 0.8542, Train: 0.7908, Val: 0.7541, Test: 0.6064\n",
            "Run: 07, Epoch: 129, Loss: 0.8548, Train: 0.7906, Val: 0.7537, Test: 0.6062\n",
            "Run: 07, Epoch: 130, Loss: 0.8544, Train: 0.7915, Val: 0.7545, Test: 0.6071\n",
            "Run: 07, Epoch: 131, Loss: 0.8526, Train: 0.7922, Val: 0.7554, Test: 0.6080\n",
            "Run: 07, Epoch: 132, Loss: 0.8512, Train: 0.7925, Val: 0.7560, Test: 0.6083\n",
            "Run: 07, Epoch: 133, Loss: 0.8521, Train: 0.7922, Val: 0.7553, Test: 0.6079\n",
            "Run: 07, Epoch: 134, Loss: 0.8509, Train: 0.7925, Val: 0.7554, Test: 0.6078\n",
            "Run: 07, Epoch: 135, Loss: 0.8481, Train: 0.7933, Val: 0.7551, Test: 0.6082\n",
            "Run: 07, Epoch: 136, Loss: 0.8489, Train: 0.7935, Val: 0.7554, Test: 0.6082\n",
            "Run: 07, Epoch: 137, Loss: 0.8472, Train: 0.7934, Val: 0.7558, Test: 0.6079\n",
            "Run: 07, Epoch: 138, Loss: 0.8452, Train: 0.7945, Val: 0.7562, Test: 0.6083\n",
            "Run: 07, Epoch: 139, Loss: 0.8452, Train: 0.7954, Val: 0.7571, Test: 0.6091\n",
            "Run: 07, Epoch: 140, Loss: 0.8418, Train: 0.7954, Val: 0.7567, Test: 0.6096\n",
            "Run: 07, Epoch: 141, Loss: 0.8438, Train: 0.7945, Val: 0.7558, Test: 0.6086\n",
            "Run: 07, Epoch: 142, Loss: 0.8419, Train: 0.7944, Val: 0.7559, Test: 0.6086\n",
            "Run: 07, Epoch: 143, Loss: 0.8426, Train: 0.7960, Val: 0.7568, Test: 0.6095\n",
            "Run: 07, Epoch: 144, Loss: 0.8378, Train: 0.7965, Val: 0.7567, Test: 0.6089\n",
            "Run: 07, Epoch: 145, Loss: 0.8422, Train: 0.7952, Val: 0.7557, Test: 0.6073\n",
            "Run: 07, Epoch: 146, Loss: 0.8393, Train: 0.7952, Val: 0.7557, Test: 0.6069\n",
            "Run: 07, Epoch: 147, Loss: 0.8376, Train: 0.7974, Val: 0.7572, Test: 0.6088\n",
            "Run: 07, Epoch: 148, Loss: 0.8367, Train: 0.7980, Val: 0.7581, Test: 0.6093\n",
            "Run: 07, Epoch: 149, Loss: 0.8381, Train: 0.7979, Val: 0.7572, Test: 0.6093\n",
            "Run: 07, Epoch: 150, Loss: 0.8367, Train: 0.7973, Val: 0.7572, Test: 0.6090\n",
            "Run: 07, Epoch: 151, Loss: 0.8359, Train: 0.7982, Val: 0.7579, Test: 0.6095\n",
            "Run: 07, Epoch: 152, Loss: 0.8364, Train: 0.7985, Val: 0.7578, Test: 0.6091\n",
            "Run: 07, Epoch: 153, Loss: 0.8345, Train: 0.7981, Val: 0.7575, Test: 0.6084\n",
            "Run: 07, Epoch: 154, Loss: 0.8335, Train: 0.7984, Val: 0.7580, Test: 0.6087\n",
            "Run: 07, Epoch: 155, Loss: 0.8316, Train: 0.7991, Val: 0.7588, Test: 0.6096\n",
            "Run: 07, Epoch: 156, Loss: 0.8314, Train: 0.7996, Val: 0.7595, Test: 0.6104\n",
            "Run: 07, Epoch: 157, Loss: 0.8317, Train: 0.7993, Val: 0.7596, Test: 0.6101\n",
            "Run: 07, Epoch: 158, Loss: 0.8309, Train: 0.8003, Val: 0.7591, Test: 0.6102\n",
            "Run: 07, Epoch: 159, Loss: 0.8306, Train: 0.8008, Val: 0.7590, Test: 0.6102\n",
            "Run: 07, Epoch: 160, Loss: 0.8282, Train: 0.8007, Val: 0.7594, Test: 0.6101\n",
            "Run: 07, Epoch: 161, Loss: 0.8280, Train: 0.8008, Val: 0.7600, Test: 0.6103\n",
            "Run: 07, Epoch: 162, Loss: 0.8272, Train: 0.8008, Val: 0.7597, Test: 0.6107\n",
            "Run: 07, Epoch: 163, Loss: 0.8259, Train: 0.8016, Val: 0.7597, Test: 0.6113\n",
            "Run: 07, Epoch: 164, Loss: 0.8268, Train: 0.8021, Val: 0.7606, Test: 0.6116\n",
            "Run: 07, Epoch: 165, Loss: 0.8243, Train: 0.8025, Val: 0.7602, Test: 0.6113\n",
            "Run: 07, Epoch: 166, Loss: 0.8236, Train: 0.8021, Val: 0.7600, Test: 0.6111\n",
            "Run: 07, Epoch: 167, Loss: 0.8249, Train: 0.8025, Val: 0.7609, Test: 0.6115\n",
            "Run: 07, Epoch: 168, Loss: 0.8224, Train: 0.8024, Val: 0.7610, Test: 0.6114\n",
            "Run: 07, Epoch: 169, Loss: 0.8226, Train: 0.8028, Val: 0.7610, Test: 0.6115\n",
            "Run: 07, Epoch: 170, Loss: 0.8210, Train: 0.8033, Val: 0.7610, Test: 0.6117\n",
            "Run: 07, Epoch: 171, Loss: 0.8203, Train: 0.8029, Val: 0.7608, Test: 0.6114\n",
            "Run: 07, Epoch: 172, Loss: 0.8187, Train: 0.8032, Val: 0.7610, Test: 0.6115\n",
            "Run: 07, Epoch: 173, Loss: 0.8181, Train: 0.8037, Val: 0.7609, Test: 0.6123\n",
            "Run: 07, Epoch: 174, Loss: 0.8199, Train: 0.8037, Val: 0.7608, Test: 0.6121\n",
            "Run: 07, Epoch: 175, Loss: 0.8190, Train: 0.8034, Val: 0.7606, Test: 0.6113\n",
            "Run: 07, Epoch: 176, Loss: 0.8163, Train: 0.8045, Val: 0.7606, Test: 0.6118\n",
            "Run: 07, Epoch: 177, Loss: 0.8168, Train: 0.8046, Val: 0.7617, Test: 0.6118\n",
            "Run: 07, Epoch: 178, Loss: 0.8160, Train: 0.8048, Val: 0.7609, Test: 0.6112\n",
            "Run: 07, Epoch: 179, Loss: 0.8155, Train: 0.8056, Val: 0.7616, Test: 0.6116\n",
            "Run: 07, Epoch: 180, Loss: 0.8134, Train: 0.8059, Val: 0.7624, Test: 0.6118\n",
            "Run: 07, Epoch: 181, Loss: 0.8132, Train: 0.8059, Val: 0.7623, Test: 0.6121\n",
            "Run: 07, Epoch: 182, Loss: 0.8134, Train: 0.8060, Val: 0.7625, Test: 0.6123\n",
            "Run: 07, Epoch: 183, Loss: 0.8114, Train: 0.8064, Val: 0.7626, Test: 0.6125\n",
            "Run: 07, Epoch: 184, Loss: 0.8100, Train: 0.8071, Val: 0.7627, Test: 0.6127\n",
            "Run: 07, Epoch: 185, Loss: 0.8095, Train: 0.8072, Val: 0.7624, Test: 0.6124\n",
            "Run: 07, Epoch: 186, Loss: 0.8130, Train: 0.8074, Val: 0.7630, Test: 0.6121\n",
            "Run: 07, Epoch: 187, Loss: 0.8098, Train: 0.8080, Val: 0.7637, Test: 0.6123\n",
            "Run: 07, Epoch: 188, Loss: 0.8093, Train: 0.8080, Val: 0.7638, Test: 0.6125\n",
            "Run: 07, Epoch: 189, Loss: 0.8085, Train: 0.8080, Val: 0.7633, Test: 0.6129\n",
            "Run: 07, Epoch: 190, Loss: 0.8079, Train: 0.8086, Val: 0.7635, Test: 0.6138\n",
            "Run: 07, Epoch: 191, Loss: 0.8095, Train: 0.8087, Val: 0.7640, Test: 0.6138\n",
            "Run: 07, Epoch: 192, Loss: 0.8079, Train: 0.8081, Val: 0.7630, Test: 0.6131\n",
            "Run: 07, Epoch: 193, Loss: 0.8096, Train: 0.8085, Val: 0.7638, Test: 0.6133\n",
            "Run: 07, Epoch: 194, Loss: 0.8054, Train: 0.8090, Val: 0.7637, Test: 0.6133\n",
            "Run: 07, Epoch: 195, Loss: 0.8051, Train: 0.8090, Val: 0.7633, Test: 0.6126\n",
            "Run: 07, Epoch: 196, Loss: 0.8031, Train: 0.8095, Val: 0.7638, Test: 0.6124\n",
            "Run: 07, Epoch: 197, Loss: 0.8038, Train: 0.8101, Val: 0.7644, Test: 0.6130\n",
            "Run: 07, Epoch: 198, Loss: 0.8043, Train: 0.8100, Val: 0.7643, Test: 0.6137\n",
            "Run: 07, Epoch: 199, Loss: 0.8022, Train: 0.8096, Val: 0.7645, Test: 0.6141\n",
            "Run: 07, Epoch: 200, Loss: 0.8059, Train: 0.8104, Val: 0.7652, Test: 0.6149\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9454, Val: 0.9125, Test: 0.8397\n",
            "340527\n",
            "\n",
            "Run 08:\n",
            "\n",
            "Run: 08, Epoch: 001, Loss: 4.0930, Train: 0.3851, Val: 0.3829, Test: 0.3214\n",
            "Run: 08, Epoch: 002, Loss: 2.9601, Train: 0.4592, Val: 0.4538, Test: 0.3769\n",
            "Run: 08, Epoch: 003, Loss: 2.1147, Train: 0.4362, Val: 0.4343, Test: 0.3705\n",
            "Run: 08, Epoch: 004, Loss: 1.8596, Train: 0.5250, Val: 0.5198, Test: 0.4309\n",
            "Run: 08, Epoch: 005, Loss: 1.7693, Train: 0.5781, Val: 0.5711, Test: 0.4711\n",
            "Run: 08, Epoch: 006, Loss: 1.6196, Train: 0.5923, Val: 0.5818, Test: 0.4753\n",
            "Run: 08, Epoch: 007, Loss: 1.5489, Train: 0.6127, Val: 0.6001, Test: 0.4923\n",
            "Run: 08, Epoch: 008, Loss: 1.4791, Train: 0.6245, Val: 0.6116, Test: 0.4984\n",
            "Run: 08, Epoch: 009, Loss: 1.4087, Train: 0.6260, Val: 0.6136, Test: 0.5002\n",
            "Run: 08, Epoch: 010, Loss: 1.3607, Train: 0.6247, Val: 0.6150, Test: 0.5007\n",
            "Run: 08, Epoch: 011, Loss: 1.3318, Train: 0.6263, Val: 0.6134, Test: 0.5010\n",
            "Run: 08, Epoch: 012, Loss: 1.3155, Train: 0.6287, Val: 0.6137, Test: 0.5012\n",
            "Run: 08, Epoch: 013, Loss: 1.2738, Train: 0.6527, Val: 0.6349, Test: 0.5182\n",
            "Run: 08, Epoch: 014, Loss: 1.2526, Train: 0.6636, Val: 0.6492, Test: 0.5265\n",
            "Run: 08, Epoch: 015, Loss: 1.2469, Train: 0.6682, Val: 0.6539, Test: 0.5308\n",
            "Run: 08, Epoch: 016, Loss: 1.2189, Train: 0.6676, Val: 0.6556, Test: 0.5316\n",
            "Run: 08, Epoch: 017, Loss: 1.2039, Train: 0.6792, Val: 0.6660, Test: 0.5407\n",
            "Run: 08, Epoch: 018, Loss: 1.1873, Train: 0.6852, Val: 0.6714, Test: 0.5455\n",
            "Run: 08, Epoch: 019, Loss: 1.1703, Train: 0.6918, Val: 0.6760, Test: 0.5498\n",
            "Run: 08, Epoch: 020, Loss: 1.1545, Train: 0.6958, Val: 0.6781, Test: 0.5524\n",
            "Run: 08, Epoch: 021, Loss: 1.1432, Train: 0.7006, Val: 0.6827, Test: 0.5577\n",
            "Run: 08, Epoch: 022, Loss: 1.1284, Train: 0.7068, Val: 0.6903, Test: 0.5638\n",
            "Run: 08, Epoch: 023, Loss: 1.1154, Train: 0.7092, Val: 0.6928, Test: 0.5648\n",
            "Run: 08, Epoch: 024, Loss: 1.1121, Train: 0.7106, Val: 0.6944, Test: 0.5634\n",
            "Run: 08, Epoch: 025, Loss: 1.1004, Train: 0.7123, Val: 0.6947, Test: 0.5625\n",
            "Run: 08, Epoch: 026, Loss: 1.0897, Train: 0.7137, Val: 0.6968, Test: 0.5630\n",
            "Run: 08, Epoch: 027, Loss: 1.0820, Train: 0.7162, Val: 0.6991, Test: 0.5646\n",
            "Run: 08, Epoch: 028, Loss: 1.0728, Train: 0.7196, Val: 0.7012, Test: 0.5676\n",
            "Run: 08, Epoch: 029, Loss: 1.0666, Train: 0.7217, Val: 0.7034, Test: 0.5702\n",
            "Run: 08, Epoch: 030, Loss: 1.0565, Train: 0.7271, Val: 0.7103, Test: 0.5745\n",
            "Run: 08, Epoch: 031, Loss: 1.0538, Train: 0.7237, Val: 0.7054, Test: 0.5706\n",
            "Run: 08, Epoch: 032, Loss: 1.0464, Train: 0.7233, Val: 0.7052, Test: 0.5689\n",
            "Run: 08, Epoch: 033, Loss: 1.0378, Train: 0.7232, Val: 0.7055, Test: 0.5678\n",
            "Run: 08, Epoch: 034, Loss: 1.0371, Train: 0.7252, Val: 0.7074, Test: 0.5690\n",
            "Run: 08, Epoch: 035, Loss: 1.0314, Train: 0.7282, Val: 0.7092, Test: 0.5723\n",
            "Run: 08, Epoch: 036, Loss: 1.0245, Train: 0.7351, Val: 0.7183, Test: 0.5789\n",
            "Run: 08, Epoch: 037, Loss: 1.0224, Train: 0.7359, Val: 0.7184, Test: 0.5797\n",
            "Run: 08, Epoch: 038, Loss: 1.0150, Train: 0.7296, Val: 0.7114, Test: 0.5742\n",
            "Run: 08, Epoch: 039, Loss: 1.0100, Train: 0.7282, Val: 0.7087, Test: 0.5713\n",
            "Run: 08, Epoch: 040, Loss: 1.0066, Train: 0.7286, Val: 0.7087, Test: 0.5700\n",
            "Run: 08, Epoch: 041, Loss: 1.0004, Train: 0.7303, Val: 0.7108, Test: 0.5708\n",
            "Run: 08, Epoch: 042, Loss: 0.9973, Train: 0.7336, Val: 0.7139, Test: 0.5738\n",
            "Run: 08, Epoch: 043, Loss: 0.9950, Train: 0.7401, Val: 0.7214, Test: 0.5798\n",
            "Run: 08, Epoch: 044, Loss: 0.9894, Train: 0.7411, Val: 0.7224, Test: 0.5814\n",
            "Run: 08, Epoch: 045, Loss: 0.9883, Train: 0.7372, Val: 0.7158, Test: 0.5779\n",
            "Run: 08, Epoch: 046, Loss: 0.9830, Train: 0.7366, Val: 0.7141, Test: 0.5762\n",
            "Run: 08, Epoch: 047, Loss: 0.9820, Train: 0.7375, Val: 0.7153, Test: 0.5759\n",
            "Run: 08, Epoch: 048, Loss: 0.9779, Train: 0.7396, Val: 0.7188, Test: 0.5773\n",
            "Run: 08, Epoch: 049, Loss: 0.9761, Train: 0.7467, Val: 0.7263, Test: 0.5829\n",
            "Run: 08, Epoch: 050, Loss: 0.9712, Train: 0.7488, Val: 0.7271, Test: 0.5851\n",
            "Run: 08, Epoch: 051, Loss: 0.9689, Train: 0.7493, Val: 0.7276, Test: 0.5862\n",
            "Run: 08, Epoch: 052, Loss: 0.9668, Train: 0.7488, Val: 0.7281, Test: 0.5861\n",
            "Run: 08, Epoch: 053, Loss: 0.9630, Train: 0.7491, Val: 0.7266, Test: 0.5859\n",
            "Run: 08, Epoch: 054, Loss: 0.9581, Train: 0.7464, Val: 0.7232, Test: 0.5835\n",
            "Run: 08, Epoch: 055, Loss: 0.9600, Train: 0.7483, Val: 0.7241, Test: 0.5840\n",
            "Run: 08, Epoch: 056, Loss: 0.9537, Train: 0.7547, Val: 0.7316, Test: 0.5880\n",
            "Run: 08, Epoch: 057, Loss: 0.9531, Train: 0.7556, Val: 0.7321, Test: 0.5887\n",
            "Run: 08, Epoch: 058, Loss: 0.9500, Train: 0.7560, Val: 0.7332, Test: 0.5896\n",
            "Run: 08, Epoch: 059, Loss: 0.9483, Train: 0.7560, Val: 0.7336, Test: 0.5901\n",
            "Run: 08, Epoch: 060, Loss: 0.9449, Train: 0.7570, Val: 0.7341, Test: 0.5909\n",
            "Run: 08, Epoch: 061, Loss: 0.9435, Train: 0.7572, Val: 0.7337, Test: 0.5907\n",
            "Run: 08, Epoch: 062, Loss: 0.9414, Train: 0.7586, Val: 0.7344, Test: 0.5915\n",
            "Run: 08, Epoch: 063, Loss: 0.9386, Train: 0.7609, Val: 0.7363, Test: 0.5933\n",
            "Run: 08, Epoch: 064, Loss: 0.9359, Train: 0.7619, Val: 0.7382, Test: 0.5942\n",
            "Run: 08, Epoch: 065, Loss: 0.9345, Train: 0.7620, Val: 0.7381, Test: 0.5940\n",
            "Run: 08, Epoch: 066, Loss: 0.9334, Train: 0.7621, Val: 0.7370, Test: 0.5938\n",
            "Run: 08, Epoch: 067, Loss: 0.9305, Train: 0.7627, Val: 0.7369, Test: 0.5942\n",
            "Run: 08, Epoch: 068, Loss: 0.9301, Train: 0.7645, Val: 0.7388, Test: 0.5957\n",
            "Run: 08, Epoch: 069, Loss: 0.9272, Train: 0.7654, Val: 0.7397, Test: 0.5960\n",
            "Run: 08, Epoch: 070, Loss: 0.9237, Train: 0.7655, Val: 0.7394, Test: 0.5952\n",
            "Run: 08, Epoch: 071, Loss: 0.9218, Train: 0.7653, Val: 0.7402, Test: 0.5947\n",
            "Run: 08, Epoch: 072, Loss: 0.9201, Train: 0.7654, Val: 0.7395, Test: 0.5946\n",
            "Run: 08, Epoch: 073, Loss: 0.9200, Train: 0.7665, Val: 0.7402, Test: 0.5956\n",
            "Run: 08, Epoch: 074, Loss: 0.9145, Train: 0.7681, Val: 0.7412, Test: 0.5971\n",
            "Run: 08, Epoch: 075, Loss: 0.9145, Train: 0.7691, Val: 0.7419, Test: 0.5979\n",
            "Run: 08, Epoch: 076, Loss: 0.9133, Train: 0.7693, Val: 0.7423, Test: 0.5982\n",
            "Run: 08, Epoch: 077, Loss: 0.9121, Train: 0.7698, Val: 0.7428, Test: 0.5980\n",
            "Run: 08, Epoch: 078, Loss: 0.9113, Train: 0.7703, Val: 0.7422, Test: 0.5976\n",
            "Run: 08, Epoch: 079, Loss: 0.9083, Train: 0.7714, Val: 0.7423, Test: 0.5978\n",
            "Run: 08, Epoch: 080, Loss: 0.9070, Train: 0.7726, Val: 0.7434, Test: 0.5985\n",
            "Run: 08, Epoch: 081, Loss: 0.9036, Train: 0.7731, Val: 0.7441, Test: 0.5992\n",
            "Run: 08, Epoch: 082, Loss: 0.9033, Train: 0.7729, Val: 0.7444, Test: 0.5991\n",
            "Run: 08, Epoch: 083, Loss: 0.9024, Train: 0.7736, Val: 0.7443, Test: 0.5993\n",
            "Run: 08, Epoch: 084, Loss: 0.9018, Train: 0.7742, Val: 0.7450, Test: 0.5994\n",
            "Run: 08, Epoch: 085, Loss: 0.8976, Train: 0.7750, Val: 0.7450, Test: 0.5999\n",
            "Run: 08, Epoch: 086, Loss: 0.8955, Train: 0.7754, Val: 0.7451, Test: 0.6004\n",
            "Run: 08, Epoch: 087, Loss: 0.8956, Train: 0.7754, Val: 0.7451, Test: 0.6006\n",
            "Run: 08, Epoch: 088, Loss: 0.8949, Train: 0.7769, Val: 0.7467, Test: 0.6016\n",
            "Run: 08, Epoch: 089, Loss: 0.8930, Train: 0.7778, Val: 0.7476, Test: 0.6022\n",
            "Run: 08, Epoch: 090, Loss: 0.8911, Train: 0.7778, Val: 0.7470, Test: 0.6019\n",
            "Run: 08, Epoch: 091, Loss: 0.8909, Train: 0.7775, Val: 0.7460, Test: 0.6010\n",
            "Run: 08, Epoch: 092, Loss: 0.8875, Train: 0.7777, Val: 0.7459, Test: 0.6010\n",
            "Run: 08, Epoch: 093, Loss: 0.8880, Train: 0.7788, Val: 0.7471, Test: 0.6020\n",
            "Run: 08, Epoch: 094, Loss: 0.8861, Train: 0.7800, Val: 0.7483, Test: 0.6027\n",
            "Run: 08, Epoch: 095, Loss: 0.8843, Train: 0.7805, Val: 0.7490, Test: 0.6028\n",
            "Run: 08, Epoch: 096, Loss: 0.8831, Train: 0.7798, Val: 0.7482, Test: 0.6020\n",
            "Run: 08, Epoch: 097, Loss: 0.8837, Train: 0.7787, Val: 0.7472, Test: 0.6016\n",
            "Run: 08, Epoch: 098, Loss: 0.8805, Train: 0.7808, Val: 0.7495, Test: 0.6036\n",
            "Run: 08, Epoch: 099, Loss: 0.8835, Train: 0.7817, Val: 0.7508, Test: 0.6040\n",
            "Run: 08, Epoch: 100, Loss: 0.8777, Train: 0.7821, Val: 0.7497, Test: 0.6031\n",
            "Run: 08, Epoch: 101, Loss: 0.8759, Train: 0.7825, Val: 0.7492, Test: 0.6026\n",
            "Run: 08, Epoch: 102, Loss: 0.8759, Train: 0.7835, Val: 0.7509, Test: 0.6037\n",
            "Run: 08, Epoch: 103, Loss: 0.8741, Train: 0.7841, Val: 0.7516, Test: 0.6048\n",
            "Run: 08, Epoch: 104, Loss: 0.8713, Train: 0.7842, Val: 0.7511, Test: 0.6048\n",
            "Run: 08, Epoch: 105, Loss: 0.8731, Train: 0.7838, Val: 0.7502, Test: 0.6042\n",
            "Run: 08, Epoch: 106, Loss: 0.8691, Train: 0.7847, Val: 0.7506, Test: 0.6048\n",
            "Run: 08, Epoch: 107, Loss: 0.8720, Train: 0.7859, Val: 0.7519, Test: 0.6059\n",
            "Run: 08, Epoch: 108, Loss: 0.8705, Train: 0.7865, Val: 0.7524, Test: 0.6061\n",
            "Run: 08, Epoch: 109, Loss: 0.8665, Train: 0.7865, Val: 0.7525, Test: 0.6056\n",
            "Run: 08, Epoch: 110, Loss: 0.8648, Train: 0.7862, Val: 0.7512, Test: 0.6049\n",
            "Run: 08, Epoch: 111, Loss: 0.8647, Train: 0.7874, Val: 0.7518, Test: 0.6056\n",
            "Run: 08, Epoch: 112, Loss: 0.8652, Train: 0.7883, Val: 0.7531, Test: 0.6061\n",
            "Run: 08, Epoch: 113, Loss: 0.8624, Train: 0.7884, Val: 0.7532, Test: 0.6064\n",
            "Run: 08, Epoch: 114, Loss: 0.8620, Train: 0.7885, Val: 0.7532, Test: 0.6063\n",
            "Run: 08, Epoch: 115, Loss: 0.8610, Train: 0.7889, Val: 0.7532, Test: 0.6064\n",
            "Run: 08, Epoch: 116, Loss: 0.8578, Train: 0.7894, Val: 0.7531, Test: 0.6068\n",
            "Run: 08, Epoch: 117, Loss: 0.8600, Train: 0.7897, Val: 0.7535, Test: 0.6070\n",
            "Run: 08, Epoch: 118, Loss: 0.8577, Train: 0.7899, Val: 0.7539, Test: 0.6067\n",
            "Run: 08, Epoch: 119, Loss: 0.8566, Train: 0.7902, Val: 0.7540, Test: 0.6068\n",
            "Run: 08, Epoch: 120, Loss: 0.8560, Train: 0.7910, Val: 0.7548, Test: 0.6078\n",
            "Run: 08, Epoch: 121, Loss: 0.8544, Train: 0.7914, Val: 0.7552, Test: 0.6087\n",
            "Run: 08, Epoch: 122, Loss: 0.8529, Train: 0.7914, Val: 0.7549, Test: 0.6081\n",
            "Run: 08, Epoch: 123, Loss: 0.8531, Train: 0.7914, Val: 0.7543, Test: 0.6072\n",
            "Run: 08, Epoch: 124, Loss: 0.8498, Train: 0.7920, Val: 0.7558, Test: 0.6074\n",
            "Run: 08, Epoch: 125, Loss: 0.8490, Train: 0.7928, Val: 0.7561, Test: 0.6079\n",
            "Run: 08, Epoch: 126, Loss: 0.8483, Train: 0.7928, Val: 0.7553, Test: 0.6076\n",
            "Run: 08, Epoch: 127, Loss: 0.8496, Train: 0.7922, Val: 0.7552, Test: 0.6078\n",
            "Run: 08, Epoch: 128, Loss: 0.8491, Train: 0.7936, Val: 0.7563, Test: 0.6095\n",
            "Run: 08, Epoch: 129, Loss: 0.8480, Train: 0.7943, Val: 0.7571, Test: 0.6092\n",
            "Run: 08, Epoch: 130, Loss: 0.8451, Train: 0.7934, Val: 0.7559, Test: 0.6075\n",
            "Run: 08, Epoch: 131, Loss: 0.8448, Train: 0.7941, Val: 0.7559, Test: 0.6078\n",
            "Run: 08, Epoch: 132, Loss: 0.8459, Train: 0.7956, Val: 0.7568, Test: 0.6092\n",
            "Run: 08, Epoch: 133, Loss: 0.8458, Train: 0.7959, Val: 0.7572, Test: 0.6096\n",
            "Run: 08, Epoch: 134, Loss: 0.8427, Train: 0.7957, Val: 0.7564, Test: 0.6089\n",
            "Run: 08, Epoch: 135, Loss: 0.8412, Train: 0.7957, Val: 0.7558, Test: 0.6087\n",
            "Run: 08, Epoch: 136, Loss: 0.8415, Train: 0.7964, Val: 0.7564, Test: 0.6091\n",
            "Run: 08, Epoch: 137, Loss: 0.8406, Train: 0.7965, Val: 0.7562, Test: 0.6090\n",
            "Run: 08, Epoch: 138, Loss: 0.8383, Train: 0.7964, Val: 0.7566, Test: 0.6093\n",
            "Run: 08, Epoch: 139, Loss: 0.8379, Train: 0.7973, Val: 0.7575, Test: 0.6099\n",
            "Run: 08, Epoch: 140, Loss: 0.8356, Train: 0.7981, Val: 0.7582, Test: 0.6109\n",
            "Run: 08, Epoch: 141, Loss: 0.8374, Train: 0.7980, Val: 0.7576, Test: 0.6103\n",
            "Run: 08, Epoch: 142, Loss: 0.8358, Train: 0.7980, Val: 0.7574, Test: 0.6098\n",
            "Run: 08, Epoch: 143, Loss: 0.8348, Train: 0.7989, Val: 0.7584, Test: 0.6103\n",
            "Run: 08, Epoch: 144, Loss: 0.8339, Train: 0.7994, Val: 0.7586, Test: 0.6109\n",
            "Run: 08, Epoch: 145, Loss: 0.8332, Train: 0.7995, Val: 0.7583, Test: 0.6108\n",
            "Run: 08, Epoch: 146, Loss: 0.8310, Train: 0.7995, Val: 0.7588, Test: 0.6110\n",
            "Run: 08, Epoch: 147, Loss: 0.8323, Train: 0.8005, Val: 0.7588, Test: 0.6116\n",
            "Run: 08, Epoch: 148, Loss: 0.8319, Train: 0.8004, Val: 0.7587, Test: 0.6111\n",
            "Run: 08, Epoch: 149, Loss: 0.8328, Train: 0.7998, Val: 0.7589, Test: 0.6101\n",
            "Run: 08, Epoch: 150, Loss: 0.8279, Train: 0.8004, Val: 0.7593, Test: 0.6104\n",
            "Run: 08, Epoch: 151, Loss: 0.8271, Train: 0.8010, Val: 0.7597, Test: 0.6107\n",
            "Run: 08, Epoch: 152, Loss: 0.8275, Train: 0.8016, Val: 0.7597, Test: 0.6104\n",
            "Run: 08, Epoch: 153, Loss: 0.8276, Train: 0.8020, Val: 0.7593, Test: 0.6108\n",
            "Run: 08, Epoch: 154, Loss: 0.8238, Train: 0.8023, Val: 0.7596, Test: 0.6118\n",
            "Run: 08, Epoch: 155, Loss: 0.8267, Train: 0.8022, Val: 0.7599, Test: 0.6121\n",
            "Run: 08, Epoch: 156, Loss: 0.8224, Train: 0.8015, Val: 0.7592, Test: 0.6111\n",
            "Run: 08, Epoch: 157, Loss: 0.8215, Train: 0.8023, Val: 0.7592, Test: 0.6113\n",
            "Run: 08, Epoch: 158, Loss: 0.8229, Train: 0.8028, Val: 0.7606, Test: 0.6117\n",
            "Run: 08, Epoch: 159, Loss: 0.8239, Train: 0.8028, Val: 0.7604, Test: 0.6110\n",
            "Run: 08, Epoch: 160, Loss: 0.8216, Train: 0.8033, Val: 0.7601, Test: 0.6110\n",
            "Run: 08, Epoch: 161, Loss: 0.8224, Train: 0.8039, Val: 0.7605, Test: 0.6119\n",
            "Run: 08, Epoch: 162, Loss: 0.8208, Train: 0.8039, Val: 0.7608, Test: 0.6119\n",
            "Run: 08, Epoch: 163, Loss: 0.8187, Train: 0.8036, Val: 0.7608, Test: 0.6117\n",
            "Run: 08, Epoch: 164, Loss: 0.8184, Train: 0.8041, Val: 0.7606, Test: 0.6114\n",
            "Run: 08, Epoch: 165, Loss: 0.8193, Train: 0.8042, Val: 0.7608, Test: 0.6111\n",
            "Run: 08, Epoch: 166, Loss: 0.8160, Train: 0.8042, Val: 0.7611, Test: 0.6108\n",
            "Run: 08, Epoch: 167, Loss: 0.8153, Train: 0.8039, Val: 0.7609, Test: 0.6105\n",
            "Run: 08, Epoch: 168, Loss: 0.8169, Train: 0.8050, Val: 0.7613, Test: 0.6118\n",
            "Run: 08, Epoch: 169, Loss: 0.8169, Train: 0.8060, Val: 0.7623, Test: 0.6130\n",
            "Run: 08, Epoch: 170, Loss: 0.8164, Train: 0.8057, Val: 0.7612, Test: 0.6128\n",
            "Run: 08, Epoch: 171, Loss: 0.8136, Train: 0.8059, Val: 0.7616, Test: 0.6132\n",
            "Run: 08, Epoch: 172, Loss: 0.8120, Train: 0.8063, Val: 0.7617, Test: 0.6134\n",
            "Run: 08, Epoch: 173, Loss: 0.8141, Train: 0.8063, Val: 0.7609, Test: 0.6125\n",
            "Run: 08, Epoch: 174, Loss: 0.8107, Train: 0.8061, Val: 0.7605, Test: 0.6123\n",
            "Run: 08, Epoch: 175, Loss: 0.8114, Train: 0.8074, Val: 0.7620, Test: 0.6141\n",
            "Run: 08, Epoch: 176, Loss: 0.8110, Train: 0.8077, Val: 0.7618, Test: 0.6148\n",
            "Run: 08, Epoch: 177, Loss: 0.8101, Train: 0.8071, Val: 0.7615, Test: 0.6140\n",
            "Run: 08, Epoch: 178, Loss: 0.8112, Train: 0.8076, Val: 0.7615, Test: 0.6137\n",
            "Run: 08, Epoch: 179, Loss: 0.8063, Train: 0.8078, Val: 0.7614, Test: 0.6136\n",
            "Run: 08, Epoch: 180, Loss: 0.8092, Train: 0.8076, Val: 0.7613, Test: 0.6131\n",
            "Run: 08, Epoch: 181, Loss: 0.8065, Train: 0.8076, Val: 0.7615, Test: 0.6130\n",
            "Run: 08, Epoch: 182, Loss: 0.8088, Train: 0.8081, Val: 0.7627, Test: 0.6131\n",
            "Run: 08, Epoch: 183, Loss: 0.8053, Train: 0.8083, Val: 0.7630, Test: 0.6124\n",
            "Run: 08, Epoch: 184, Loss: 0.8062, Train: 0.8088, Val: 0.7629, Test: 0.6119\n",
            "Run: 08, Epoch: 185, Loss: 0.8045, Train: 0.8091, Val: 0.7635, Test: 0.6127\n",
            "Run: 08, Epoch: 186, Loss: 0.8031, Train: 0.8097, Val: 0.7633, Test: 0.6136\n",
            "Run: 08, Epoch: 187, Loss: 0.8034, Train: 0.8096, Val: 0.7621, Test: 0.6142\n",
            "Run: 08, Epoch: 188, Loss: 0.8039, Train: 0.8101, Val: 0.7628, Test: 0.6148\n",
            "Run: 08, Epoch: 189, Loss: 0.8027, Train: 0.8093, Val: 0.7629, Test: 0.6140\n",
            "Run: 08, Epoch: 190, Loss: 0.8022, Train: 0.8093, Val: 0.7628, Test: 0.6132\n",
            "Run: 08, Epoch: 191, Loss: 0.8013, Train: 0.8099, Val: 0.7638, Test: 0.6130\n",
            "Run: 08, Epoch: 192, Loss: 0.8027, Train: 0.8099, Val: 0.7628, Test: 0.6123\n",
            "Run: 08, Epoch: 193, Loss: 0.8012, Train: 0.8100, Val: 0.7629, Test: 0.6128\n",
            "Run: 08, Epoch: 194, Loss: 0.8009, Train: 0.8115, Val: 0.7635, Test: 0.6146\n",
            "Run: 08, Epoch: 195, Loss: 0.8016, Train: 0.8114, Val: 0.7632, Test: 0.6147\n",
            "Run: 08, Epoch: 196, Loss: 0.7988, Train: 0.8114, Val: 0.7629, Test: 0.6146\n",
            "Run: 08, Epoch: 197, Loss: 0.7964, Train: 0.8116, Val: 0.7633, Test: 0.6139\n",
            "Run: 08, Epoch: 198, Loss: 0.7939, Train: 0.8115, Val: 0.7634, Test: 0.6133\n",
            "Run: 08, Epoch: 199, Loss: 0.7969, Train: 0.8118, Val: 0.7634, Test: 0.6133\n",
            "Run: 08, Epoch: 200, Loss: 0.7957, Train: 0.8117, Val: 0.7630, Test: 0.6137\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9460, Val: 0.9121, Test: 0.8407\n",
            "340527\n",
            "\n",
            "Run 09:\n",
            "\n",
            "Run: 09, Epoch: 001, Loss: 4.1354, Train: 0.3777, Val: 0.3757, Test: 0.3137\n",
            "Run: 09, Epoch: 002, Loss: 3.0500, Train: 0.5027, Val: 0.4982, Test: 0.4122\n",
            "Run: 09, Epoch: 003, Loss: 2.1733, Train: 0.5098, Val: 0.5061, Test: 0.4255\n",
            "Run: 09, Epoch: 004, Loss: 1.8635, Train: 0.5605, Val: 0.5500, Test: 0.4543\n",
            "Run: 09, Epoch: 005, Loss: 1.7469, Train: 0.5796, Val: 0.5727, Test: 0.4671\n",
            "Run: 09, Epoch: 006, Loss: 1.5878, Train: 0.5954, Val: 0.5872, Test: 0.4764\n",
            "Run: 09, Epoch: 007, Loss: 1.5751, Train: 0.6174, Val: 0.6093, Test: 0.4975\n",
            "Run: 09, Epoch: 008, Loss: 1.4516, Train: 0.6276, Val: 0.6163, Test: 0.5016\n",
            "Run: 09, Epoch: 009, Loss: 1.4205, Train: 0.6366, Val: 0.6266, Test: 0.5069\n",
            "Run: 09, Epoch: 010, Loss: 1.3525, Train: 0.6390, Val: 0.6259, Test: 0.5082\n",
            "Run: 09, Epoch: 011, Loss: 1.3120, Train: 0.6401, Val: 0.6286, Test: 0.5103\n",
            "Run: 09, Epoch: 012, Loss: 1.2931, Train: 0.6457, Val: 0.6340, Test: 0.5147\n",
            "Run: 09, Epoch: 013, Loss: 1.2702, Train: 0.6512, Val: 0.6372, Test: 0.5189\n",
            "Run: 09, Epoch: 014, Loss: 1.2482, Train: 0.6582, Val: 0.6445, Test: 0.5236\n",
            "Run: 09, Epoch: 015, Loss: 1.2324, Train: 0.6644, Val: 0.6507, Test: 0.5284\n",
            "Run: 09, Epoch: 016, Loss: 1.2090, Train: 0.6707, Val: 0.6554, Test: 0.5324\n",
            "Run: 09, Epoch: 017, Loss: 1.1961, Train: 0.6782, Val: 0.6630, Test: 0.5382\n",
            "Run: 09, Epoch: 018, Loss: 1.1811, Train: 0.6872, Val: 0.6718, Test: 0.5444\n",
            "Run: 09, Epoch: 019, Loss: 1.1633, Train: 0.6923, Val: 0.6773, Test: 0.5482\n",
            "Run: 09, Epoch: 020, Loss: 1.1514, Train: 0.6958, Val: 0.6800, Test: 0.5514\n",
            "Run: 09, Epoch: 021, Loss: 1.1366, Train: 0.6984, Val: 0.6843, Test: 0.5549\n",
            "Run: 09, Epoch: 022, Loss: 1.1277, Train: 0.7030, Val: 0.6894, Test: 0.5587\n",
            "Run: 09, Epoch: 023, Loss: 1.1148, Train: 0.7055, Val: 0.6903, Test: 0.5606\n",
            "Run: 09, Epoch: 024, Loss: 1.1056, Train: 0.7077, Val: 0.6912, Test: 0.5620\n",
            "Run: 09, Epoch: 025, Loss: 1.0967, Train: 0.7117, Val: 0.6964, Test: 0.5646\n",
            "Run: 09, Epoch: 026, Loss: 1.0872, Train: 0.7158, Val: 0.7015, Test: 0.5666\n",
            "Run: 09, Epoch: 027, Loss: 1.0761, Train: 0.7183, Val: 0.7039, Test: 0.5675\n",
            "Run: 09, Epoch: 028, Loss: 1.0695, Train: 0.7199, Val: 0.7038, Test: 0.5678\n",
            "Run: 09, Epoch: 029, Loss: 1.0636, Train: 0.7210, Val: 0.7045, Test: 0.5679\n",
            "Run: 09, Epoch: 030, Loss: 1.0593, Train: 0.7214, Val: 0.7051, Test: 0.5676\n",
            "Run: 09, Epoch: 031, Loss: 1.0483, Train: 0.7228, Val: 0.7060, Test: 0.5681\n",
            "Run: 09, Epoch: 032, Loss: 1.0461, Train: 0.7248, Val: 0.7087, Test: 0.5697\n",
            "Run: 09, Epoch: 033, Loss: 1.0389, Train: 0.7266, Val: 0.7105, Test: 0.5714\n",
            "Run: 09, Epoch: 034, Loss: 1.0364, Train: 0.7284, Val: 0.7117, Test: 0.5725\n",
            "Run: 09, Epoch: 035, Loss: 1.0285, Train: 0.7290, Val: 0.7113, Test: 0.5728\n",
            "Run: 09, Epoch: 036, Loss: 1.0221, Train: 0.7289, Val: 0.7115, Test: 0.5729\n",
            "Run: 09, Epoch: 037, Loss: 1.0187, Train: 0.7286, Val: 0.7120, Test: 0.5729\n",
            "Run: 09, Epoch: 038, Loss: 1.0157, Train: 0.7300, Val: 0.7128, Test: 0.5734\n",
            "Run: 09, Epoch: 039, Loss: 1.0096, Train: 0.7308, Val: 0.7138, Test: 0.5735\n",
            "Run: 09, Epoch: 040, Loss: 1.0074, Train: 0.7318, Val: 0.7147, Test: 0.5735\n",
            "Run: 09, Epoch: 041, Loss: 1.0021, Train: 0.7326, Val: 0.7144, Test: 0.5740\n",
            "Run: 09, Epoch: 042, Loss: 0.9965, Train: 0.7334, Val: 0.7150, Test: 0.5742\n",
            "Run: 09, Epoch: 043, Loss: 0.9952, Train: 0.7341, Val: 0.7151, Test: 0.5746\n",
            "Run: 09, Epoch: 044, Loss: 0.9903, Train: 0.7353, Val: 0.7166, Test: 0.5753\n",
            "Run: 09, Epoch: 045, Loss: 0.9870, Train: 0.7364, Val: 0.7179, Test: 0.5760\n",
            "Run: 09, Epoch: 046, Loss: 0.9842, Train: 0.7371, Val: 0.7185, Test: 0.5765\n",
            "Run: 09, Epoch: 047, Loss: 0.9813, Train: 0.7380, Val: 0.7190, Test: 0.5774\n",
            "Run: 09, Epoch: 048, Loss: 0.9779, Train: 0.7389, Val: 0.7195, Test: 0.5780\n",
            "Run: 09, Epoch: 049, Loss: 0.9745, Train: 0.7402, Val: 0.7204, Test: 0.5788\n",
            "Run: 09, Epoch: 050, Loss: 0.9713, Train: 0.7417, Val: 0.7213, Test: 0.5799\n",
            "Run: 09, Epoch: 051, Loss: 0.9671, Train: 0.7426, Val: 0.7216, Test: 0.5807\n",
            "Run: 09, Epoch: 052, Loss: 0.9658, Train: 0.7432, Val: 0.7219, Test: 0.5813\n",
            "Run: 09, Epoch: 053, Loss: 0.9616, Train: 0.7443, Val: 0.7233, Test: 0.5825\n",
            "Run: 09, Epoch: 054, Loss: 0.9615, Train: 0.7511, Val: 0.7308, Test: 0.5876\n",
            "Run: 09, Epoch: 055, Loss: 0.9564, Train: 0.7525, Val: 0.7312, Test: 0.5883\n",
            "Run: 09, Epoch: 056, Loss: 0.9554, Train: 0.7528, Val: 0.7310, Test: 0.5878\n",
            "Run: 09, Epoch: 057, Loss: 0.9550, Train: 0.7532, Val: 0.7313, Test: 0.5878\n",
            "Run: 09, Epoch: 058, Loss: 0.9514, Train: 0.7546, Val: 0.7320, Test: 0.5891\n",
            "Run: 09, Epoch: 059, Loss: 0.9490, Train: 0.7561, Val: 0.7342, Test: 0.5910\n",
            "Run: 09, Epoch: 060, Loss: 0.9449, Train: 0.7575, Val: 0.7346, Test: 0.5917\n",
            "Run: 09, Epoch: 061, Loss: 0.9435, Train: 0.7578, Val: 0.7341, Test: 0.5909\n",
            "Run: 09, Epoch: 062, Loss: 0.9418, Train: 0.7573, Val: 0.7337, Test: 0.5896\n",
            "Run: 09, Epoch: 063, Loss: 0.9400, Train: 0.7573, Val: 0.7332, Test: 0.5894\n",
            "Run: 09, Epoch: 064, Loss: 0.9349, Train: 0.7590, Val: 0.7350, Test: 0.5912\n",
            "Run: 09, Epoch: 065, Loss: 0.9340, Train: 0.7608, Val: 0.7364, Test: 0.5934\n",
            "Run: 09, Epoch: 066, Loss: 0.9326, Train: 0.7615, Val: 0.7370, Test: 0.5942\n",
            "Run: 09, Epoch: 067, Loss: 0.9302, Train: 0.7624, Val: 0.7377, Test: 0.5944\n",
            "Run: 09, Epoch: 068, Loss: 0.9291, Train: 0.7629, Val: 0.7376, Test: 0.5936\n",
            "Run: 09, Epoch: 069, Loss: 0.9268, Train: 0.7627, Val: 0.7374, Test: 0.5928\n",
            "Run: 09, Epoch: 070, Loss: 0.9263, Train: 0.7635, Val: 0.7384, Test: 0.5934\n",
            "Run: 09, Epoch: 071, Loss: 0.9246, Train: 0.7646, Val: 0.7394, Test: 0.5948\n",
            "Run: 09, Epoch: 072, Loss: 0.9205, Train: 0.7660, Val: 0.7400, Test: 0.5958\n",
            "Run: 09, Epoch: 073, Loss: 0.9169, Train: 0.7668, Val: 0.7403, Test: 0.5960\n",
            "Run: 09, Epoch: 074, Loss: 0.9176, Train: 0.7666, Val: 0.7394, Test: 0.5954\n",
            "Run: 09, Epoch: 075, Loss: 0.9171, Train: 0.7678, Val: 0.7400, Test: 0.5959\n",
            "Run: 09, Epoch: 076, Loss: 0.9136, Train: 0.7686, Val: 0.7419, Test: 0.5965\n",
            "Run: 09, Epoch: 077, Loss: 0.9129, Train: 0.7694, Val: 0.7424, Test: 0.5968\n",
            "Run: 09, Epoch: 078, Loss: 0.9122, Train: 0.7697, Val: 0.7426, Test: 0.5966\n",
            "Run: 09, Epoch: 079, Loss: 0.9092, Train: 0.7695, Val: 0.7424, Test: 0.5956\n",
            "Run: 09, Epoch: 080, Loss: 0.9084, Train: 0.7699, Val: 0.7415, Test: 0.5955\n",
            "Run: 09, Epoch: 081, Loss: 0.9075, Train: 0.7708, Val: 0.7413, Test: 0.5970\n",
            "Run: 09, Epoch: 082, Loss: 0.9050, Train: 0.7720, Val: 0.7423, Test: 0.5991\n",
            "Run: 09, Epoch: 083, Loss: 0.9042, Train: 0.7723, Val: 0.7441, Test: 0.5995\n",
            "Run: 09, Epoch: 084, Loss: 0.9010, Train: 0.7724, Val: 0.7452, Test: 0.5990\n",
            "Run: 09, Epoch: 085, Loss: 0.9018, Train: 0.7732, Val: 0.7447, Test: 0.5986\n",
            "Run: 09, Epoch: 086, Loss: 0.8992, Train: 0.7740, Val: 0.7447, Test: 0.5991\n",
            "Run: 09, Epoch: 087, Loss: 0.8987, Train: 0.7746, Val: 0.7449, Test: 0.5998\n",
            "Run: 09, Epoch: 088, Loss: 0.8960, Train: 0.7747, Val: 0.7446, Test: 0.6004\n",
            "Run: 09, Epoch: 089, Loss: 0.8923, Train: 0.7752, Val: 0.7449, Test: 0.6009\n",
            "Run: 09, Epoch: 090, Loss: 0.8909, Train: 0.7761, Val: 0.7450, Test: 0.6012\n",
            "Run: 09, Epoch: 091, Loss: 0.8925, Train: 0.7772, Val: 0.7463, Test: 0.6017\n",
            "Run: 09, Epoch: 092, Loss: 0.8908, Train: 0.7773, Val: 0.7471, Test: 0.6016\n",
            "Run: 09, Epoch: 093, Loss: 0.8911, Train: 0.7777, Val: 0.7467, Test: 0.6012\n",
            "Run: 09, Epoch: 094, Loss: 0.8874, Train: 0.7779, Val: 0.7461, Test: 0.6009\n",
            "Run: 09, Epoch: 095, Loss: 0.8874, Train: 0.7786, Val: 0.7463, Test: 0.6015\n",
            "Run: 09, Epoch: 096, Loss: 0.8850, Train: 0.7794, Val: 0.7475, Test: 0.6026\n",
            "Run: 09, Epoch: 097, Loss: 0.8840, Train: 0.7802, Val: 0.7488, Test: 0.6034\n",
            "Run: 09, Epoch: 098, Loss: 0.8806, Train: 0.7807, Val: 0.7496, Test: 0.6036\n",
            "Run: 09, Epoch: 099, Loss: 0.8796, Train: 0.7806, Val: 0.7491, Test: 0.6027\n",
            "Run: 09, Epoch: 100, Loss: 0.8806, Train: 0.7807, Val: 0.7483, Test: 0.6017\n",
            "Run: 09, Epoch: 101, Loss: 0.8792, Train: 0.7815, Val: 0.7493, Test: 0.6028\n",
            "Run: 09, Epoch: 102, Loss: 0.8780, Train: 0.7827, Val: 0.7504, Test: 0.6040\n",
            "Run: 09, Epoch: 103, Loss: 0.8765, Train: 0.7823, Val: 0.7507, Test: 0.6042\n",
            "Run: 09, Epoch: 104, Loss: 0.8754, Train: 0.7821, Val: 0.7498, Test: 0.6040\n",
            "Run: 09, Epoch: 105, Loss: 0.8758, Train: 0.7830, Val: 0.7498, Test: 0.6045\n",
            "Run: 09, Epoch: 106, Loss: 0.8726, Train: 0.7843, Val: 0.7508, Test: 0.6050\n",
            "Run: 09, Epoch: 107, Loss: 0.8717, Train: 0.7848, Val: 0.7511, Test: 0.6049\n",
            "Run: 09, Epoch: 108, Loss: 0.8723, Train: 0.7848, Val: 0.7511, Test: 0.6045\n",
            "Run: 09, Epoch: 109, Loss: 0.8689, Train: 0.7850, Val: 0.7523, Test: 0.6046\n",
            "Run: 09, Epoch: 110, Loss: 0.8688, Train: 0.7851, Val: 0.7523, Test: 0.6048\n",
            "Run: 09, Epoch: 111, Loss: 0.8672, Train: 0.7856, Val: 0.7520, Test: 0.6053\n",
            "Run: 09, Epoch: 112, Loss: 0.8662, Train: 0.7862, Val: 0.7516, Test: 0.6053\n",
            "Run: 09, Epoch: 113, Loss: 0.8659, Train: 0.7864, Val: 0.7515, Test: 0.6052\n",
            "Run: 09, Epoch: 114, Loss: 0.8632, Train: 0.7866, Val: 0.7522, Test: 0.6045\n",
            "Run: 09, Epoch: 115, Loss: 0.8616, Train: 0.7873, Val: 0.7531, Test: 0.6048\n",
            "Run: 09, Epoch: 116, Loss: 0.8629, Train: 0.7879, Val: 0.7531, Test: 0.6057\n",
            "Run: 09, Epoch: 117, Loss: 0.8601, Train: 0.7882, Val: 0.7538, Test: 0.6059\n",
            "Run: 09, Epoch: 118, Loss: 0.8596, Train: 0.7879, Val: 0.7525, Test: 0.6057\n",
            "Run: 09, Epoch: 119, Loss: 0.8586, Train: 0.7878, Val: 0.7528, Test: 0.6065\n",
            "Run: 09, Epoch: 120, Loss: 0.8579, Train: 0.7894, Val: 0.7536, Test: 0.6078\n",
            "Run: 09, Epoch: 121, Loss: 0.8553, Train: 0.7904, Val: 0.7546, Test: 0.6080\n",
            "Run: 09, Epoch: 122, Loss: 0.8555, Train: 0.7905, Val: 0.7550, Test: 0.6074\n",
            "Run: 09, Epoch: 123, Loss: 0.8510, Train: 0.7900, Val: 0.7542, Test: 0.6064\n",
            "Run: 09, Epoch: 124, Loss: 0.8538, Train: 0.7901, Val: 0.7546, Test: 0.6065\n",
            "Run: 09, Epoch: 125, Loss: 0.8516, Train: 0.7909, Val: 0.7554, Test: 0.6077\n",
            "Run: 09, Epoch: 126, Loss: 0.8543, Train: 0.7915, Val: 0.7559, Test: 0.6082\n",
            "Run: 09, Epoch: 127, Loss: 0.8534, Train: 0.7920, Val: 0.7557, Test: 0.6080\n",
            "Run: 09, Epoch: 128, Loss: 0.8522, Train: 0.7921, Val: 0.7562, Test: 0.6083\n",
            "Run: 09, Epoch: 129, Loss: 0.8489, Train: 0.7925, Val: 0.7568, Test: 0.6084\n",
            "Run: 09, Epoch: 130, Loss: 0.8474, Train: 0.7928, Val: 0.7571, Test: 0.6081\n",
            "Run: 09, Epoch: 131, Loss: 0.8455, Train: 0.7930, Val: 0.7566, Test: 0.6073\n",
            "Run: 09, Epoch: 132, Loss: 0.8453, Train: 0.7936, Val: 0.7569, Test: 0.6075\n",
            "Run: 09, Epoch: 133, Loss: 0.8455, Train: 0.7939, Val: 0.7569, Test: 0.6079\n",
            "Run: 09, Epoch: 134, Loss: 0.8433, Train: 0.7937, Val: 0.7572, Test: 0.6083\n",
            "Run: 09, Epoch: 135, Loss: 0.8439, Train: 0.7941, Val: 0.7578, Test: 0.6086\n",
            "Run: 09, Epoch: 136, Loss: 0.8415, Train: 0.7945, Val: 0.7582, Test: 0.6088\n",
            "Run: 09, Epoch: 137, Loss: 0.8425, Train: 0.7953, Val: 0.7574, Test: 0.6087\n",
            "Run: 09, Epoch: 138, Loss: 0.8409, Train: 0.7955, Val: 0.7574, Test: 0.6086\n",
            "Run: 09, Epoch: 139, Loss: 0.8378, Train: 0.7956, Val: 0.7576, Test: 0.6090\n",
            "Run: 09, Epoch: 140, Loss: 0.8391, Train: 0.7954, Val: 0.7573, Test: 0.6088\n",
            "Run: 09, Epoch: 141, Loss: 0.8359, Train: 0.7960, Val: 0.7577, Test: 0.6089\n",
            "Run: 09, Epoch: 142, Loss: 0.8367, Train: 0.7966, Val: 0.7579, Test: 0.6094\n",
            "Run: 09, Epoch: 143, Loss: 0.8359, Train: 0.7975, Val: 0.7584, Test: 0.6102\n",
            "Run: 09, Epoch: 144, Loss: 0.8370, Train: 0.7978, Val: 0.7587, Test: 0.6106\n",
            "Run: 09, Epoch: 145, Loss: 0.8325, Train: 0.7976, Val: 0.7585, Test: 0.6109\n",
            "Run: 09, Epoch: 146, Loss: 0.8341, Train: 0.7979, Val: 0.7591, Test: 0.6114\n",
            "Run: 09, Epoch: 147, Loss: 0.8350, Train: 0.7983, Val: 0.7588, Test: 0.6113\n",
            "Run: 09, Epoch: 148, Loss: 0.8335, Train: 0.7979, Val: 0.7586, Test: 0.6106\n",
            "Run: 09, Epoch: 149, Loss: 0.8303, Train: 0.7981, Val: 0.7593, Test: 0.6104\n",
            "Run: 09, Epoch: 150, Loss: 0.8303, Train: 0.7991, Val: 0.7597, Test: 0.6104\n",
            "Run: 09, Epoch: 151, Loss: 0.8318, Train: 0.7995, Val: 0.7598, Test: 0.6101\n",
            "Run: 09, Epoch: 152, Loss: 0.8260, Train: 0.7992, Val: 0.7594, Test: 0.6096\n",
            "Run: 09, Epoch: 153, Loss: 0.8327, Train: 0.7993, Val: 0.7596, Test: 0.6100\n",
            "Run: 09, Epoch: 154, Loss: 0.8284, Train: 0.7995, Val: 0.7602, Test: 0.6104\n",
            "Run: 09, Epoch: 155, Loss: 0.8277, Train: 0.8002, Val: 0.7607, Test: 0.6107\n",
            "Run: 09, Epoch: 156, Loss: 0.8271, Train: 0.8002, Val: 0.7600, Test: 0.6098\n",
            "Run: 09, Epoch: 157, Loss: 0.8275, Train: 0.8001, Val: 0.7595, Test: 0.6097\n",
            "Run: 09, Epoch: 158, Loss: 0.8224, Train: 0.8008, Val: 0.7608, Test: 0.6108\n",
            "Run: 09, Epoch: 159, Loss: 0.8262, Train: 0.8012, Val: 0.7607, Test: 0.6110\n",
            "Run: 09, Epoch: 160, Loss: 0.8228, Train: 0.8016, Val: 0.7616, Test: 0.6113\n",
            "Run: 09, Epoch: 161, Loss: 0.8212, Train: 0.8022, Val: 0.7616, Test: 0.6117\n",
            "Run: 09, Epoch: 162, Loss: 0.8216, Train: 0.8027, Val: 0.7625, Test: 0.6116\n",
            "Run: 09, Epoch: 163, Loss: 0.8226, Train: 0.8032, Val: 0.7622, Test: 0.6116\n",
            "Run: 09, Epoch: 164, Loss: 0.8197, Train: 0.8030, Val: 0.7620, Test: 0.6112\n",
            "Run: 09, Epoch: 165, Loss: 0.8197, Train: 0.8031, Val: 0.7617, Test: 0.6109\n",
            "Run: 09, Epoch: 166, Loss: 0.8179, Train: 0.8031, Val: 0.7612, Test: 0.6106\n",
            "Run: 09, Epoch: 167, Loss: 0.8198, Train: 0.8030, Val: 0.7605, Test: 0.6107\n",
            "Run: 09, Epoch: 168, Loss: 0.8187, Train: 0.8033, Val: 0.7612, Test: 0.6117\n",
            "Run: 09, Epoch: 169, Loss: 0.8160, Train: 0.8044, Val: 0.7627, Test: 0.6126\n",
            "Run: 09, Epoch: 170, Loss: 0.8173, Train: 0.8048, Val: 0.7629, Test: 0.6128\n",
            "Run: 09, Epoch: 171, Loss: 0.8152, Train: 0.8052, Val: 0.7623, Test: 0.6123\n",
            "Run: 09, Epoch: 172, Loss: 0.8145, Train: 0.8054, Val: 0.7629, Test: 0.6123\n",
            "Run: 09, Epoch: 173, Loss: 0.8145, Train: 0.8048, Val: 0.7619, Test: 0.6118\n",
            "Run: 09, Epoch: 174, Loss: 0.8138, Train: 0.8040, Val: 0.7609, Test: 0.6114\n",
            "Run: 09, Epoch: 175, Loss: 0.8128, Train: 0.8045, Val: 0.7611, Test: 0.6114\n",
            "Run: 09, Epoch: 176, Loss: 0.8120, Train: 0.8056, Val: 0.7625, Test: 0.6124\n",
            "Run: 09, Epoch: 177, Loss: 0.8109, Train: 0.8062, Val: 0.7635, Test: 0.6128\n",
            "Run: 09, Epoch: 178, Loss: 0.8116, Train: 0.8058, Val: 0.7630, Test: 0.6124\n",
            "Run: 09, Epoch: 179, Loss: 0.8135, Train: 0.8062, Val: 0.7629, Test: 0.6124\n",
            "Run: 09, Epoch: 180, Loss: 0.8105, Train: 0.8069, Val: 0.7639, Test: 0.6135\n",
            "Run: 09, Epoch: 181, Loss: 0.8085, Train: 0.8069, Val: 0.7634, Test: 0.6133\n",
            "Run: 09, Epoch: 182, Loss: 0.8096, Train: 0.8059, Val: 0.7623, Test: 0.6120\n",
            "Run: 09, Epoch: 183, Loss: 0.8071, Train: 0.8068, Val: 0.7628, Test: 0.6124\n",
            "Run: 09, Epoch: 184, Loss: 0.8085, Train: 0.8075, Val: 0.7631, Test: 0.6129\n",
            "Run: 09, Epoch: 185, Loss: 0.8100, Train: 0.8076, Val: 0.7639, Test: 0.6130\n",
            "Run: 09, Epoch: 186, Loss: 0.8068, Train: 0.8077, Val: 0.7633, Test: 0.6133\n",
            "Run: 09, Epoch: 187, Loss: 0.8060, Train: 0.8087, Val: 0.7647, Test: 0.6146\n",
            "Run: 09, Epoch: 188, Loss: 0.8043, Train: 0.8093, Val: 0.7650, Test: 0.6149\n",
            "Run: 09, Epoch: 189, Loss: 0.8028, Train: 0.8091, Val: 0.7645, Test: 0.6140\n",
            "Run: 09, Epoch: 190, Loss: 0.8035, Train: 0.8091, Val: 0.7648, Test: 0.6137\n",
            "Run: 09, Epoch: 191, Loss: 0.8049, Train: 0.8093, Val: 0.7649, Test: 0.6140\n",
            "Run: 09, Epoch: 192, Loss: 0.8023, Train: 0.8090, Val: 0.7641, Test: 0.6133\n",
            "Run: 09, Epoch: 193, Loss: 0.8001, Train: 0.8092, Val: 0.7641, Test: 0.6129\n",
            "Run: 09, Epoch: 194, Loss: 0.8013, Train: 0.8098, Val: 0.7652, Test: 0.6137\n",
            "Run: 09, Epoch: 195, Loss: 0.8005, Train: 0.8100, Val: 0.7655, Test: 0.6137\n",
            "Run: 09, Epoch: 196, Loss: 0.8004, Train: 0.8098, Val: 0.7643, Test: 0.6127\n",
            "Run: 09, Epoch: 197, Loss: 0.8006, Train: 0.8103, Val: 0.7642, Test: 0.6134\n",
            "Run: 09, Epoch: 198, Loss: 0.7990, Train: 0.8110, Val: 0.7653, Test: 0.6141\n",
            "Run: 09, Epoch: 199, Loss: 0.7986, Train: 0.8107, Val: 0.7650, Test: 0.6138\n",
            "Run: 09, Epoch: 200, Loss: 0.7990, Train: 0.8101, Val: 0.7641, Test: 0.6128\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9459, Val: 0.9125, Test: 0.8416\n",
            "340527\n",
            "\n",
            "Run 10:\n",
            "\n",
            "Run: 10, Epoch: 001, Loss: 4.0407, Train: 0.3648, Val: 0.3632, Test: 0.3002\n",
            "Run: 10, Epoch: 002, Loss: 3.0277, Train: 0.4703, Val: 0.4668, Test: 0.3925\n",
            "Run: 10, Epoch: 003, Loss: 2.2058, Train: 0.5017, Val: 0.4976, Test: 0.4204\n",
            "Run: 10, Epoch: 004, Loss: 1.9459, Train: 0.5549, Val: 0.5465, Test: 0.4457\n",
            "Run: 10, Epoch: 005, Loss: 1.8365, Train: 0.5684, Val: 0.5580, Test: 0.4557\n",
            "Run: 10, Epoch: 006, Loss: 1.6261, Train: 0.5925, Val: 0.5816, Test: 0.4742\n",
            "Run: 10, Epoch: 007, Loss: 1.6059, Train: 0.6203, Val: 0.6043, Test: 0.4952\n",
            "Run: 10, Epoch: 008, Loss: 1.4923, Train: 0.6246, Val: 0.6088, Test: 0.4977\n",
            "Run: 10, Epoch: 009, Loss: 1.4454, Train: 0.6341, Val: 0.6229, Test: 0.5089\n",
            "Run: 10, Epoch: 010, Loss: 1.3961, Train: 0.6307, Val: 0.6196, Test: 0.5064\n",
            "Run: 10, Epoch: 011, Loss: 1.3573, Train: 0.6347, Val: 0.6216, Test: 0.5088\n",
            "Run: 10, Epoch: 012, Loss: 1.3299, Train: 0.6500, Val: 0.6355, Test: 0.5198\n",
            "Run: 10, Epoch: 013, Loss: 1.2997, Train: 0.6604, Val: 0.6440, Test: 0.5261\n",
            "Run: 10, Epoch: 014, Loss: 1.2833, Train: 0.6648, Val: 0.6483, Test: 0.5293\n",
            "Run: 10, Epoch: 015, Loss: 1.2581, Train: 0.6650, Val: 0.6503, Test: 0.5296\n",
            "Run: 10, Epoch: 016, Loss: 1.2349, Train: 0.6667, Val: 0.6550, Test: 0.5322\n",
            "Run: 10, Epoch: 017, Loss: 1.2204, Train: 0.6730, Val: 0.6594, Test: 0.5358\n",
            "Run: 10, Epoch: 018, Loss: 1.2062, Train: 0.6803, Val: 0.6659, Test: 0.5409\n",
            "Run: 10, Epoch: 019, Loss: 1.1886, Train: 0.6855, Val: 0.6695, Test: 0.5447\n",
            "Run: 10, Epoch: 020, Loss: 1.1756, Train: 0.6918, Val: 0.6760, Test: 0.5496\n",
            "Run: 10, Epoch: 021, Loss: 1.1597, Train: 0.6958, Val: 0.6788, Test: 0.5531\n",
            "Run: 10, Epoch: 022, Loss: 1.1504, Train: 0.6984, Val: 0.6817, Test: 0.5560\n",
            "Run: 10, Epoch: 023, Loss: 1.1357, Train: 0.7000, Val: 0.6840, Test: 0.5568\n",
            "Run: 10, Epoch: 024, Loss: 1.1245, Train: 0.7015, Val: 0.6852, Test: 0.5572\n",
            "Run: 10, Epoch: 025, Loss: 1.1187, Train: 0.7040, Val: 0.6881, Test: 0.5571\n",
            "Run: 10, Epoch: 026, Loss: 1.1043, Train: 0.7057, Val: 0.6891, Test: 0.5566\n",
            "Run: 10, Epoch: 027, Loss: 1.1001, Train: 0.7090, Val: 0.6921, Test: 0.5574\n",
            "Run: 10, Epoch: 028, Loss: 1.0928, Train: 0.7116, Val: 0.6951, Test: 0.5593\n",
            "Run: 10, Epoch: 029, Loss: 1.0828, Train: 0.7132, Val: 0.6975, Test: 0.5619\n",
            "Run: 10, Epoch: 030, Loss: 1.0765, Train: 0.7146, Val: 0.6994, Test: 0.5640\n",
            "Run: 10, Epoch: 031, Loss: 1.0671, Train: 0.7153, Val: 0.6996, Test: 0.5645\n",
            "Run: 10, Epoch: 032, Loss: 1.0645, Train: 0.7172, Val: 0.6999, Test: 0.5645\n",
            "Run: 10, Epoch: 033, Loss: 1.0582, Train: 0.7187, Val: 0.7008, Test: 0.5645\n",
            "Run: 10, Epoch: 034, Loss: 1.0533, Train: 0.7198, Val: 0.7019, Test: 0.5645\n",
            "Run: 10, Epoch: 035, Loss: 1.0483, Train: 0.7203, Val: 0.7046, Test: 0.5646\n",
            "Run: 10, Epoch: 036, Loss: 1.0418, Train: 0.7214, Val: 0.7048, Test: 0.5648\n",
            "Run: 10, Epoch: 037, Loss: 1.0353, Train: 0.7223, Val: 0.7045, Test: 0.5655\n",
            "Run: 10, Epoch: 038, Loss: 1.0316, Train: 0.7236, Val: 0.7051, Test: 0.5667\n",
            "Run: 10, Epoch: 039, Loss: 1.0279, Train: 0.7243, Val: 0.7048, Test: 0.5679\n",
            "Run: 10, Epoch: 040, Loss: 1.0217, Train: 0.7252, Val: 0.7050, Test: 0.5689\n",
            "Run: 10, Epoch: 041, Loss: 1.0177, Train: 0.7263, Val: 0.7083, Test: 0.5701\n",
            "Run: 10, Epoch: 042, Loss: 1.0130, Train: 0.7280, Val: 0.7095, Test: 0.5709\n",
            "Run: 10, Epoch: 043, Loss: 1.0079, Train: 0.7287, Val: 0.7108, Test: 0.5713\n",
            "Run: 10, Epoch: 044, Loss: 1.0052, Train: 0.7302, Val: 0.7112, Test: 0.5716\n",
            "Run: 10, Epoch: 045, Loss: 1.0032, Train: 0.7317, Val: 0.7124, Test: 0.5720\n",
            "Run: 10, Epoch: 046, Loss: 0.9989, Train: 0.7330, Val: 0.7123, Test: 0.5727\n",
            "Run: 10, Epoch: 047, Loss: 0.9950, Train: 0.7334, Val: 0.7128, Test: 0.5731\n",
            "Run: 10, Epoch: 048, Loss: 0.9908, Train: 0.7339, Val: 0.7131, Test: 0.5729\n",
            "Run: 10, Epoch: 049, Loss: 0.9887, Train: 0.7349, Val: 0.7138, Test: 0.5731\n",
            "Run: 10, Epoch: 050, Loss: 0.9867, Train: 0.7368, Val: 0.7161, Test: 0.5746\n",
            "Run: 10, Epoch: 051, Loss: 0.9809, Train: 0.7439, Val: 0.7234, Test: 0.5802\n",
            "Run: 10, Epoch: 052, Loss: 0.9789, Train: 0.7452, Val: 0.7245, Test: 0.5816\n",
            "Run: 10, Epoch: 053, Loss: 0.9770, Train: 0.7458, Val: 0.7243, Test: 0.5822\n",
            "Run: 10, Epoch: 054, Loss: 0.9745, Train: 0.7459, Val: 0.7243, Test: 0.5823\n",
            "Run: 10, Epoch: 055, Loss: 0.9700, Train: 0.7469, Val: 0.7251, Test: 0.5830\n",
            "Run: 10, Epoch: 056, Loss: 0.9679, Train: 0.7481, Val: 0.7265, Test: 0.5841\n",
            "Run: 10, Epoch: 057, Loss: 0.9660, Train: 0.7509, Val: 0.7283, Test: 0.5861\n",
            "Run: 10, Epoch: 058, Loss: 0.9630, Train: 0.7525, Val: 0.7298, Test: 0.5874\n",
            "Run: 10, Epoch: 059, Loss: 0.9612, Train: 0.7530, Val: 0.7301, Test: 0.5874\n",
            "Run: 10, Epoch: 060, Loss: 0.9590, Train: 0.7524, Val: 0.7290, Test: 0.5859\n",
            "Run: 10, Epoch: 061, Loss: 0.9571, Train: 0.7526, Val: 0.7298, Test: 0.5855\n",
            "Run: 10, Epoch: 062, Loss: 0.9517, Train: 0.7535, Val: 0.7302, Test: 0.5866\n",
            "Run: 10, Epoch: 063, Loss: 0.9518, Train: 0.7556, Val: 0.7331, Test: 0.5891\n",
            "Run: 10, Epoch: 064, Loss: 0.9502, Train: 0.7569, Val: 0.7346, Test: 0.5908\n",
            "Run: 10, Epoch: 065, Loss: 0.9471, Train: 0.7574, Val: 0.7355, Test: 0.5915\n",
            "Run: 10, Epoch: 066, Loss: 0.9451, Train: 0.7577, Val: 0.7346, Test: 0.5902\n",
            "Run: 10, Epoch: 067, Loss: 0.9425, Train: 0.7570, Val: 0.7323, Test: 0.5887\n",
            "Run: 10, Epoch: 068, Loss: 0.9387, Train: 0.7580, Val: 0.7334, Test: 0.5893\n",
            "Run: 10, Epoch: 069, Loss: 0.9367, Train: 0.7594, Val: 0.7355, Test: 0.5911\n",
            "Run: 10, Epoch: 070, Loss: 0.9338, Train: 0.7611, Val: 0.7377, Test: 0.5928\n",
            "Run: 10, Epoch: 071, Loss: 0.9348, Train: 0.7617, Val: 0.7384, Test: 0.5935\n",
            "Run: 10, Epoch: 072, Loss: 0.9305, Train: 0.7620, Val: 0.7380, Test: 0.5929\n",
            "Run: 10, Epoch: 073, Loss: 0.9295, Train: 0.7615, Val: 0.7373, Test: 0.5920\n",
            "Run: 10, Epoch: 074, Loss: 0.9286, Train: 0.7617, Val: 0.7374, Test: 0.5918\n",
            "Run: 10, Epoch: 075, Loss: 0.9273, Train: 0.7630, Val: 0.7377, Test: 0.5925\n",
            "Run: 10, Epoch: 076, Loss: 0.9241, Train: 0.7648, Val: 0.7393, Test: 0.5941\n",
            "Run: 10, Epoch: 077, Loss: 0.9229, Train: 0.7656, Val: 0.7400, Test: 0.5950\n",
            "Run: 10, Epoch: 078, Loss: 0.9212, Train: 0.7659, Val: 0.7400, Test: 0.5950\n",
            "Run: 10, Epoch: 079, Loss: 0.9206, Train: 0.7661, Val: 0.7398, Test: 0.5947\n",
            "Run: 10, Epoch: 080, Loss: 0.9181, Train: 0.7670, Val: 0.7404, Test: 0.5953\n",
            "Run: 10, Epoch: 081, Loss: 0.9172, Train: 0.7681, Val: 0.7418, Test: 0.5961\n",
            "Run: 10, Epoch: 082, Loss: 0.9135, Train: 0.7687, Val: 0.7421, Test: 0.5966\n",
            "Run: 10, Epoch: 083, Loss: 0.9103, Train: 0.7694, Val: 0.7420, Test: 0.5966\n",
            "Run: 10, Epoch: 084, Loss: 0.9105, Train: 0.7697, Val: 0.7416, Test: 0.5970\n",
            "Run: 10, Epoch: 085, Loss: 0.9096, Train: 0.7702, Val: 0.7425, Test: 0.5975\n",
            "Run: 10, Epoch: 086, Loss: 0.9072, Train: 0.7714, Val: 0.7436, Test: 0.5982\n",
            "Run: 10, Epoch: 087, Loss: 0.9074, Train: 0.7718, Val: 0.7447, Test: 0.5988\n",
            "Run: 10, Epoch: 088, Loss: 0.9065, Train: 0.7719, Val: 0.7446, Test: 0.5984\n",
            "Run: 10, Epoch: 089, Loss: 0.9037, Train: 0.7723, Val: 0.7439, Test: 0.5978\n",
            "Run: 10, Epoch: 090, Loss: 0.9009, Train: 0.7724, Val: 0.7436, Test: 0.5976\n",
            "Run: 10, Epoch: 091, Loss: 0.8999, Train: 0.7732, Val: 0.7444, Test: 0.5979\n",
            "Run: 10, Epoch: 092, Loss: 0.8970, Train: 0.7739, Val: 0.7448, Test: 0.5986\n",
            "Run: 10, Epoch: 093, Loss: 0.8966, Train: 0.7744, Val: 0.7452, Test: 0.5992\n",
            "Run: 10, Epoch: 094, Loss: 0.8972, Train: 0.7744, Val: 0.7446, Test: 0.5995\n",
            "Run: 10, Epoch: 095, Loss: 0.8931, Train: 0.7745, Val: 0.7447, Test: 0.5997\n",
            "Run: 10, Epoch: 096, Loss: 0.8934, Train: 0.7754, Val: 0.7459, Test: 0.6002\n",
            "Run: 10, Epoch: 097, Loss: 0.8922, Train: 0.7761, Val: 0.7461, Test: 0.6006\n",
            "Run: 10, Epoch: 098, Loss: 0.8907, Train: 0.7764, Val: 0.7463, Test: 0.6005\n",
            "Run: 10, Epoch: 099, Loss: 0.8880, Train: 0.7766, Val: 0.7459, Test: 0.6000\n",
            "Run: 10, Epoch: 100, Loss: 0.8867, Train: 0.7776, Val: 0.7458, Test: 0.6003\n",
            "Run: 10, Epoch: 101, Loss: 0.8865, Train: 0.7787, Val: 0.7478, Test: 0.6017\n",
            "Run: 10, Epoch: 102, Loss: 0.8863, Train: 0.7793, Val: 0.7493, Test: 0.6022\n",
            "Run: 10, Epoch: 103, Loss: 0.8850, Train: 0.7792, Val: 0.7485, Test: 0.6018\n",
            "Run: 10, Epoch: 104, Loss: 0.8833, Train: 0.7787, Val: 0.7472, Test: 0.6006\n",
            "Run: 10, Epoch: 105, Loss: 0.8811, Train: 0.7796, Val: 0.7475, Test: 0.6005\n",
            "Run: 10, Epoch: 106, Loss: 0.8807, Train: 0.7808, Val: 0.7488, Test: 0.6014\n",
            "Run: 10, Epoch: 107, Loss: 0.8774, Train: 0.7815, Val: 0.7495, Test: 0.6022\n",
            "Run: 10, Epoch: 108, Loss: 0.8776, Train: 0.7820, Val: 0.7499, Test: 0.6025\n",
            "Run: 10, Epoch: 109, Loss: 0.8779, Train: 0.7826, Val: 0.7507, Test: 0.6030\n",
            "Run: 10, Epoch: 110, Loss: 0.8771, Train: 0.7830, Val: 0.7503, Test: 0.6030\n",
            "Run: 10, Epoch: 111, Loss: 0.8744, Train: 0.7832, Val: 0.7499, Test: 0.6027\n",
            "Run: 10, Epoch: 112, Loss: 0.8729, Train: 0.7834, Val: 0.7503, Test: 0.6025\n",
            "Run: 10, Epoch: 113, Loss: 0.8733, Train: 0.7840, Val: 0.7509, Test: 0.6030\n",
            "Run: 10, Epoch: 114, Loss: 0.8716, Train: 0.7851, Val: 0.7522, Test: 0.6041\n",
            "Run: 10, Epoch: 115, Loss: 0.8688, Train: 0.7856, Val: 0.7525, Test: 0.6045\n",
            "Run: 10, Epoch: 116, Loss: 0.8686, Train: 0.7857, Val: 0.7517, Test: 0.6040\n",
            "Run: 10, Epoch: 117, Loss: 0.8705, Train: 0.7849, Val: 0.7510, Test: 0.6030\n",
            "Run: 10, Epoch: 118, Loss: 0.8672, Train: 0.7859, Val: 0.7519, Test: 0.6038\n",
            "Run: 10, Epoch: 119, Loss: 0.8647, Train: 0.7869, Val: 0.7526, Test: 0.6050\n",
            "Run: 10, Epoch: 120, Loss: 0.8642, Train: 0.7876, Val: 0.7528, Test: 0.6055\n",
            "Run: 10, Epoch: 121, Loss: 0.8642, Train: 0.7879, Val: 0.7528, Test: 0.6053\n",
            "Run: 10, Epoch: 122, Loss: 0.8653, Train: 0.7884, Val: 0.7533, Test: 0.6049\n",
            "Run: 10, Epoch: 123, Loss: 0.8594, Train: 0.7885, Val: 0.7538, Test: 0.6042\n",
            "Run: 10, Epoch: 124, Loss: 0.8615, Train: 0.7884, Val: 0.7538, Test: 0.6041\n",
            "Run: 10, Epoch: 125, Loss: 0.8607, Train: 0.7885, Val: 0.7538, Test: 0.6048\n",
            "Run: 10, Epoch: 126, Loss: 0.8602, Train: 0.7893, Val: 0.7547, Test: 0.6057\n",
            "Run: 10, Epoch: 127, Loss: 0.8565, Train: 0.7900, Val: 0.7556, Test: 0.6064\n",
            "Run: 10, Epoch: 128, Loss: 0.8579, Train: 0.7905, Val: 0.7556, Test: 0.6060\n",
            "Run: 10, Epoch: 129, Loss: 0.8568, Train: 0.7903, Val: 0.7542, Test: 0.6052\n",
            "Run: 10, Epoch: 130, Loss: 0.8546, Train: 0.7902, Val: 0.7534, Test: 0.6049\n",
            "Run: 10, Epoch: 131, Loss: 0.8540, Train: 0.7910, Val: 0.7539, Test: 0.6058\n",
            "Run: 10, Epoch: 132, Loss: 0.8505, Train: 0.7922, Val: 0.7544, Test: 0.6066\n",
            "Run: 10, Epoch: 133, Loss: 0.8494, Train: 0.7922, Val: 0.7551, Test: 0.6068\n",
            "Run: 10, Epoch: 134, Loss: 0.8525, Train: 0.7928, Val: 0.7554, Test: 0.6071\n",
            "Run: 10, Epoch: 135, Loss: 0.8478, Train: 0.7930, Val: 0.7550, Test: 0.6073\n",
            "Run: 10, Epoch: 136, Loss: 0.8488, Train: 0.7934, Val: 0.7546, Test: 0.6073\n",
            "Run: 10, Epoch: 137, Loss: 0.8468, Train: 0.7934, Val: 0.7553, Test: 0.6074\n",
            "Run: 10, Epoch: 138, Loss: 0.8465, Train: 0.7939, Val: 0.7557, Test: 0.6077\n",
            "Run: 10, Epoch: 139, Loss: 0.8474, Train: 0.7943, Val: 0.7566, Test: 0.6079\n",
            "Run: 10, Epoch: 140, Loss: 0.8476, Train: 0.7948, Val: 0.7575, Test: 0.6079\n",
            "Run: 10, Epoch: 141, Loss: 0.8451, Train: 0.7950, Val: 0.7574, Test: 0.6078\n",
            "Run: 10, Epoch: 142, Loss: 0.8432, Train: 0.7949, Val: 0.7569, Test: 0.6074\n",
            "Run: 10, Epoch: 143, Loss: 0.8435, Train: 0.7948, Val: 0.7563, Test: 0.6073\n",
            "Run: 10, Epoch: 144, Loss: 0.8430, Train: 0.7960, Val: 0.7566, Test: 0.6083\n",
            "Run: 10, Epoch: 145, Loss: 0.8408, Train: 0.7963, Val: 0.7575, Test: 0.6087\n",
            "Run: 10, Epoch: 146, Loss: 0.8386, Train: 0.7959, Val: 0.7574, Test: 0.6082\n",
            "Run: 10, Epoch: 147, Loss: 0.8394, Train: 0.7962, Val: 0.7567, Test: 0.6082\n",
            "Run: 10, Epoch: 148, Loss: 0.8378, Train: 0.7964, Val: 0.7567, Test: 0.6085\n",
            "Run: 10, Epoch: 149, Loss: 0.8383, Train: 0.7972, Val: 0.7559, Test: 0.6085\n",
            "Run: 10, Epoch: 150, Loss: 0.8379, Train: 0.7978, Val: 0.7576, Test: 0.6087\n",
            "Run: 10, Epoch: 151, Loss: 0.8358, Train: 0.7985, Val: 0.7584, Test: 0.6095\n",
            "Run: 10, Epoch: 152, Loss: 0.8355, Train: 0.7989, Val: 0.7592, Test: 0.6099\n",
            "Run: 10, Epoch: 153, Loss: 0.8337, Train: 0.7987, Val: 0.7584, Test: 0.6095\n",
            "Run: 10, Epoch: 154, Loss: 0.8334, Train: 0.7990, Val: 0.7584, Test: 0.6093\n",
            "Run: 10, Epoch: 155, Loss: 0.8329, Train: 0.7995, Val: 0.7596, Test: 0.6094\n",
            "Run: 10, Epoch: 156, Loss: 0.8302, Train: 0.8000, Val: 0.7593, Test: 0.6095\n",
            "Run: 10, Epoch: 157, Loss: 0.8304, Train: 0.8003, Val: 0.7587, Test: 0.6093\n",
            "Run: 10, Epoch: 158, Loss: 0.8320, Train: 0.8001, Val: 0.7582, Test: 0.6095\n",
            "Run: 10, Epoch: 159, Loss: 0.8285, Train: 0.8006, Val: 0.7590, Test: 0.6105\n",
            "Run: 10, Epoch: 160, Loss: 0.8305, Train: 0.8006, Val: 0.7598, Test: 0.6109\n",
            "Run: 10, Epoch: 161, Loss: 0.8278, Train: 0.8007, Val: 0.7591, Test: 0.6105\n",
            "Run: 10, Epoch: 162, Loss: 0.8263, Train: 0.8007, Val: 0.7579, Test: 0.6099\n",
            "Run: 10, Epoch: 163, Loss: 0.8285, Train: 0.8016, Val: 0.7597, Test: 0.6103\n",
            "Run: 10, Epoch: 164, Loss: 0.8257, Train: 0.8020, Val: 0.7602, Test: 0.6104\n",
            "Run: 10, Epoch: 165, Loss: 0.8254, Train: 0.8016, Val: 0.7596, Test: 0.6103\n",
            "Run: 10, Epoch: 166, Loss: 0.8250, Train: 0.8013, Val: 0.7581, Test: 0.6107\n",
            "Run: 10, Epoch: 167, Loss: 0.8228, Train: 0.8024, Val: 0.7612, Test: 0.6118\n",
            "Run: 10, Epoch: 168, Loss: 0.8253, Train: 0.8027, Val: 0.7611, Test: 0.6112\n",
            "Run: 10, Epoch: 169, Loss: 0.8220, Train: 0.8028, Val: 0.7597, Test: 0.6103\n",
            "Run: 10, Epoch: 170, Loss: 0.8224, Train: 0.8033, Val: 0.7609, Test: 0.6106\n",
            "Run: 10, Epoch: 171, Loss: 0.8208, Train: 0.8035, Val: 0.7609, Test: 0.6113\n",
            "Run: 10, Epoch: 172, Loss: 0.8209, Train: 0.8033, Val: 0.7607, Test: 0.6115\n",
            "Run: 10, Epoch: 173, Loss: 0.8208, Train: 0.8037, Val: 0.7602, Test: 0.6117\n",
            "Run: 10, Epoch: 174, Loss: 0.8195, Train: 0.8044, Val: 0.7620, Test: 0.6120\n",
            "Run: 10, Epoch: 175, Loss: 0.8167, Train: 0.8046, Val: 0.7619, Test: 0.6120\n",
            "Run: 10, Epoch: 176, Loss: 0.8149, Train: 0.8043, Val: 0.7607, Test: 0.6114\n",
            "Run: 10, Epoch: 177, Loss: 0.8181, Train: 0.8044, Val: 0.7606, Test: 0.6114\n",
            "Run: 10, Epoch: 178, Loss: 0.8183, Train: 0.8048, Val: 0.7619, Test: 0.6119\n",
            "Run: 10, Epoch: 179, Loss: 0.8166, Train: 0.8048, Val: 0.7617, Test: 0.6120\n",
            "Run: 10, Epoch: 180, Loss: 0.8165, Train: 0.8050, Val: 0.7618, Test: 0.6119\n",
            "Run: 10, Epoch: 181, Loss: 0.8155, Train: 0.8055, Val: 0.7621, Test: 0.6116\n",
            "Run: 10, Epoch: 182, Loss: 0.8147, Train: 0.8063, Val: 0.7631, Test: 0.6120\n",
            "Run: 10, Epoch: 183, Loss: 0.8130, Train: 0.8063, Val: 0.7631, Test: 0.6122\n",
            "Run: 10, Epoch: 184, Loss: 0.8108, Train: 0.8059, Val: 0.7623, Test: 0.6118\n",
            "Run: 10, Epoch: 185, Loss: 0.8122, Train: 0.8059, Val: 0.7622, Test: 0.6120\n",
            "Run: 10, Epoch: 186, Loss: 0.8079, Train: 0.8066, Val: 0.7627, Test: 0.6125\n",
            "Run: 10, Epoch: 187, Loss: 0.8111, Train: 0.8072, Val: 0.7628, Test: 0.6128\n",
            "Run: 10, Epoch: 188, Loss: 0.8104, Train: 0.8077, Val: 0.7629, Test: 0.6128\n",
            "Run: 10, Epoch: 189, Loss: 0.8088, Train: 0.8082, Val: 0.7640, Test: 0.6133\n",
            "Run: 10, Epoch: 190, Loss: 0.8071, Train: 0.8076, Val: 0.7626, Test: 0.6127\n",
            "Run: 10, Epoch: 191, Loss: 0.8087, Train: 0.8081, Val: 0.7626, Test: 0.6129\n",
            "Run: 10, Epoch: 192, Loss: 0.8066, Train: 0.8085, Val: 0.7632, Test: 0.6132\n",
            "Run: 10, Epoch: 193, Loss: 0.8081, Train: 0.8085, Val: 0.7632, Test: 0.6128\n",
            "Run: 10, Epoch: 194, Loss: 0.8072, Train: 0.8083, Val: 0.7629, Test: 0.6124\n",
            "Run: 10, Epoch: 195, Loss: 0.8042, Train: 0.8088, Val: 0.7632, Test: 0.6128\n",
            "Run: 10, Epoch: 196, Loss: 0.8058, Train: 0.8096, Val: 0.7647, Test: 0.6136\n",
            "Run: 10, Epoch: 197, Loss: 0.8051, Train: 0.8092, Val: 0.7638, Test: 0.6135\n",
            "Run: 10, Epoch: 198, Loss: 0.8028, Train: 0.8094, Val: 0.7634, Test: 0.6136\n",
            "Run: 10, Epoch: 199, Loss: 0.8044, Train: 0.8101, Val: 0.7645, Test: 0.6143\n",
            "Run: 10, Epoch: 200, Loss: 0.8034, Train: 0.8101, Val: 0.7642, Test: 0.6140\n",
            "Correct and smooth...\n",
            "Done!\n",
            "Train: 0.9452, Val: 0.9127, Test: 0.8405\n",
            "All runs:\n",
            "Highest Train: 94.57 ± 0.04\n",
            "Highest Valid: 91.24 ± 0.04\n",
            "  Final Train: 94.57 ± 0.04\n",
            "   Final Test: 84.10 ± 0.06\n"
          ]
        }
      ]
    }
  ]
}